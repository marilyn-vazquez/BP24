{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161acffa-b448-4b83-81f4-24efa3bcc570",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde92cb5-f744-48b6-8f54-04af24047e43",
   "metadata": {},
   "source": [
    "### Import Data & Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "242a2fb9-ddf3-416b-89d5-ae3affe6a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "# data = np.loadtxt(\"C:/Users/aceme/OneDrive/Documents/GitHub/BP24/AceMejiaSanchez/Data/gaussian_small_d_1.tex\")\n",
    "data = np.loadtxt(\"C:/Users/aceme/OneDrive/Documents/GitHub/BP24/Kate/Data/uniform_small_d_1.tex\")\n",
    "# data = np.loadtxt(\"C:/Users/aceme/OneDrive/Documents/GitHub/BP24/Fabiana/Demos Fabi/uniform_large_d_1.tex\")\n",
    "# data = np.loadtxt(\"C:/Users/aceme/OneDrive/Documents/GitHub/BP24/Ellee/Data/gaussian_large_d_1.tex\")\n",
    "\n",
    "# Creating NumPy array\n",
    "array = np.array(data)\n",
    "\n",
    "# Converting to Pandas DataFrame\n",
    "df = pd.DataFrame(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be45cd6-ec01-4103-b03a-87fbb8c40d10",
   "metadata": {},
   "source": [
    "### Prepping data & full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b78e078f-1506-4df6-9c70-feb3232eade1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048794</td>\n",
       "      <td>0.116295</td>\n",
       "      <td>0.750023</td>\n",
       "      <td>0.504721</td>\n",
       "      <td>0.482695</td>\n",
       "      <td>0.061228</td>\n",
       "      <td>0.800553</td>\n",
       "      <td>0.815441</td>\n",
       "      <td>0.997321</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638190</td>\n",
       "      <td>0.252662</td>\n",
       "      <td>0.584352</td>\n",
       "      <td>0.731232</td>\n",
       "      <td>0.420754</td>\n",
       "      <td>0.293242</td>\n",
       "      <td>0.294986</td>\n",
       "      <td>0.651735</td>\n",
       "      <td>0.504970</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905198</td>\n",
       "      <td>0.530844</td>\n",
       "      <td>0.446072</td>\n",
       "      <td>0.157876</td>\n",
       "      <td>0.661326</td>\n",
       "      <td>0.562504</td>\n",
       "      <td>0.474810</td>\n",
       "      <td>0.189050</td>\n",
       "      <td>0.602920</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692931</td>\n",
       "      <td>0.695386</td>\n",
       "      <td>0.780015</td>\n",
       "      <td>0.914984</td>\n",
       "      <td>0.757668</td>\n",
       "      <td>0.329954</td>\n",
       "      <td>0.865520</td>\n",
       "      <td>0.779557</td>\n",
       "      <td>0.979039</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458531</td>\n",
       "      <td>0.257464</td>\n",
       "      <td>0.371455</td>\n",
       "      <td>0.015987</td>\n",
       "      <td>0.082521</td>\n",
       "      <td>0.108772</td>\n",
       "      <td>0.681779</td>\n",
       "      <td>0.217713</td>\n",
       "      <td>0.070986</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9    ...       141  \\\n",
       "159  1.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  1.0  1.0  ...  0.048794   \n",
       "198  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  ...  0.638190   \n",
       "259  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.905198   \n",
       "301  0.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  1.0  1.0  ...  0.692931   \n",
       "220  1.0  1.0  1.0  0.0  1.0  1.0  1.0  0.0  1.0  1.0  ...  0.458531   \n",
       "\n",
       "          142       143       144       145       146       147       148  \\\n",
       "159  0.116295  0.750023  0.504721  0.482695  0.061228  0.800553  0.815441   \n",
       "198  0.252662  0.584352  0.731232  0.420754  0.293242  0.294986  0.651735   \n",
       "259  0.530844  0.446072  0.157876  0.661326  0.562504  0.474810  0.189050   \n",
       "301  0.695386  0.780015  0.914984  0.757668  0.329954  0.865520  0.779557   \n",
       "220  0.257464  0.371455  0.015987  0.082521  0.108772  0.681779  0.217713   \n",
       "\n",
       "          149  150  \n",
       "159  0.997321  0.0  \n",
       "198  0.504970  1.0  \n",
       "259  0.602920  0.0  \n",
       "301  0.979039  0.0  \n",
       "220  0.070986  0.0  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting 25 columns from numerical floats -> categorical integers\n",
    "for i in range(25):\n",
    "    \n",
    "    df.iloc[:,i] = df.iloc[:,i].round() # Rounding\n",
    "    df.iloc[:,i] = df.iloc[:,i].astype(int) # Integer\n",
    "\n",
    "# Split dataset into X_train and y_train\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0:150], df.iloc[:,-1], test_size=0.2, random_state=52)\n",
    "\n",
    "# Forming complete training set by attaching X_train and y_train\n",
    "train_combined = pd.concat([X_train, y_train], axis = 1)\n",
    "train_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74dfb6-8539-437b-9d2a-e259aca1c595",
   "metadata": {},
   "source": [
    "### Applying SMOTE & XGBoost 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0a17e27c-9c21-4514-93cf-ccdbb20c584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an array of zeros with size 100\n",
    "f1_scores_array = np.zeros(100)\n",
    "\n",
    "# Loop 100 times to populate the array\n",
    "for i in range(100):\n",
    "    \n",
    "    ##################################### Imbalancing: Class 0 Majority #####################################\n",
    "    \n",
    "    # Assuming df_table is your DataFrame and the label column is at index 150\n",
    "    # Create a new label column with 300 zeros and 200 ones\n",
    "    label1 = np.array([0] * 250 + [1] * 150)\n",
    "    \n",
    "    # Shuffle randomly the new label column\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(label1)\n",
    "    \n",
    "    # Replace the label column with the new imbalanced labels\n",
    "    train_combined['label_0_majority'] = label1\n",
    "    #print(train_combined['label_0_majority'].value_counts())\n",
    "    \n",
    "    ##################################### Imbalancing: Class 1 Majority #####################################\n",
    "    \n",
    "    # Create a second imbalanced label column with 200 zeros and 300 ones\n",
    "    label2 = np.array([0] * 150 + [1] * 250)\n",
    "    \n",
    "    # Shuffle randomly the new label column\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(label2)\n",
    "    \n",
    "    # Add the new label column to the DataFrame\n",
    "    train_combined['label_1_majority'] = label2\n",
    "    \n",
    "    #print(train_combined['label_1_majority'].value_counts())\n",
    "    \n",
    "    ##################################### Subsets #####################################\n",
    "    \n",
    "    # Subset for the 1 majority\n",
    "    df_imbalance_0 = train_combined.drop(columns=[150, 'label_1_majority']) # Drop original label in Col 150 & label_1_majority\n",
    "    df_imbalance_0.head()\n",
    "    \n",
    "    # Subset for the 0 majority\n",
    "    df_imbalance_1 = train_combined.drop(columns=[150, 'label_0_majority']) # Drop original label in Col 150 & label_1_majority\n",
    "    df_imbalance_1.head()\n",
    "    \n",
    "    ##################################### Data Augmentation: SMOTE #####################################\n",
    "    \n",
    "    ####### SMOTE applied to label_0_majority class\n",
    "    \n",
    "    # Creating x and y \n",
    "    x=df_imbalance_0.drop(['label_0_majority'],axis=1)\n",
    "    y=df_imbalance_0['label_0_majority'] # creating imbalance\n",
    "    \n",
    "    # Applying SMOTE to balance classes\n",
    "    smote=SMOTE(sampling_strategy='minority')  # generating synthetic samples for minority class\n",
    "    x,y=smote.fit_resample(x,y) # re-sampling\n",
    "    \n",
    "    ####### Creating a new training set with the new 100 rows (Class Majority 0) we augmented and the old training set\n",
    "    \n",
    "    # Re-combine balanced x and y into a dataframe\n",
    "    new_train_combined_0 = pd.concat([x, y], axis = 1)\n",
    "    \n",
    "    # Re-naming the 'label_0_majority' label by its index 150, which creates a duplicate, & dropping the remaining 'label_0_majority'\n",
    "    new_train_combined_0[150] = new_train_combined_0['label_0_majority']\n",
    "    new_train_combined_0 = new_train_combined_0.drop(columns=['label_0_majority']) \n",
    "    \n",
    "    # Sanity checks: making sure the right columns are there & the correct # of class values\n",
    "    # print(new_train_combined_0[150].value_counts()) \n",
    "    # new_train_combined_0.head()\n",
    "    \n",
    "    # Subset the last 100 rows that were augmented in new_train_combined_0\n",
    "    aug_0 = new_train_combined_0.tail(100)\n",
    "    \n",
    "    # Dropping extra labels to have right dimensions\n",
    "    train_combined = train_combined.drop(columns=['label_0_majority', 'label_1_majority']) \n",
    "    \n",
    "    # Checking it concatenated properly\n",
    "    # Check the shape of the original dataFrames\n",
    "    # print(\"Shape of train_combined:\", train_combined.shape)\n",
    "    # print(\"Shape of aug_0:\", aug_0.shape)\n",
    "    \n",
    "    # Concatenating the dataframes\n",
    "    train_combined_PRE_FINAL = pd.concat([train_combined, aug_0], ignore_index=True)\n",
    "    \n",
    "    # Check the shape of the combined dataframe\n",
    "    # print(\"Shape of train_combined_PRE_FINAL:\", train_combined_PRE_FINAL.shape)\n",
    "    \n",
    "    # Print the combined dataframe\n",
    "    # print(train_combined_PRE_FINAL)\n",
    "    \n",
    "    ####### SMOTE applied to label_1_majority class\n",
    "    \n",
    "    # Creating x and y \n",
    "    x=df_imbalance_1.drop(['label_1_majority'],axis=1)\n",
    "    y=df_imbalance_1['label_1_majority'] # creating imbalance\n",
    "    \n",
    "    # Applying SMOTE to balance classes\n",
    "    smote=SMOTE(sampling_strategy='minority')  # generating synthetic samples for minority class\n",
    "    x,y=smote.fit_resample(x,y) # re-sampling\n",
    "    \n",
    "    ####### Creating a new training set with the new 100 rows (Class Majority 1) we augmented and the old training set\n",
    "    \n",
    "    # Re-combine balanced x and y into a dataframe\n",
    "    new_train_combined_1 = pd.concat([x, y], axis = 1)\n",
    "    \n",
    "    # Re-naming the 'label_1_majority' label by its index 150, which creates a duplicate, & dropping the remaining 'label_1_majority'\n",
    "    new_train_combined_1[150] = new_train_combined_1['label_1_majority']\n",
    "    new_train_combined_1 = new_train_combined_1.drop(columns=['label_1_majority']) \n",
    "    \n",
    "    # Sanity checks: making sure the right columns are there & the correct # of class values\n",
    "    # print(new_train_combined_1[150].value_counts()) \n",
    "    # new_train_combined_1.head()\n",
    "    \n",
    "    # Subset the last 100 rows that were augmented in new_train_combined_0\n",
    "    aug_1 = new_train_combined_1.tail(100)\n",
    "    \n",
    "    # Checking it concatenated properly\n",
    "    # Check the shape of the original dataFrames\n",
    "    # print(\"Shape of train_combined:\", train_combined_PRE_FINAL.shape)\n",
    "    # print(\"Shape of aug_0:\", aug_1.shape)\n",
    "    \n",
    "    # Concatenating the DataFrames\n",
    "    new_train_combined = pd.concat([train_combined_PRE_FINAL, aug_1], ignore_index=True)\n",
    "    \n",
    "    # Check the shape of the combined dataFrame\n",
    "    # print(\"Shape of new_train_combined:\", new_train_combined.shape)\n",
    "    \n",
    "    # Print the combined dataFrame\n",
    "    # print(new_train_combined)\n",
    "    \n",
    "    ##################################### Classifier: XGBOOST #####################################\n",
    "    \n",
    "    # Prepping our new dataset by 'splitting' it\n",
    "    X_train = new_train_combined.iloc[:,0:150]\n",
    "    y_train = new_train_combined.iloc[:,-1]\n",
    "    \n",
    "    # First, put this prompt: \"conda install -c conda-forge py-xgboost\" in anaconda to download xgboost package\n",
    "    # install xgboost in jupyter\n",
    "    # !pip install xgboost\n",
    "    \n",
    "    # create model instance\n",
    "    # n_estimators: number of trees(estimators) the model uses --> the more used, the more accurate the model is\n",
    "    # max_depth: maximum depth of tree --> higher number makes model more complex, but too high can cause overfitting\n",
    "    # learning_rate: quantifies each tree's contribution to total prediction --> lower number takes longer, but can lead to better generalization\n",
    "    # objective: binary:logistic outputs probabilities. if classification is wanted, use binary:hinge\n",
    "    bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic', enable_categorical=True)\n",
    "    \n",
    "    # fit model with the training data\n",
    "    bst.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions for the test dataset\n",
    "    preds = bst.predict(X_test)\n",
    "    \n",
    "    # print predictions\n",
    "    # print(preds)\n",
    "    \n",
    "    # print model Accuracy (how often the classifier is correct)\n",
    "    # print(\"Accuracy:\",metrics.accuracy_score(y_test, preds))\n",
    "\n",
    "    # Assign the accuracy directly to array\n",
    "    f1_scores_array[i] = skm.f1_score(y_test, preds)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "37d03ac8-cbdc-4c29-a0fc-bf318fb7bb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148 0.72131148\n",
      " 0.72131148 0.72131148 0.72131148 0.72131148]\n"
     ]
    }
   ],
   "source": [
    "print(f1_scores_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee7be30-6cd2-4de3-a1c8-abaa2c274602",
   "metadata": {},
   "source": [
    "### Saving full training and augmented dataset combo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9de577b-4e08-447c-82fc-b494e01258af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe \n",
    "# new_train_combined.to_csv('new_train_combined.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552fab9-ec2b-4d64-946a-fffdf8437a5a",
   "metadata": {},
   "source": [
    "# Timing 1 run of SMOTE Aug & XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c89d1b0f-6a23-4d4b-9572-55faef976165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.30624890327453613 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 162\u001b[0m\n\u001b[0;32m    159\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "# start_time = time.time()\n",
    "# ##################################### Imbalancing: Class 0 Majority #####################################\n",
    "# # Assuming df_table is your DataFrame and the label column is at index 150\n",
    "# # Create a new label column with 300 zeros and 200 ones\n",
    "# label1 = np.array([0] * 250 + [1] * 150)\n",
    "\n",
    "# # Shuffle randomly the new label column\n",
    "# np.random.seed(1)\n",
    "# np.random.shuffle(label1)\n",
    "\n",
    "# # Replace the label column with the new imbalanced labels\n",
    "# train_combined['label_0_majority'] = label1\n",
    "# #print(train_combined['label_0_majority'].value_counts())\n",
    "\n",
    "# ##################################### Imbalancing: Class 1 Majority #####################################\n",
    "\n",
    "# # Create a second imbalanced label column with 200 zeros and 300 ones\n",
    "# label2 = np.array([0] * 150 + [1] * 250)\n",
    "\n",
    "# # Shuffle randomly the new label column\n",
    "# np.random.seed(1)\n",
    "# np.random.shuffle(label2)\n",
    "\n",
    "# # Add the new label column to the DataFrame\n",
    "# train_combined['label_1_majority'] = label2\n",
    "\n",
    "# #print(train_combined['label_1_majority'].value_counts())\n",
    "\n",
    "# ##################################### Subsets #####################################\n",
    "\n",
    "# # Subset for the 1 majority\n",
    "# df_imbalance_0 = train_combined.drop(columns=[150, 'label_1_majority']) # Drop original label in Col 150 & label_1_majority\n",
    "# df_imbalance_0.head()\n",
    "\n",
    "# # Subset for the 0 majority\n",
    "# df_imbalance_1 = train_combined.drop(columns=[150, 'label_0_majority']) # Drop original label in Col 150 & label_1_majority\n",
    "# df_imbalance_1.head()\n",
    "\n",
    "# ##################################### Data Augmentation: SMOTE #####################################\n",
    "\n",
    "# ####### SMOTE applied to label_0_majority class\n",
    "\n",
    "# # Creating x and y \n",
    "# x=df_imbalance_0.drop(['label_0_majority'],axis=1)\n",
    "# y=df_imbalance_0['label_0_majority'] # creating imbalance\n",
    "\n",
    "# # Applying SMOTE to balance classes\n",
    "# smote=SMOTE(sampling_strategy='minority')  # generating synthetic samples for minority class\n",
    "# x,y=smote.fit_resample(x,y) # re-sampling\n",
    "\n",
    "# ####### Creating a new training set with the new 100 rows (Class Majority 0) we augmented and the old training set\n",
    "\n",
    "# # Re-combine balanced x and y into a dataframe\n",
    "# new_train_combined_0 = pd.concat([x, y], axis = 1)\n",
    "\n",
    "# # Re-naming the 'label_0_majority' label by its index 150, which creates a duplicate, & dropping the remaining 'label_0_majority'\n",
    "# new_train_combined_0[150] = new_train_combined_0['label_0_majority']\n",
    "# new_train_combined_0 = new_train_combined_0.drop(columns=['label_0_majority']) \n",
    "\n",
    "# # Sanity checks: making sure the right columns are there & the correct # of class values\n",
    "# # print(new_train_combined_0[150].value_counts()) \n",
    "# # new_train_combined_0.head()\n",
    "\n",
    "# # Subset the last 100 rows that were augmented in new_train_combined_0\n",
    "# aug_0 = new_train_combined_0.tail(100)\n",
    "\n",
    "# # Dropping extra labels to have right dimensions\n",
    "# train_combined = train_combined.drop(columns=['label_0_majority', 'label_1_majority']) \n",
    "\n",
    "# # Checking it concatenated properly\n",
    "# # Check the shape of the original dataFrames\n",
    "# # print(\"Shape of train_combined:\", train_combined.shape)\n",
    "# # print(\"Shape of aug_0:\", aug_0.shape)\n",
    "\n",
    "# # Concatenating the dataframes\n",
    "# train_combined_PRE_FINAL = pd.concat([train_combined, aug_0], ignore_index=True)\n",
    "\n",
    "# # Check the shape of the combined dataframe\n",
    "# # print(\"Shape of train_combined_PRE_FINAL:\", train_combined_PRE_FINAL.shape)\n",
    "\n",
    "# # Print the combined dataframe\n",
    "# # print(train_combined_PRE_FINAL)\n",
    "\n",
    "# ####### SMOTE applied to label_1_majority class\n",
    "\n",
    "# # Creating x and y \n",
    "# x=df_imbalance_1.drop(['label_1_majority'],axis=1)\n",
    "# y=df_imbalance_1['label_1_majority'] # creating imbalance\n",
    "\n",
    "# # Applying SMOTE to balance classes\n",
    "# smote=SMOTE(sampling_strategy='minority')  # generating synthetic samples for minority class\n",
    "# x,y=smote.fit_resample(x,y) # re-sampling\n",
    "\n",
    "# ####### Creating a new training set with the new 100 rows (Class Majority 1) we augmented and the old training set\n",
    "\n",
    "# # Re-combine balanced x and y into a dataframe\n",
    "# new_train_combined_1 = pd.concat([x, y], axis = 1)\n",
    "\n",
    "# # Re-naming the 'label_1_majority' label by its index 150, which creates a duplicate, & dropping the remaining 'label_1_majority'\n",
    "# new_train_combined_1[150] = new_train_combined_1['label_1_majority']\n",
    "# new_train_combined_1 = new_train_combined_1.drop(columns=['label_1_majority']) \n",
    "\n",
    "# # Sanity checks: making sure the right columns are there & the correct # of class values\n",
    "# # print(new_train_combined_1[150].value_counts()) \n",
    "# # new_train_combined_1.head()\n",
    "\n",
    "# # Subset the last 100 rows that were augmented in new_train_combined_0\n",
    "# aug_1 = new_train_combined_1.tail(100)\n",
    "\n",
    "# # Checking it concatenated properly\n",
    "# # Check the shape of the original dataFrames\n",
    "# # print(\"Shape of train_combined:\", train_combined_PRE_FINAL.shape)\n",
    "# # print(\"Shape of aug_0:\", aug_1.shape)\n",
    "\n",
    "# # Concatenating the DataFrames\n",
    "# new_train_combined = pd.concat([train_combined_PRE_FINAL, aug_1], ignore_index=True)\n",
    "\n",
    "# # Check the shape of the combined dataFrame\n",
    "# # print(\"Shape of new_train_combined:\", new_train_combined.shape)\n",
    "\n",
    "# # Print the combined dataFrame\n",
    "# # print(new_train_combined)\n",
    "\n",
    "# ##################################### Classifier: XGBOOST #####################################\n",
    "\n",
    "# # Prepping our new dataset by 'splitting' it\n",
    "# X_train = new_train_combined.iloc[:,0:150]\n",
    "# y_train = new_train_combined.iloc[:,-1]\n",
    "\n",
    "# # First, put this prompt: \"conda install -c conda-forge py-xgboost\" in anaconda to download xgboost package\n",
    "# # install xgboost in jupyter\n",
    "# # !pip install xgboost\n",
    "\n",
    "# # create model instance\n",
    "# # n_estimators: number of trees(estimators) the model uses --> the more used, the more accurate the model is\n",
    "# # max_depth: maximum depth of tree --> higher number makes model more complex, but too high can cause overfitting\n",
    "# # learning_rate: quantifies each tree's contribution to total prediction --> lower number takes longer, but can lead to better generalization\n",
    "# # objective: binary:logistic outputs probabilities. if classification is wanted, use binary:hinge\n",
    "# bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic', enable_categorical=True)\n",
    "\n",
    "# # fit model with the training data\n",
    "# bst.fit(X_train, y_train)\n",
    "\n",
    "# # make predictions for the test dataset\n",
    "# preds = bst.predict(X_test)\n",
    "\n",
    "# # print predictions\n",
    "# # print(preds)\n",
    "\n",
    "# # print model Accuracy (how often the classifier is correct)\n",
    "# # print(\"Accuracy:\",metrics.accuracy_score(y_test, preds))\n",
    "\n",
    "# # Assign the accuracy directly to array\n",
    "# skm.f1_score(y_test, preds)\n",
    "\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "\n",
    "# print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
