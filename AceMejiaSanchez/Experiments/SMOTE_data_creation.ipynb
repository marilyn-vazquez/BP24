{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459da863-68de-4b22-b759-c943ff6106ba",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2498c25c-5576-462e-b9d9-7f402649bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0592c3d3-9e73-48d3-8969-da47bf8d0851",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "677cce43-3879-4f50-afcc-b83b343ddc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original datasets\n",
    "data1 = pd.read_csv(\"C:/Users/aceme/OneDrive/Documents/GitHub/BP24/Ellee/Data/Gaussian/gaussian_orig.csv\", header=None)\n",
    "#data1 = pd.read_csv(\"C:/Users/aceme/OneDrive/Documents/GitHub/BP24/Ellee/Data/Uniform/uniform_orig.csv\", header=None)\n",
    "#data1 = pd.read_csv(\"C:/Users/aceme/OneDrive/Documents/GitHub/BP24/Ellee/Data/Stacked/stacked_orig.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5feb299c-07aa-461f-9afb-af48b9d8f909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 13)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7e869-9be6-4089-85ec-e3a125034e43",
   "metadata": {},
   "source": [
    "# Convert columns to CATEGORICAL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088bb4f9-0554-4a97-866a-f491f003c0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e954549b-908f-4b98-8015-952f9a7f79f0",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42ad0715-ce89-4d57-bf9a-30fc7fc5a978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12\n",
       "1.0    69\n",
       "0.0    68\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data1\n",
    "X = data1.iloc[:, :-1]\n",
    "y = data1.iloc[:, -1]\n",
    "\n",
    "# Split dataset into X_train and y_train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Look at y_train counts pre-SMOTE\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe192f4-3538-455c-813e-aa00688fe120",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a829bd49-b3b8-4735-bab0-3eb16a1e28d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12\n",
       "1.0    69\n",
       "0.0    69\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SMOTE to balance species classes\n",
    "smote=SMOTE(sampling_strategy='minority')  # generating synthetic samples for minority class\n",
    "\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train) # re-sampling\n",
    "\n",
    "y_train.value_counts() # Looking at counts post-SMOTE algoirthm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b10c7-7a64-401b-9790-608020a36a63",
   "metadata": {},
   "source": [
    "# Github SMOTE Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dc1e2b1-89b7-4102-9e9b-470a4337ac95",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1825833475.py, line 409)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[52], line 409\u001b[1;36m\u001b[0m\n\u001b[1;33m    )\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Base class and original SMOTE methods for over-sampling\"\"\"\n",
    "\n",
    "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
    "#          Fernando Nogueira\n",
    "#          Christos Aridas\n",
    "#          Dzianis Dudnik\n",
    "# License: MIT\n",
    "\n",
    "import math\n",
    "import numbers\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from scipy import sparse\n",
    "from sklearn.base import clone\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.utils import (\n",
    "    _safe_indexing,\n",
    "    check_array,\n",
    "    check_random_state,\n",
    ")\n",
    "from sklearn.utils.fixes import parse_version\n",
    "from sklearn.utils.sparsefuncs_fast import (\n",
    "    csr_mean_variance_axis0,\n",
    ")\n",
    "from sklearn.utils.validation import _num_features\n",
    "\n",
    "from ...metrics.pairwise import ValueDifferenceMetric\n",
    "from ...utils import Substitution, check_neighbors_object, check_target_type\n",
    "from ...utils._docstring import _n_jobs_docstring, _random_state_docstring\n",
    "from ...utils._param_validation import HasMethods, Interval, StrOptions\n",
    "from ...utils._validation import _check_X\n",
    "from ...utils.fixes import _is_pandas_df, _mode\n",
    "from ..base import BaseOverSampler\n",
    "\n",
    "sklearn_version = parse_version(sklearn.__version__).base_version\n",
    "if parse_version(sklearn_version) < parse_version(\"1.5\"):\n",
    "    from sklearn.utils import _get_column_indices\n",
    "else:\n",
    "    from sklearn.utils._indexing import _get_column_indices\n",
    "\n",
    "\n",
    "class BaseSMOTE(BaseOverSampler):\n",
    "    \"\"\"Base class for the different SMOTE algorithms.\"\"\"\n",
    "\n",
    "    _parameter_constraints: dict = {\n",
    "        **BaseOverSampler._parameter_constraints,\n",
    "        \"k_neighbors\": [\n",
    "            Interval(numbers.Integral, 1, None, closed=\"left\"),\n",
    "            HasMethods([\"kneighbors\", \"kneighbors_graph\"]),\n",
    "        ],\n",
    "        \"n_jobs\": [numbers.Integral, None],\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_strategy=\"auto\",\n",
    "        random_state=None,\n",
    "        k_neighbors=5,\n",
    "        n_jobs=None,\n",
    "    ):\n",
    "        super().__init__(sampling_strategy=sampling_strategy)\n",
    "        self.random_state = random_state\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        \"\"\"Check the NN estimators shared across the different SMOTE\n",
    "        algorithms.\n",
    "        \"\"\"\n",
    "        self.nn_k_ = check_neighbors_object(\n",
    "            \"k_neighbors\", self.k_neighbors, additional_neighbor=1\n",
    "        )\n",
    "\n",
    "    def _make_samples(\n",
    "        self, X, y_dtype, y_type, nn_data, nn_num, n_samples, step_size=1.0, y=None\n",
    "    ):\n",
    "        \"\"\"A support function that returns artificial samples constructed along\n",
    "        the line connecting nearest neighbours.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Points from which the points will be created.\n",
    "\n",
    "        y_dtype : dtype\n",
    "            The data type of the targets.\n",
    "\n",
    "        y_type : str or int\n",
    "            The minority target value, just so the function can return the\n",
    "            target values for the synthetic variables with correct length in\n",
    "            a clear format.\n",
    "\n",
    "        nn_data : ndarray of shape (n_samples_all, n_features)\n",
    "            Data set carrying all the neighbours to be used\n",
    "\n",
    "        nn_num : ndarray of shape (n_samples_all, k_nearest_neighbours)\n",
    "            The nearest neighbours of each sample in `nn_data`.\n",
    "\n",
    "        n_samples : int\n",
    "            The number of samples to generate.\n",
    "\n",
    "        step_size : float, default=1.0\n",
    "            The step size to create samples.\n",
    "\n",
    "        y : ndarray of shape (n_samples_all,), default=None\n",
    "            The true target associated with `nn_data`. Used by Borderline SMOTE-2 to\n",
    "            weight the distances in the sample generation process.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : {ndarray, sparse matrix} of shape (n_samples_new, n_features)\n",
    "            Synthetically generated samples.\n",
    "\n",
    "        y_new : ndarray of shape (n_samples_new,)\n",
    "            Target values for synthetic samples.\n",
    "        \"\"\"\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        samples_indices = random_state.randint(low=0, high=nn_num.size, size=n_samples)\n",
    "\n",
    "        # np.newaxis for backwards compatability with random_state\n",
    "        steps = step_size * random_state.uniform(size=n_samples)[:, np.newaxis]\n",
    "        rows = np.floor_divide(samples_indices, nn_num.shape[1])\n",
    "        cols = np.mod(samples_indices, nn_num.shape[1])\n",
    "\n",
    "        X_new = self._generate_samples(X, nn_data, nn_num, rows, cols, steps, y_type, y)\n",
    "        y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)\n",
    "        return X_new, y_new\n",
    "\n",
    "    def _generate_samples(\n",
    "        self, X, nn_data, nn_num, rows, cols, steps, y_type=None, y=None\n",
    "    ):\n",
    "        r\"\"\"Generate a synthetic sample.\n",
    "\n",
    "        The rule for the generation is:\n",
    "\n",
    "        .. math::\n",
    "           \\mathbf{s_{s}} = \\mathbf{s_{i}} + \\mathcal{u}(0, 1) \\times\n",
    "           (\\mathbf{s_{i}} - \\mathbf{s_{nn}}) \\,\n",
    "\n",
    "        where \\mathbf{s_{s}} is the new synthetic samples, \\mathbf{s_{i}} is\n",
    "        the current sample, \\mathbf{s_{nn}} is a randomly selected neighbors of\n",
    "        \\mathbf{s_{i}} and \\mathcal{u}(0, 1) is a random number between [0, 1).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Points from which the points will be created.\n",
    "\n",
    "        nn_data : ndarray of shape (n_samples_all, n_features)\n",
    "            Data set carrying all the neighbours to be used.\n",
    "\n",
    "        nn_num : ndarray of shape (n_samples_all, k_nearest_neighbours)\n",
    "            The nearest neighbours of each sample in `nn_data`.\n",
    "\n",
    "        rows : ndarray of shape (n_samples,), dtype=int\n",
    "            Indices pointing at feature vector in X which will be used\n",
    "            as a base for creating new samples.\n",
    "\n",
    "        cols : ndarray of shape (n_samples,), dtype=int\n",
    "            Indices pointing at which nearest neighbor of base feature vector\n",
    "            will be used when creating new samples.\n",
    "\n",
    "        steps : ndarray of shape (n_samples,), dtype=float\n",
    "            Step sizes for new samples.\n",
    "\n",
    "        y_type : str, int or None, default=None\n",
    "            Class label of the current target classes for which we want to generate\n",
    "            samples.\n",
    "\n",
    "        y : ndarray of shape (n_samples_all,), default=None\n",
    "            The true target associated with `nn_data`. Used by Borderline SMOTE-2 to\n",
    "            weight the distances in the sample generation process.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
    "            Synthetically generated samples.\n",
    "        \"\"\"\n",
    "        diffs = nn_data[nn_num[rows, cols]] - X[rows]\n",
    "        if y is not None:  # only entering for BorderlineSMOTE-2\n",
    "            random_state = check_random_state(self.random_state)\n",
    "            mask_pair_samples = y[nn_num[rows, cols]] != y_type\n",
    "            diffs[mask_pair_samples] *= random_state.uniform(\n",
    "                low=0.0, high=0.5, size=(mask_pair_samples.sum(), 1)\n",
    "            )\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            sparse_func = type(X).__name__\n",
    "            steps = getattr(sparse, sparse_func)(steps)\n",
    "            X_new = X[rows] + steps.multiply(diffs)\n",
    "        else:\n",
    "            X_new = X[rows] + steps * diffs\n",
    "\n",
    "        return X_new.astype(X.dtype)\n",
    "\n",
    "    def _in_danger_noise(self, nn_estimator, samples, target_class, y, kind=\"danger\"):\n",
    "        \"\"\"Estimate if a set of sample are in danger or noise.\n",
    "\n",
    "        Used by BorderlineSMOTE and SVMSMOTE.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nn_estimator : estimator object\n",
    "            An estimator that inherits from\n",
    "            :class:`~sklearn.neighbors.base.KNeighborsMixin` use to determine\n",
    "            if a sample is in danger/noise.\n",
    "\n",
    "        samples : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The samples to check if either they are in danger or not.\n",
    "\n",
    "        target_class : int or str\n",
    "            The target corresponding class being over-sampled.\n",
    "\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The true label in order to check the neighbour labels.\n",
    "\n",
    "        kind : {'danger', 'noise'}, default='danger'\n",
    "            The type of classification to use. Can be either:\n",
    "\n",
    "            - If 'danger', check if samples are in danger,\n",
    "            - If 'noise', check if samples are noise.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output : ndarray of shape (n_samples,)\n",
    "            A boolean array where True refer to samples in danger or noise.\n",
    "        \"\"\"\n",
    "        x = nn_estimator.kneighbors(samples, return_distance=False)[:, 1:]\n",
    "        nn_label = (y[x] != target_class).astype(int)\n",
    "        n_maj = np.sum(nn_label, axis=1)\n",
    "\n",
    "        if kind == \"danger\":\n",
    "            # Samples are in danger for m/2 <= m' < m\n",
    "            return np.bitwise_and(\n",
    "                n_maj >= (nn_estimator.n_neighbors - 1) / 2,\n",
    "                n_maj < nn_estimator.n_neighbors - 1,\n",
    "            )\n",
    "        else:  # kind == \"noise\":\n",
    "            # Samples are noise for m = m'\n",
    "            return n_maj == nn_estimator.n_neighbors - 1\n",
    "\n",
    "\n",
    "@Substitution(\n",
    "    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,\n",
    "    n_jobs=_n_jobs_docstring,\n",
    "    random_state=_random_state_docstring,\n",
    ")\n",
    "class SMOTE(BaseSMOTE):\n",
    "    \"\"\"Class to perform over-sampling using SMOTE.\n",
    "\n",
    "    This object is an implementation of SMOTE - Synthetic Minority\n",
    "    Over-sampling Technique as presented in [1]_.\n",
    "\n",
    "    Read more in the :ref:`User Guide <smote_adasyn>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    {sampling_strategy}\n",
    "\n",
    "    {random_state}\n",
    "\n",
    "    k_neighbors : int or object, default=5\n",
    "        The nearest neighbors used to define the neighborhood of samples to use\n",
    "        to generate the synthetic samples. You can pass:\n",
    "\n",
    "        - an `int` corresponding to the number of neighbors to use. A\n",
    "          `~sklearn.neighbors.NearestNeighbors` instance will be fitted in this\n",
    "          case.\n",
    "        - an instance of a compatible nearest neighbors algorithm that should\n",
    "          implement both methods `kneighbors` and `kneighbors_graph`. For\n",
    "          instance, it could correspond to a\n",
    "          :class:`~sklearn.neighbors.NearestNeighbors` but could be extended to\n",
    "          any compatible class.\n",
    "\n",
    "    {n_jobs}\n",
    "\n",
    "        .. deprecated:: 0.10\n",
    "           `n_jobs` has been deprecated in 0.10 and will be removed in 0.12.\n",
    "           It was previously used to set `n_jobs` of nearest neighbors\n",
    "           algorithm. From now on, you can pass an estimator where `n_jobs` is\n",
    "           already set instead.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    sampling_strategy_ : dict\n",
    "        Dictionary containing the information to sample the dataset. The keys\n",
    "        corresponds to the class labels from which to sample and the values\n",
    "        are the number of samples to sample.\n",
    "\n",
    "    nn_k_ : estimator object\n",
    "        Validated k-nearest neighbours created from the `k_neighbors` parameter.\n",
    "\n",
    "    n_features_in_ : int\n",
    "        Number of features in the input dataset.\n",
    "\n",
    "        .. versionadded:: 0.9\n",
    "\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during `fit`. Defined only when `X` has feature\n",
    "        names that are all strings.\n",
    "\n",
    "        .. versionadded:: 0.10\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    SMOTENC : Over-sample using SMOTE for continuous and categorical features.\n",
    "\n",
    "    SMOTEN : Over-sample using the SMOTE variant specifically for categorical\n",
    "        features only.\n",
    "\n",
    "    BorderlineSMOTE : Over-sample using the borderline-SMOTE variant.\n",
    "\n",
    "    SVMSMOTE : Over-sample using the SVM-SMOTE variant.\n",
    "\n",
    "    ADASYN : Over-sample using ADASYN.\n",
    "\n",
    "    KMeansSMOTE : Over-sample applying a clustering before to oversample using\n",
    "        SMOTE.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See the original papers: [1]_ for more details.\n",
    "\n",
    "    Supports multi-class resampling. A one-vs.-rest scheme is used as\n",
    "    originally proposed in [1]_.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, \"SMOTE:\n",
    "       synthetic minority over-sampling technique,\" Journal of artificial\n",
    "       intelligence research, 321-357, 2002.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from collections import Counter\n",
    "    >>> from sklearn.datasets import make_classification\n",
    "    >>> from imblearn.over_sampling import SMOTE\n",
    "    >>> X, y = make_classification(n_classes=2, class_sep=2,\n",
    "    ... weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "    ... n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "    >>> print('Original dataset shape %s' % Counter(y))\n",
    "    Original dataset shape Counter({{1: 900, 0: 100}})\n",
    "    >>> sm = SMOTE(random_state=42)\n",
    "    >>> X_res, y_res = sm.fit_resample(X, y)\n",
    "    >>> print('Resampled dataset shape %s' % Counter(y_res))\n",
    "    Resampled dataset shape Counter({{0: 900, 1: 900}})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        sampling_strategy=\"auto\",\n",
    "        random_state=None,\n",
    "        k_neighbors=5,\n",
    "        n_jobs=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            sampling_strategy=sampling_strategy,\n",
    "            random_state=random_state,\n",
    "            k_neighbors=k_neighbors,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        # FIXME: to be removed in 0.12\n",
    "        if self.n_jobs is not None:\n",
    "            warnings.warn(\n",
    "                \"The parameter `n_jobs` has been deprecated in 0.10 and will be \"\n",
    "                \"removed in 0.12. You can pass an nearest neighbors estimator where \"\n",
    "                \"`n_jobs` is already set instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "\n",
    "        self._validate_estimator()\n",
    "\n",
    "        X_resampled = [X.copy()]\n",
    "        y_resampled = [y.copy()]\n",
    "\n",
    "        for class_sample, n_samples in self.sampling_strategy_.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = _safe_indexing(X, target_class_indices)\n",
    "\n",
    "            self.nn_k_.fit(X_class)\n",
    "            nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "            X_new, y_new = self._make_samples(\n",
    "                X_class, y.dtype, class_sample, X_class, nns, n_samples, 1.0\n",
    "            )\n",
    "            X_resampled.append(X_new)\n",
    "            y_resampled.append(y_new)\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            X_resampled = sparse.vstack(X_resampled, format=X.format)\n",
    "        else:\n",
    "            X_resampled = np.vstack(X_resampled)\n",
    "        y_resampled = np.hstack(y_resampled)\n",
    "\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "@Substitution(\n",
    "    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,\n",
    "    n_jobs=_n_jobs_docstring,\n",
    "    random_state=_random_state_docstring,\n",
    ")\n",
    "class SMOTENC(SMOTE):\n",
    "    \"\"\"Synthetic Minority Over-sampling Technique for Nominal and Continuous.\n",
    "\n",
    "    Unlike :class:`SMOTE`, SMOTE-NC for dataset containing numerical and\n",
    "    categorical features. However, it is not designed to work with only\n",
    "    categorical features.\n",
    "\n",
    "    Read more in the :ref:`User Guide <smote_adasyn>`.\n",
    "\n",
    "    .. versionadded:: 0.4\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    categorical_features : \"infer\" or array-like of shape (n_cat_features,) or \\\n",
    "            (n_features,), dtype={{bool, int, str}}\n",
    "        Specified which features are categorical. Can either be:\n",
    "\n",
    "        - \"auto\" (default) to automatically detect categorical features. Only\n",
    "          supported when `X` is a :class:`pandas.DataFrame` and it corresponds\n",
    "          to columns that have a :class:`pandas.CategoricalDtype`;\n",
    "        - array of `int` corresponding to the indices specifying the categorical\n",
    "          features;\n",
    "        - array of `str` corresponding to the feature names. `X` should be a pandas\n",
    "          :class:`pandas.DataFrame` in this case.\n",
    "        - mask array of shape (n_features, ) and ``bool`` dtype for which\n",
    "          ``True`` indicates the categorical features.\n",
    "\n",
    "    categorical_encoder : estimator, default=None\n",
    "        One-hot encoder used to encode the categorical features. If `None`, a\n",
    "        :class:`~sklearn.preprocessing.OneHotEncoder` is used with default parameters\n",
    "        apart from `handle_unknown` which is set to 'ignore'.\n",
    "\n",
    "    {sampling_strategy}\n",
    "\n",
    "    {random_state}\n",
    "\n",
    "    k_neighbors : int or object, default=5\n",
    "        The nearest neighbors used to define the neighborhood of samples to use\n",
    "        to generate the synthetic samples. You can pass:\n",
    "\n",
    "        - an `int` corresponding to the number of neighbors to use. A\n",
    "          `~sklearn.neighbors.NearestNeighbors` instance will be fitted in this\n",
    "          case.\n",
    "        - an instance of a compatible nearest neighbors algorithm that should\n",
    "          implement both methods `kneighbors` and `kneighbors_graph`. For\n",
    "          instance, it could correspond to a\n",
    "          :class:`~sklearn.neighbors.NearestNeighbors` but could be extended to\n",
    "          any compatible class.\n",
    "\n",
    "    {n_jobs}\n",
    "\n",
    "        .. deprecated:: 0.10\n",
    "           `n_jobs` has been deprecated in 0.10 and will be removed in 0.12.\n",
    "           It was previously used to set `n_jobs` of nearest neighbors\n",
    "           algorithm. From now on, you can pass an estimator where `n_jobs` is\n",
    "           already set instead.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    sampling_strategy_ : dict\n",
    "        Dictionary containing the information to sample the dataset. The keys\n",
    "        corresponds to the class labels from which to sample and the values\n",
    "        are the number of samples to sample.\n",
    "\n",
    "    nn_k_ : estimator object\n",
    "        Validated k-nearest neighbours created from the `k_neighbors` parameter.\n",
    "\n",
    "    ohe_ : :class:`~sklearn.preprocessing.OneHotEncoder`\n",
    "        The one-hot encoder used to encode the categorical features.\n",
    "\n",
    "        .. deprecated:: 0.11\n",
    "           `ohe_` is deprecated in 0.11 and will be removed in 0.13. Use\n",
    "           `categorical_encoder_` instead.\n",
    "\n",
    "    categorical_encoder_ : estimator\n",
    "        The encoder used to encode the categorical features.\n",
    "\n",
    "    categorical_features_ : ndarray of shape (n_cat_features,), dtype=np.int64\n",
    "        Indices of the categorical features.\n",
    "\n",
    "    continuous_features_ : ndarray of shape (n_cont_features,), dtype=np.int64\n",
    "        Indices of the continuous features.\n",
    "\n",
    "    median_std_ : dict of int -> float\n",
    "        Median of the standard deviation of the continuous features for each\n",
    "        class to be over-sampled.\n",
    "\n",
    "    n_features_ : int\n",
    "        Number of features observed at `fit`.\n",
    "\n",
    "    n_features_in_ : int\n",
    "        Number of features in the input dataset.\n",
    "\n",
    "        .. versionadded:: 0.9\n",
    "\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during `fit`. Defined only when `X` has feature\n",
    "        names that are all strings.\n",
    "\n",
    "        .. versionadded:: 0.10\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    SMOTE : Over-sample using SMOTE.\n",
    "\n",
    "    SMOTEN : Over-sample using the SMOTE variant specifically for categorical\n",
    "        features only.\n",
    "\n",
    "    SVMSMOTE : Over-sample using SVM-SMOTE variant.\n",
    "\n",
    "    BorderlineSMOTE : Over-sample using Borderline-SMOTE variant.\n",
    "\n",
    "    ADASYN : Over-sample using ADASYN.\n",
    "\n",
    "    KMeansSMOTE : Over-sample applying a clustering before to oversample using\n",
    "        SMOTE.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See the original paper [1]_ for more details.\n",
    "\n",
    "    Supports multi-class resampling. A one-vs.-rest scheme is used as\n",
    "    originally proposed in [1]_.\n",
    "\n",
    "    See\n",
    "    :ref:`sphx_glr_auto_examples_over-sampling_plot_comparison_over_sampling.py`,\n",
    "    and\n",
    "    :ref:`sphx_glr_auto_examples_over-sampling_plot_illustration_generation_sample.py`.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, \"SMOTE:\n",
    "       synthetic minority over-sampling technique,\" Journal of artificial\n",
    "       intelligence research, 321-357, 2002.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from collections import Counter\n",
    "    >>> from numpy.random import RandomState\n",
    "    >>> from sklearn.datasets import make_classification\n",
    "    >>> from imblearn.over_sampling import SMOTENC\n",
    "    >>> X, y = make_classification(n_classes=2, class_sep=2,\n",
    "    ... weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "    ... n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "    >>> print(f'Original dataset shape {{X.shape}}')\n",
    "    Original dataset shape (1000, 20)\n",
    "    >>> print(f'Original dataset samples per class {{Counter(y)}}')\n",
    "    Original dataset samples per class Counter({{1: 900, 0: 100}})\n",
    "    >>> # simulate the 2 last columns to be categorical features\n",
    "    >>> X[:, -2:] = RandomState(10).randint(0, 4, size=(1000, 2))\n",
    "    >>> sm = SMOTENC(random_state=42, categorical_features=[18, 19])\n",
    "    >>> X_res, y_res = sm.fit_resample(X, y)\n",
    "    >>> print(f'Resampled dataset samples per class {{Counter(y_res)}}')\n",
    "    Resampled dataset samples per class Counter({{0: 900, 1: 900}})\n",
    "    \"\"\"\n",
    "\n",
    "    _required_parameters = [\"categorical_features\"]\n",
    "\n",
    "    _parameter_constraints: dict = {\n",
    "        **SMOTE._parameter_constraints,\n",
    "        \"categorical_features\": [\"array-like\", StrOptions({\"auto\"})],\n",
    "        \"categorical_encoder\": [\n",
    "            HasMethods([\"fit_transform\", \"inverse_transform\"]),\n",
    "            None,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_features,\n",
    "        *,\n",
    "        categorical_encoder=None,\n",
    "        sampling_strategy=\"auto\",\n",
    "        random_state=None,\n",
    "        k_neighbors=5,\n",
    "        n_jobs=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            sampling_strategy=sampling_strategy,\n",
    "            random_state=random_state,\n",
    "            k_neighbors=k_neighbors,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "        self.categorical_features = categorical_features\n",
    "        self.categorical_encoder = categorical_encoder\n",
    "\n",
    "    def _check_X_y(self, X, y):\n",
    "        \"\"\"Overwrite the checking to let pass some string for categorical\n",
    "        features.\n",
    "        \"\"\"\n",
    "        y, binarize_y = check_target_type(y, indicate_one_vs_all=True)\n",
    "        X = _check_X(X)\n",
    "        self._check_n_features(X, reset=True)\n",
    "        self._check_feature_names(X, reset=True)\n",
    "        return X, y, binarize_y\n",
    "\n",
    "    def _validate_column_types(self, X):\n",
    "        \"\"\"Compute the indices of the categorical and continuous features.\"\"\"\n",
    "        if self.categorical_features == \"auto\":\n",
    "            if not _is_pandas_df(X):\n",
    "                raise ValueError(\n",
    "                    \"When `categorical_features='auto'`, the input data \"\n",
    "                    f\"should be a pandas.DataFrame. Got {type(X)} instead.\"\n",
    "                )\n",
    "            import pandas as pd  # safely import pandas now\n",
    "\n",
    "            are_columns_categorical = np.array(\n",
    "                [isinstance(col_dtype, pd.CategoricalDtype) for col_dtype in X.dtypes]\n",
    "            )\n",
    "            self.categorical_features_ = np.flatnonzero(are_columns_categorical)\n",
    "            self.continuous_features_ = np.flatnonzero(~are_columns_categorical)\n",
    "        else:\n",
    "            self.categorical_features_ = np.array(\n",
    "                _get_column_indices(X, self.categorical_features)\n",
    "            )\n",
    "            self.continuous_features_ = np.setdiff1d(\n",
    "                np.arange(self.n_features_), self.categorical_features_\n",
    "            )\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        super()._validate_estimator()\n",
    "        if self.categorical_features_.size == self.n_features_in_:\n",
    "            raise ValueError(\n",
    "                \"SMOTE-NC is not designed to work only with categorical \"\n",
    "                \"features. It requires some numerical features.\"\n",
    "            )\n",
    "        elif self.categorical_features_.size == 0:\n",
    "            raise ValueError(\n",
    "                \"SMOTE-NC is not designed to work only with numerical \"\n",
    "                \"features. It requires some categorical features.\"\n",
    "            )\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        # FIXME: to be removed in 0.12\n",
    "        if self.n_jobs is not None:\n",
    "            warnings.warn(\n",
    "                \"The parameter `n_jobs` has been deprecated in 0.10 and will be \"\n",
    "                \"removed in 0.12. You can pass an nearest neighbors estimator where \"\n",
    "                \"`n_jobs` is already set instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "\n",
    "        self.n_features_ = _num_features(X)\n",
    "        self._validate_column_types(X)\n",
    "        self._validate_estimator()\n",
    "\n",
    "        X_continuous = _safe_indexing(X, self.continuous_features_, axis=1)\n",
    "        X_continuous = check_array(X_continuous, accept_sparse=[\"csr\", \"csc\"])\n",
    "        X_categorical = _safe_indexing(X, self.categorical_features_, axis=1)\n",
    "        if X_continuous.dtype.name != \"object\":\n",
    "            dtype_ohe = X_continuous.dtype\n",
    "        else:\n",
    "            dtype_ohe = np.float64\n",
    "\n",
    "        if self.categorical_encoder is None:\n",
    "            self.categorical_encoder_ = OneHotEncoder(\n",
    "                handle_unknown=\"ignore\", dtype=dtype_ohe\n",
    "            )\n",
    "        else:\n",
    "            self.categorical_encoder_ = clone(self.categorical_encoder)\n",
    "\n",
    "        # the input of the OneHotEncoder needs to be dense\n",
    "        X_ohe = self.categorical_encoder_.fit_transform(\n",
    "            X_categorical.toarray() if sparse.issparse(X_categorical) else X_categorical\n",
    "        )\n",
    "        if not sparse.issparse(X_ohe):\n",
    "            X_ohe = sparse.csr_matrix(X_ohe, dtype=dtype_ohe)\n",
    "\n",
    "        X_encoded = sparse.hstack((X_continuous, X_ohe), format=\"csr\", dtype=dtype_ohe)\n",
    "        X_resampled = [X_encoded.copy()]\n",
    "        y_resampled = [y.copy()]\n",
    "\n",
    "        # SMOTE resampling starts here\n",
    "        self.median_std_ = {}\n",
    "        for class_sample, n_samples in self.sampling_strategy_.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = _safe_indexing(X_encoded, target_class_indices)\n",
    "\n",
    "            _, var = csr_mean_variance_axis0(\n",
    "                X_class[:, : self.continuous_features_.size]\n",
    "            )\n",
    "            self.median_std_[class_sample] = np.median(np.sqrt(var))\n",
    "\n",
    "            # In the edge case where the median of the std is equal to 0, the 1s\n",
    "            # entries will be also nullified. In this case, we store the original\n",
    "            # categorical encoding which will be later used for inverting the OHE\n",
    "            if math.isclose(self.median_std_[class_sample], 0):\n",
    "                # This variable will be used when generating data\n",
    "                self._X_categorical_minority_encoded = X_class[\n",
    "                    :, self.continuous_features_.size :\n",
    "                ].toarray()\n",
    "\n",
    "            # we can replace the 1 entries of the categorical features with the\n",
    "            # median of the standard deviation. It will ensure that whenever\n",
    "            # distance is computed between 2 samples, the difference will be equal\n",
    "            # to the median of the standard deviation as in the original paper.\n",
    "            X_class_categorical = X_class[:, self.continuous_features_.size :]\n",
    "            # With one-hot encoding, the median will be repeated twice. We need\n",
    "            # to divide by sqrt(2) such that we only have one median value\n",
    "            # contributing to the Euclidean distance\n",
    "            X_class_categorical.data[:] = self.median_std_[class_sample] / np.sqrt(2)\n",
    "            X_class[:, self.continuous_features_.size :] = X_class_categorical\n",
    "\n",
    "            self.nn_k_.fit(X_class)\n",
    "            nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "            X_new, y_new = self._make_samples(\n",
    "                X_class, y.dtype, class_sample, X_class, nns, n_samples, 1.0\n",
    "            )\n",
    "            X_resampled.append(X_new)\n",
    "            y_resampled.append(y_new)\n",
    "\n",
    "        X_resampled = sparse.vstack(X_resampled, format=X_encoded.format)\n",
    "        y_resampled = np.hstack(y_resampled)\n",
    "        # SMOTE resampling ends here\n",
    "\n",
    "        # reverse the encoding of the categorical features\n",
    "        X_res_cat = X_resampled[:, self.continuous_features_.size :]\n",
    "        X_res_cat.data = np.ones_like(X_res_cat.data)\n",
    "        X_res_cat_dec = self.categorical_encoder_.inverse_transform(X_res_cat)\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            X_resampled = sparse.hstack(\n",
    "                (\n",
    "                    X_resampled[:, : self.continuous_features_.size],\n",
    "                    X_res_cat_dec,\n",
    "                ),\n",
    "                format=\"csr\",\n",
    "            )\n",
    "        else:\n",
    "            X_resampled = np.hstack(\n",
    "                (\n",
    "                    X_resampled[:, : self.continuous_features_.size].toarray(),\n",
    "                    X_res_cat_dec,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        indices_reordered = np.argsort(\n",
    "            np.hstack((self.continuous_features_, self.categorical_features_))\n",
    "        )\n",
    "        if sparse.issparse(X_resampled):\n",
    "            # the matrix is supposed to be in the CSR format after the stacking\n",
    "            col_indices = X_resampled.indices.copy()\n",
    "            for idx, col_idx in enumerate(indices_reordered):\n",
    "                mask = X_resampled.indices == col_idx\n",
    "                col_indices[mask] = idx\n",
    "            X_resampled.indices = col_indices\n",
    "        else:\n",
    "            X_resampled = X_resampled[:, indices_reordered]\n",
    "\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "    def _generate_samples(self, X, nn_data, nn_num, rows, cols, steps, y_type, y=None):\n",
    "        \"\"\"Generate a synthetic sample with an additional steps for the\n",
    "        categorical features.\n",
    "\n",
    "        Each new sample is generated the same way than in SMOTE. However, the\n",
    "        categorical features are mapped to the most frequent nearest neighbors\n",
    "        of the majority class.\n",
    "        \"\"\"\n",
    "        rng = check_random_state(self.random_state)\n",
    "        X_new = super()._generate_samples(X, nn_data, nn_num, rows, cols, steps)\n",
    "        # change in sparsity structure more efficient with LIL than CSR\n",
    "        X_new = X_new.tolil() if sparse.issparse(X_new) else X_new\n",
    "\n",
    "        # convert to dense array since scipy.sparse doesn't handle 3D\n",
    "        nn_data = nn_data.toarray() if sparse.issparse(nn_data) else nn_data\n",
    "\n",
    "        # In the case that the median std was equal to zeros, we have to\n",
    "        # create non-null entry based on the encoded of OHE\n",
    "        if math.isclose(self.median_std_[y_type], 0):\n",
    "            nn_data[\n",
    "                :, self.continuous_features_.size :\n",
    "            ] = self._X_categorical_minority_encoded\n",
    "\n",
    "        all_neighbors = nn_data[nn_num[rows]]\n",
    "\n",
    "        categories_size = [self.continuous_features_.size] + [\n",
    "            cat.size for cat in self.categorical_encoder_.categories_\n",
    "        ]\n",
    "\n",
    "        for start_idx, end_idx in zip(\n",
    "            np.cumsum(categories_size)[:-1], np.cumsum(categories_size)[1:]\n",
    "        ):\n",
    "            col_maxs = all_neighbors[:, :, start_idx:end_idx].sum(axis=1)\n",
    "            # tie breaking argmax\n",
    "            is_max = np.isclose(col_maxs, col_maxs.max(axis=1, keepdims=True))\n",
    "            max_idxs = rng.permutation(np.argwhere(is_max))\n",
    "            xs, idx_sels = np.unique(max_idxs[:, 0], return_index=True)\n",
    "            col_sels = max_idxs[idx_sels, 1]\n",
    "\n",
    "            ys = start_idx + col_sels\n",
    "            X_new[:, start_idx:end_idx] = 0\n",
    "            X_new[xs, ys] = 1\n",
    "\n",
    "        return X_new\n",
    "\n",
    "    @property\n",
    "    def ohe_(self):\n",
    "        \"\"\"One-hot encoder used to encode the categorical features.\"\"\"\n",
    "        warnings.warn(\n",
    "            \"'ohe_' attribute has been deprecated in 0.11 and will be removed \"\n",
    "            \"in 0.13. Use 'categorical_encoder_' instead.\",\n",
    "            FutureWarning,\n",
    "        )\n",
    "        return self.categorical_encoder_\n",
    "\n",
    "\n",
    "@Substitution(\n",
    "    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,\n",
    "    n_jobs=_n_jobs_docstring,\n",
    "    random_state=_random_state_docstring,\n",
    ")\n",
    "class SMOTEN(SMOTE):\n",
    "    \"\"\"Synthetic Minority Over-sampling Technique for Nominal.\n",
    "\n",
    "    This method is referred as SMOTEN in [1]_. It expects that the data to\n",
    "    resample are only made of categorical features.\n",
    "\n",
    "    Read more in the :ref:`User Guide <smote_adasyn>`.\n",
    "\n",
    "    .. versionadded:: 0.8\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    categorical_encoder : estimator, default=None\n",
    "        Ordinal encoder used to encode the categorical features. If `None`, a\n",
    "        :class:`~sklearn.preprocessing.OrdinalEncoder` is used with default parameters.\n",
    "\n",
    "    {sampling_strategy}\n",
    "\n",
    "    {random_state}\n",
    "\n",
    "    k_neighbors : int or object, default=5\n",
    "        The nearest neighbors used to define the neighborhood of samples to use\n",
    "        to generate the synthetic samples. You can pass:\n",
    "\n",
    "        - an `int` corresponding to the number of neighbors to use. A\n",
    "          `~sklearn.neighbors.NearestNeighbors` instance will be fitted in this\n",
    "          case.\n",
    "        - an instance of a compatible nearest neighbors algorithm that should\n",
    "          implement both methods `kneighbors` and `kneighbors_graph`. For\n",
    "          instance, it could correspond to a\n",
    "          :class:`~sklearn.neighbors.NearestNeighbors` but could be extended to\n",
    "          any compatible class.\n",
    "\n",
    "    {n_jobs}\n",
    "\n",
    "        .. deprecated:: 0.10\n",
    "           `n_jobs` has been deprecated in 0.10 and will be removed in 0.12.\n",
    "           It was previously used to set `n_jobs` of nearest neighbors\n",
    "           algorithm. From now on, you can pass an estimator where `n_jobs` is\n",
    "           already set instead.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    categorical_encoder_ : estimator\n",
    "        The encoder used to encode the categorical features.\n",
    "\n",
    "    sampling_strategy_ : dict\n",
    "        Dictionary containing the information to sample the dataset. The keys\n",
    "        corresponds to the class labels from which to sample and the values\n",
    "        are the number of samples to sample.\n",
    "\n",
    "    nn_k_ : estimator object\n",
    "        Validated k-nearest neighbours created from the `k_neighbors` parameter.\n",
    "\n",
    "    n_features_in_ : int\n",
    "        Number of features in the input dataset.\n",
    "\n",
    "        .. versionadded:: 0.9\n",
    "\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during `fit`. Defined only when `X` has feature\n",
    "        names that are all strings.\n",
    "\n",
    "        .. versionadded:: 0.10\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    SMOTE : Over-sample using SMOTE.\n",
    "\n",
    "    SMOTENC : Over-sample using SMOTE for continuous and categorical features.\n",
    "\n",
    "    BorderlineSMOTE : Over-sample using the borderline-SMOTE variant.\n",
    "\n",
    "    SVMSMOTE : Over-sample using the SVM-SMOTE variant.\n",
    "\n",
    "    ADASYN : Over-sample using ADASYN.\n",
    "\n",
    "    KMeansSMOTE : Over-sample applying a clustering before to oversample using\n",
    "        SMOTE.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See the original papers: [1]_ for more details.\n",
    "\n",
    "    Supports multi-class resampling. A one-vs.-rest scheme is used as\n",
    "    originally proposed in [1]_.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, \"SMOTE:\n",
    "       synthetic minority over-sampling technique,\" Journal of artificial\n",
    "       intelligence research, 321-357, 2002.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([\"A\"] * 10 + [\"B\"] * 20 + [\"C\"] * 30, dtype=object).reshape(-1, 1)\n",
    "    >>> y = np.array([0] * 20 + [1] * 40, dtype=np.int32)\n",
    "    >>> from collections import Counter\n",
    "    >>> print(f\"Original class counts: {{Counter(y)}}\")\n",
    "    Original class counts: Counter({{1: 40, 0: 20}})\n",
    "    >>> from imblearn.over_sampling import SMOTEN\n",
    "    >>> sampler = SMOTEN(random_state=0)\n",
    "    >>> X_res, y_res = sampler.fit_resample(X, y)\n",
    "    >>> print(f\"Class counts after resampling {{Counter(y_res)}}\")\n",
    "    Class counts after resampling Counter({{0: 40, 1: 40}})\n",
    "    \"\"\"\n",
    "\n",
    "    _parameter_constraints: dict = {\n",
    "        **SMOTE._parameter_constraints,\n",
    "        \"categorical_encoder\": [\n",
    "            HasMethods([\"fit_transform\", \"inverse_transform\"]),\n",
    "            None,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_encoder=None,\n",
    "        *,\n",
    "        sampling_strategy=\"auto\",\n",
    "        random_state=None,\n",
    "        k_neighbors=5,\n",
    "        n_jobs=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            sampling_strategy=sampling_strategy,\n",
    "            random_state=random_state,\n",
    "            k_neighbors=k_neighbors,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "        self.categorical_encoder = categorical_encoder\n",
    "\n",
    "    def _check_X_y(self, X, y):\n",
    "        \"\"\"Check should accept strings and not sparse matrices.\"\"\"\n",
    "        y, binarize_y = check_target_type(y, indicate_one_vs_all=True)\n",
    "        X, y = self._validate_data(\n",
    "            X,\n",
    "            y,\n",
    "            reset=True,\n",
    "            dtype=None,\n",
    "            accept_sparse=[\"csr\", \"csc\"],\n",
    "        )\n",
    "        return X, y, binarize_y\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        \"\"\"Force to use precomputed distance matrix.\"\"\"\n",
    "        super()._validate_estimator()\n",
    "        self.nn_k_.set_params(metric=\"precomputed\")\n",
    "\n",
    "    def _make_samples(self, X_class, klass, y_dtype, nn_indices, n_samples):\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        # generate sample indices that will be used to generate new samples\n",
    "        samples_indices = random_state.choice(\n",
    "            np.arange(X_class.shape[0]), size=n_samples, replace=True\n",
    "        )\n",
    "        # for each drawn samples, select its k-neighbors and generate a sample\n",
    "        # where for each feature individually, each category generated is the\n",
    "        # most common category\n",
    "        X_new = np.squeeze(\n",
    "            _mode(X_class[nn_indices[samples_indices]], axis=1).mode, axis=1\n",
    "        )\n",
    "        y_new = np.full(n_samples, fill_value=klass, dtype=y_dtype)\n",
    "        return X_new, y_new\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        # FIXME: to be removed in 0.12\n",
    "        if self.n_jobs is not None:\n",
    "            warnings.warn(\n",
    "                \"The parameter `n_jobs` has been deprecated in 0.10 and will be \"\n",
    "                \"removed in 0.12. You can pass an nearest neighbors estimator where \"\n",
    "                \"`n_jobs` is already set instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            X_sparse_format = X.format\n",
    "            X = X.toarray()\n",
    "            warnings.warn(\n",
    "                \"Passing a sparse matrix to SMOTEN is not really efficient since it is\"\n",
    "                \" converted to a dense array internally.\",\n",
    "                DataConversionWarning,\n",
    "            )\n",
    "        else:\n",
    "            X_sparse_format = None\n",
    "\n",
    "        self._validate_estimator()\n",
    "\n",
    "        X_resampled = [X.copy()]\n",
    "        y_resampled = [y.copy()]\n",
    "\n",
    "        if self.categorical_encoder is None:\n",
    "            self.categorical_encoder_ = OrdinalEncoder(dtype=np.int32)\n",
    "        else:\n",
    "            self.categorical_encoder_ = clone(self.categorical_encoder)\n",
    "        X_encoded = self.categorical_encoder_.fit_transform(X)\n",
    "\n",
    "        vdm = ValueDifferenceMetric(\n",
    "            n_categories=[len(cat) for cat in self.categorical_encoder_.categories_]\n",
    "        ).fit(X_encoded, y)\n",
    "\n",
    "        for class_sample, n_samples in self.sampling_strategy_.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = _safe_indexing(X_encoded, target_class_indices)\n",
    "\n",
    "            X_class_dist = vdm.pairwise(X_class)\n",
    "            self.nn_k_.fit(X_class_dist)\n",
    "            # the kneigbors search will include the sample itself which is\n",
    "            # expected from the original algorithm\n",
    "            nn_indices = self.nn_k_.kneighbors(X_class_dist, return_distance=False)\n",
    "            X_new, y_new = self._make_samples(\n",
    "                X_class, class_sample, y.dtype, nn_indices, n_samples\n",
    "            )\n",
    "\n",
    "            X_new = self.categorical_encoder_.inverse_transform(X_new)\n",
    "            X_resampled.append(X_new)\n",
    "            y_resampled.append(y_new)\n",
    "\n",
    "        X_resampled = np.vstack(X_resampled)\n",
    "        y_resampled = np.hstack(y_resampled)\n",
    "\n",
    "        if X_sparse_format == \"csr\":\n",
    "            return sparse.csr_matrix(X_resampled), y_resampled\n",
    "        elif X_sparse_format == \"csc\":\n",
    "            return sparse.csc_matrix(X_resampled), y_resampled\n",
    "        else:\n",
    "            return X_resampled, y_resampled\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"X_types\": [\"2darray\", \"dataframe\", \"string\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d11ad-f2ab-4065-b415-d0e1d745ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_samples(\n",
    "    self, X, y_dtype, y_type, nn_data, nn_num, n_samples, step_size=1.0, y=None\n",
    "):\n",
    "    \"\"\"A support function that returns artificial samples constructed along\n",
    "    the line connecting nearest neighbours.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "        Points from which the points will be created.\n",
    "\n",
    "    y_dtype : dtype\n",
    "        The data type of the targets.\n",
    "\n",
    "    y_type : str or int\n",
    "        The minority target value, just so the function can return the\n",
    "        target values for the synthetic variables with correct length in\n",
    "        a clear format.\n",
    "\n",
    "    nn_data : ndarray of shape (n_samples_all, n_features)\n",
    "        Data set carrying all the neighbours to be used\n",
    "\n",
    "    nn_num : ndarray of shape (n_samples_all, k_nearest_neighbours)\n",
    "        The nearest neighbours of each sample in `nn_data`.\n",
    "\n",
    "    n_samples : int\n",
    "        The number of samples to generate.\n",
    "\n",
    "    step_size : float, default=1.0\n",
    "        The step size to create samples.\n",
    "\n",
    "    y : ndarray of shape (n_samples_all,), default=None\n",
    "        The true target associated with `nn_data`. Used by Borderline SMOTE-2 to\n",
    "        weight the distances in the sample generation process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_new : {ndarray, sparse matrix} of shape (n_samples_new, n_features)\n",
    "        Synthetically generated samples.\n",
    "\n",
    "    y_new : ndarray of shape (n_samples_new,)\n",
    "        Target values for synthetic samples.\n",
    "    \"\"\"\n",
    "    random_state = check_random_state(self.random_state)\n",
    "    samples_indices = random_state.randint(low=0, high=nn_num.size, size=n_samples)\n",
    "\n",
    "    # np.newaxis for backwards compatability with random_state\n",
    "    steps = step_size * random_state.uniform(size=n_samples)[:, np.newaxis]\n",
    "    rows = np.floor_divide(samples_indices, nn_num.shape[1])\n",
    "    cols = np.mod(samples_indices, nn_num.shape[1])\n",
    "\n",
    "    X_new = self._generate_samples(X, nn_data, nn_num, rows, cols, steps, y_type, y)\n",
    "    y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)\n",
    "    return X_new, y_new\n",
    "\n",
    "def _generate_samples(\n",
    "    self, X, nn_data, nn_num, rows, cols, steps, y_type=None, y=None\n",
    "):\n",
    "    r\"\"\"Generate a synthetic sample.\n",
    "\n",
    "    The rule for the generation is:\n",
    "\n",
    "    .. math::\n",
    "       \\mathbf{s_{s}} = \\mathbf{s_{i}} + \\mathcal{u}(0, 1) \\times\n",
    "       (\\mathbf{s_{i}} - \\mathbf{s_{nn}}) \\,\n",
    "\n",
    "    where \\mathbf{s_{s}} is the new synthetic samples, \\mathbf{s_{i}} is\n",
    "    the current sample, \\mathbf{s_{nn}} is a randomly selected neighbors of\n",
    "    \\mathbf{s_{i}} and \\mathcal{u}(0, 1) is a random number between [0, 1).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "        Points from which the points will be created.\n",
    "\n",
    "    nn_data : ndarray of shape (n_samples_all, n_features)\n",
    "        Data set carrying all the neighbours to be used.\n",
    "\n",
    "    nn_num : ndarray of shape (n_samples_all, k_nearest_neighbours)\n",
    "        The nearest neighbours of each sample in `nn_data`.\n",
    "\n",
    "    rows : ndarray of shape (n_samples,), dtype=int\n",
    "        Indices pointing at feature vector in X which will be used\n",
    "        as a base for creating new samples.\n",
    "\n",
    "    cols : ndarray of shape (n_samples,), dtype=int\n",
    "        Indices pointing at which nearest neighbor of base feature vector\n",
    "        will be used when creating new samples.\n",
    "\n",
    "    steps : ndarray of shape (n_samples,), dtype=float\n",
    "        Step sizes for new samples.\n",
    "\n",
    "    y_type : str, int or None, default=None\n",
    "        Class label of the current target classes for which we want to generate\n",
    "        samples.\n",
    "\n",
    "    y : ndarray of shape (n_samples_all,), default=None\n",
    "        The true target associated with `nn_data`. Used by Borderline SMOTE-2 to\n",
    "        weight the distances in the sample generation process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_new : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
    "        Synthetically generated samples.\n",
    "    \"\"\"\n",
    "    diffs = nn_data[nn_num[rows, cols]] - X[rows]\n",
    "    if y is not None:  # only entering for BorderlineSMOTE-2\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        mask_pair_samples = y[nn_num[rows, cols]] != y_type\n",
    "        diffs[mask_pair_samples] *= random_state.uniform(\n",
    "            low=0.0, high=0.5, size=(mask_pair_samples.sum(), 1)\n",
    "        )\n",
    "\n",
    "    if sparse.issparse(X):\n",
    "        sparse_func = type(X).__name__\n",
    "        steps = getattr(sparse, sparse_func)(steps)\n",
    "        X_new = X[rows] + steps.multiply(diffs)\n",
    "    else:\n",
    "        X_new = X[rows] + steps * diffs\n",
    "\n",
    "    return X_new.astype(X.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bdf23c-7dfb-4db4-84d7-a2e06843f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_new, y_new = self._make_samples(X_class, y.dtype, class_sample, X_class, nns, n_samples, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6015dfa-942f-4fa1-8701-68d1f52198e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1820094979.py, line 410)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[54], line 410\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Base class and original SMOTE methods for over-sampling\"\"\"\n",
    "\n",
    "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
    "#          Fernando Nogueira\n",
    "#          Christos Aridas\n",
    "#          Dzianis Dudnik\n",
    "# License: MIT\n",
    "\n",
    "import math\n",
    "import numbers\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from scipy import sparse\n",
    "from sklearn.base import clone\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.utils import (\n",
    "    _safe_indexing,\n",
    "    check_array,\n",
    "    check_random_state,\n",
    ")\n",
    "from sklearn.utils.fixes import parse_version\n",
    "from sklearn.utils.sparsefuncs_fast import (\n",
    "    csr_mean_variance_axis0,\n",
    ")\n",
    "from sklearn.utils.validation import _num_features\n",
    "\n",
    "from ...metrics.pairwise import ValueDifferenceMetric\n",
    "from ...utils import Substitution, check_neighbors_object, check_target_type\n",
    "from ...utils._docstring import _n_jobs_docstring, _random_state_docstring\n",
    "from ...utils._param_validation import HasMethods, Interval, StrOptions\n",
    "from ...utils._validation import _check_X\n",
    "from ...utils.fixes import _is_pandas_df, _mode\n",
    "from ..base import BaseOverSampler\n",
    "\n",
    "sklearn_version = parse_version(sklearn.__version__).base_version\n",
    "if parse_version(sklearn_version) < parse_version(\"1.5\"):\n",
    "    from sklearn.utils import _get_column_indices\n",
    "else:\n",
    "    from sklearn.utils._indexing import _get_column_indices\n",
    "\n",
    "\n",
    "class BaseSMOTE(BaseOverSampler):\n",
    "    \"\"\"Base class for the different SMOTE algorithms.\"\"\"\n",
    "\n",
    "    _parameter_constraints: dict = {\n",
    "        **BaseOverSampler._parameter_constraints,\n",
    "        \"k_neighbors\": [\n",
    "            Interval(numbers.Integral, 1, None, closed=\"left\"),\n",
    "            HasMethods([\"kneighbors\", \"kneighbors_graph\"]),\n",
    "        ],\n",
    "        \"n_jobs\": [numbers.Integral, None],\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_strategy=\"auto\",\n",
    "        random_state=None,\n",
    "        k_neighbors=5,\n",
    "        n_jobs=None,\n",
    "    ):\n",
    "        super().__init__(sampling_strategy=sampling_strategy)\n",
    "        self.random_state = random_state\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        \"\"\"Check the NN estimators shared across the different SMOTE\n",
    "        algorithms.\n",
    "        \"\"\"\n",
    "        self.nn_k_ = check_neighbors_object(\n",
    "            \"k_neighbors\", self.k_neighbors, additional_neighbor=1\n",
    "        )\n",
    "\n",
    "    def _make_samples(\n",
    "        self, X, y_dtype, y_type, nn_data, nn_num, n_samples, step_size=1.0, y=None\n",
    "    ):\n",
    "        \"\"\"A support function that returns artificial samples constructed along\n",
    "        the line connecting nearest neighbours.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Points from which the points will be created.\n",
    "\n",
    "        y_dtype : dtype\n",
    "            The data type of the targets.\n",
    "\n",
    "        y_type : str or int\n",
    "            The minority target value, just so the function can return the\n",
    "            target values for the synthetic variables with correct length in\n",
    "            a clear format.\n",
    "\n",
    "        nn_data : ndarray of shape (n_samples_all, n_features)\n",
    "            Data set carrying all the neighbours to be used\n",
    "\n",
    "        nn_num : ndarray of shape (n_samples_all, k_nearest_neighbours)\n",
    "            The nearest neighbours of each sample in `nn_data`.\n",
    "\n",
    "        n_samples : int\n",
    "            The number of samples to generate.\n",
    "\n",
    "        step_size : float, default=1.0\n",
    "            The step size to create samples.\n",
    "\n",
    "        y : ndarray of shape (n_samples_all,), default=None\n",
    "            The true target associated with `nn_data`. Used by Borderline SMOTE-2 to\n",
    "            weight the distances in the sample generation process.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : {ndarray, sparse matrix} of shape (n_samples_new, n_features)\n",
    "            Synthetically generated samples.\n",
    "\n",
    "        y_new : ndarray of shape (n_samples_new,)\n",
    "            Target values for synthetic samples.\n",
    "        \"\"\"\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        samples_indices = random_state.randint(low=0, high=nn_num.size, size=n_samples)\n",
    "\n",
    "        # np.newaxis for backwards compatability with random_state\n",
    "        steps = step_size * random_state.uniform(size=n_samples)[:, np.newaxis]\n",
    "        rows = np.floor_divide(samples_indices, nn_num.shape[1])\n",
    "        cols = np.mod(samples_indices, nn_num.shape[1])\n",
    "\n",
    "        X_new = self._generate_samples(X, nn_data, nn_num, rows, cols, steps, y_type, y)\n",
    "        y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)\n",
    "        return X_new, y_new\n",
    "\n",
    "    def _generate_samples(\n",
    "        self, X, nn_data, nn_num, rows, cols, steps, y_type=None, y=None\n",
    "    ):\n",
    "        r\"\"\"Generate a synthetic sample.\n",
    "\n",
    "        The rule for the generation is:\n",
    "\n",
    "        .. math::\n",
    "           \\mathbf{s_{s}} = \\mathbf{s_{i}} + \\mathcal{u}(0, 1) \\times\n",
    "           (\\mathbf{s_{i}} - \\mathbf{s_{nn}}) \\,\n",
    "\n",
    "        where \\mathbf{s_{s}} is the new synthetic samples, \\mathbf{s_{i}} is\n",
    "        the current sample, \\mathbf{s_{nn}} is a randomly selected neighbors of\n",
    "        \\mathbf{s_{i}} and \\mathcal{u}(0, 1) is a random number between [0, 1).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Points from which the points will be created.\n",
    "\n",
    "        nn_data : ndarray of shape (n_samples_all, n_features)\n",
    "            Data set carrying all the neighbours to be used.\n",
    "\n",
    "        nn_num : ndarray of shape (n_samples_all, k_nearest_neighbours)\n",
    "            The nearest neighbours of each sample in `nn_data`.\n",
    "\n",
    "        rows : ndarray of shape (n_samples,), dtype=int\n",
    "            Indices pointing at feature vector in X which will be used\n",
    "            as a base for creating new samples.\n",
    "\n",
    "        cols : ndarray of shape (n_samples,), dtype=int\n",
    "            Indices pointing at which nearest neighbor of base feature vector\n",
    "            will be used when creating new samples.\n",
    "\n",
    "        steps : ndarray of shape (n_samples,), dtype=float\n",
    "            Step sizes for new samples.\n",
    "\n",
    "        y_type : str, int or None, default=None\n",
    "            Class label of the current target classes for which we want to generate\n",
    "            samples.\n",
    "\n",
    "        y : ndarray of shape (n_samples_all,), default=None\n",
    "            The true target associated with `nn_data`. Used by Borderline SMOTE-2 to\n",
    "            weight the distances in the sample generation process.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
    "            Synthetically generated samples.\n",
    "        \"\"\"\n",
    "        diffs = nn_data[nn_num[rows, cols]] - X[rows]\n",
    "        if y is not None:  # only entering for BorderlineSMOTE-2\n",
    "            random_state = check_random_state(self.random_state)\n",
    "            mask_pair_samples = y[nn_num[rows, cols]] != y_type\n",
    "            diffs[mask_pair_samples] *= random_state.uniform(\n",
    "                low=0.0, high=0.5, size=(mask_pair_samples.sum(), 1)\n",
    "            )\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            sparse_func = type(X).__name__\n",
    "            steps = getattr(sparse, sparse_func)(steps)\n",
    "            X_new = X[rows] + steps.multiply(diffs)\n",
    "        else:\n",
    "            X_new = X[rows] + steps * diffs\n",
    "\n",
    "        return X_new.astype(X.dtype)\n",
    "\n",
    "    def _in_danger_noise(self, nn_estimator, samples, target_class, y, kind=\"danger\"):\n",
    "        \"\"\"Estimate if a set of sample are in danger or noise.\n",
    "\n",
    "        Used by BorderlineSMOTE and SVMSMOTE.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nn_estimator : estimator object\n",
    "            An estimator that inherits from\n",
    "            :class:`~sklearn.neighbors.base.KNeighborsMixin` use to determine\n",
    "            if a sample is in danger/noise.\n",
    "\n",
    "        samples : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The samples to check if either they are in danger or not.\n",
    "\n",
    "        target_class : int or str\n",
    "            The target corresponding class being over-sampled.\n",
    "\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The true label in order to check the neighbour labels.\n",
    "\n",
    "        kind : {'danger', 'noise'}, default='danger'\n",
    "            The type of classification to use. Can be either:\n",
    "\n",
    "            - If 'danger', check if samples are in danger,\n",
    "            - If 'noise', check if samples are noise.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output : ndarray of shape (n_samples,)\n",
    "            A boolean array where True refer to samples in danger or noise.\n",
    "        \"\"\"\n",
    "        x = nn_estimator.kneighbors(samples, return_distance=False)[:, 1:]\n",
    "        nn_label = (y[x] != target_class).astype(int)\n",
    "        n_maj = np.sum(nn_label, axis=1)\n",
    "\n",
    "        if kind == \"danger\":\n",
    "            # Samples are in danger for m/2 <= m' < m\n",
    "            return np.bitwise_and(\n",
    "                n_maj >= (nn_estimator.n_neighbors - 1) / 2,\n",
    "                n_maj < nn_estimator.n_neighbors - 1,\n",
    "            )\n",
    "        else:  # kind == \"noise\":\n",
    "            # Samples are noise for m = m'\n",
    "            return n_maj == nn_estimator.n_neighbors - 1\n",
    "\n",
    "\n",
    "@Substitution(\n",
    "    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,\n",
    "    n_jobs=_n_jobs_docstring,\n",
    "    random_state=_random_state_docstring,\n",
    ")\n",
    "class SMOTE(BaseSMOTE):\n",
    "    \"\"\"Class to perform over-sampling using SMOTE.\n",
    "\n",
    "    This object is an implementation of SMOTE - Synthetic Minority\n",
    "    Over-sampling Technique as presented in [1]_.\n",
    "\n",
    "    Read more in the :ref:`User Guide <smote_adasyn>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    {sampling_strategy}\n",
    "\n",
    "    {random_state}\n",
    "\n",
    "    k_neighbors : int or object, default=5\n",
    "        The nearest neighbors used to define the neighborhood of samples to use\n",
    "        to generate the synthetic samples. You can pass:\n",
    "\n",
    "        - an `int` corresponding to the number of neighbors to use. A\n",
    "          `~sklearn.neighbors.NearestNeighbors` instance will be fitted in this\n",
    "          case.\n",
    "        - an instance of a compatible nearest neighbors algorithm that should\n",
    "          implement both methods `kneighbors` and `kneighbors_graph`. For\n",
    "          instance, it could correspond to a\n",
    "          :class:`~sklearn.neighbors.NearestNeighbors` but could be extended to\n",
    "          any compatible class.\n",
    "\n",
    "    {n_jobs}\n",
    "\n",
    "        .. deprecated:: 0.10\n",
    "           `n_jobs` has been deprecated in 0.10 and will be removed in 0.12.\n",
    "           It was previously used to set `n_jobs` of nearest neighbors\n",
    "           algorithm. From now on, you can pass an estimator where `n_jobs` is\n",
    "           already set instead.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    sampling_strategy_ : dict\n",
    "        Dictionary containing the information to sample the dataset. The keys\n",
    "        corresponds to the class labels from which to sample and the values\n",
    "        are the number of samples to sample.\n",
    "\n",
    "    nn_k_ : estimator object\n",
    "        Validated k-nearest neighbours created from the `k_neighbors` parameter.\n",
    "\n",
    "    n_features_in_ : int\n",
    "        Number of features in the input dataset.\n",
    "\n",
    "        .. versionadded:: 0.9\n",
    "\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during `fit`. Defined only when `X` has feature\n",
    "        names that are all strings.\n",
    "\n",
    "        .. versionadded:: 0.10\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    SMOTENC : Over-sample using SMOTE for continuous and categorical features.\n",
    "\n",
    "    SMOTEN : Over-sample using the SMOTE variant specifically for categorical\n",
    "        features only.\n",
    "\n",
    "    BorderlineSMOTE : Over-sample using the borderline-SMOTE variant.\n",
    "\n",
    "    SVMSMOTE : Over-sample using the SVM-SMOTE variant.\n",
    "\n",
    "    ADASYN : Over-sample using ADASYN.\n",
    "\n",
    "    KMeansSMOTE : Over-sample applying a clustering before to oversample using\n",
    "        SMOTE.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See the original papers: [1]_ for more details.\n",
    "\n",
    "    Supports multi-class resampling. A one-vs.-rest scheme is used as\n",
    "    originally proposed in [1]_.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, \"SMOTE:\n",
    "       synthetic minority over-sampling technique,\" Journal of artificial\n",
    "       intelligence research, 321-357, 2002.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from collections import Counter\n",
    "    >>> from sklearn.datasets import make_classification\n",
    "    >>> from imblearn.over_sampling import SMOTE\n",
    "    >>> X, y = make_classification(n_classes=2, class_sep=2,\n",
    "    ... weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "    ... n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "    >>> print('Original dataset shape %s' % Counter(y))\n",
    "    Original dataset shape Counter({{1: 900, 0: 100}})\n",
    "    >>> sm = SMOTE(random_state=42)\n",
    "    >>> X_res, y_res = sm.fit_resample(X, y)\n",
    "    >>> print('Resampled dataset shape %s' % Counter(y_res))\n",
    "    Resampled dataset shape Counter({{0: 900, 1: 900}})\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        sampling_strategy=\"auto\",\n",
    "        random_state=None,\n",
    "        k_neighbors=5,\n",
    "        n_jobs=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            sampling_strategy=sampling_strategy,\n",
    "            random_state=random_state,\n",
    "            k_neighbors=k_neighbors,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        # FIXME: to be removed in 0.12\n",
    "        if self.n_jobs is not None:\n",
    "            warnings.warn(\n",
    "                \"The parameter `n_jobs` has been deprecated in 0.10 and will be \"\n",
    "                \"removed in 0.12. You can pass an nearest neighbors estimator where \"\n",
    "                \"`n_jobs` is already set instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "\n",
    "        self._validate_estimator()\n",
    "\n",
    "        X_resampled = [X.copy()]\n",
    "        y_resampled = [y.copy()]\n",
    "\n",
    "        for class_sample, n_samples in self.sampling_strategy_.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = _safe_indexing(X, target_class_indices)\n",
    "\n",
    "            self.nn_k_.fit(X_class)\n",
    "            nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "            X_new, y_new = self._make_samples(\n",
    "                X_class, y.dtype, class_sample, X_class, nns, n_samples, 1.0\n",
    "            )\n",
    "            X_resampled.append(X_new)\n",
    "            y_resampled.append(y_new)\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            X_resampled = sparse.vstack(X_resampled, format=X.format)\n",
    "        else:\n",
    "            X_resampled = np.vstack(X_resampled)\n",
    "        y_resampled = np.hstack(y_resampled)\n",
    "\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "\n",
    "@Substitution(\n",
    "    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,\n",
    "    n_jobs=_n_jobs_docstring,\n",
    "    random_state=_random_state_docstring,\n",
    ")\n",
    "class SMOTENC(SMOTE):\n",
    "    \"\"\"Synthetic Minority Over-sampling Technique for Nominal and Continuous.\n",
    "\n",
    "    Unlike :class:`SMOTE`, SMOTE-NC for dataset containing numerical and\n",
    "    categorical features. However, it is not designed to work with only\n",
    "    categorical features.\n",
    "\n",
    "    Read more in the :ref:`User Guide <smote_adasyn>`.\n",
    "\n",
    "    .. versionadded:: 0.4\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    categorical_features : \"infer\" or array-like of shape (n_cat_features,) or \\\n",
    "            (n_features,), dtype={{bool, int, str}}\n",
    "        Specified which features are categorical. Can either be:\n",
    "\n",
    "        - \"auto\" (default) to automatically detect categorical features. Only\n",
    "          supported when `X` is a :class:`pandas.DataFrame` and it corresponds\n",
    "          to columns that have a :class:`pandas.CategoricalDtype`;\n",
    "        - array of `int` corresponding to the indices specifying the categorical\n",
    "          features;\n",
    "        - array of `str` corresponding to the feature names. `X` should be a pandas\n",
    "          :class:`pandas.DataFrame` in this case.\n",
    "        - mask array of shape (n_features, ) and ``bool`` dtype for which\n",
    "          ``True`` indicates the categorical features.\n",
    "\n",
    "    categorical_encoder : estimator, default=None\n",
    "        One-hot encoder used to encode the categorical features. If `None`, a\n",
    "        :class:`~sklearn.preprocessing.OneHotEncoder` is used with default parameters\n",
    "        apart from `handle_unknown` which is set to 'ignore'.\n",
    "\n",
    "    {sampling_strategy}\n",
    "\n",
    "    {random_state}\n",
    "\n",
    "    k_neighbors : int or object, default=5\n",
    "        The nearest neighbors used to define the neighborhood of samples to use\n",
    "        to generate the synthetic samples. You can pass:\n",
    "\n",
    "        - an `int` corresponding to the number of neighbors to use. A\n",
    "          `~sklearn.neighbors.NearestNeighbors` instance will be fitted in this\n",
    "          case.\n",
    "        - an instance of a compatible nearest neighbors algorithm that should\n",
    "          implement both methods `kneighbors` and `kneighbors_graph`. For\n",
    "          instance, it could correspond to a\n",
    "          :class:`~sklearn.neighbors.NearestNeighbors` but could be extended to\n",
    "          any compatible class.\n",
    "\n",
    "    {n_jobs}\n",
    "\n",
    "        .. deprecated:: 0.10\n",
    "           `n_jobs` has been deprecated in 0.10 and will be removed in 0.12.\n",
    "           It was previously used to set `n_jobs` of nearest neighbors\n",
    "           algorithm. From now on, you can pass an estimator where `n_jobs` is\n",
    "           already set instead.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    sampling_strategy_ : dict\n",
    "        Dictionary containing the information to sample the dataset. The keys\n",
    "        corresponds to the class labels from which to sample and the values\n",
    "        are the number of samples to sample.\n",
    "\n",
    "    nn_k_ : estimator object\n",
    "        Validated k-nearest neighbours created from the `k_neighbors` parameter.\n",
    "\n",
    "    ohe_ : :class:`~sklearn.preprocessing.OneHotEncoder`\n",
    "        The one-hot encoder used to encode the categorical features.\n",
    "\n",
    "        .. deprecated:: 0.11\n",
    "           `ohe_` is deprecated in 0.11 and will be removed in 0.13. Use\n",
    "           `categorical_encoder_` instead.\n",
    "\n",
    "    categorical_encoder_ : estimator\n",
    "        The encoder used to encode the categorical features.\n",
    "\n",
    "    categorical_features_ : ndarray of shape (n_cat_features,), dtype=np.int64\n",
    "        Indices of the categorical features.\n",
    "\n",
    "    continuous_features_ : ndarray of shape (n_cont_features,), dtype=np.int64\n",
    "        Indices of the continuous features.\n",
    "\n",
    "    median_std_ : dict of int -> float\n",
    "        Median of the standard deviation of the continuous features for each\n",
    "        class to be over-sampled.\n",
    "\n",
    "    n_features_ : int\n",
    "        Number of features observed at `fit`.\n",
    "\n",
    "    n_features_in_ : int\n",
    "        Number of features in the input dataset.\n",
    "\n",
    "        .. versionadded:: 0.9\n",
    "\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during `fit`. Defined only when `X` has feature\n",
    "        names that are all strings.\n",
    "\n",
    "        .. versionadded:: 0.10\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    SMOTE : Over-sample using SMOTE.\n",
    "\n",
    "    SMOTEN : Over-sample using the SMOTE variant specifically for categorical\n",
    "        features only.\n",
    "\n",
    "    SVMSMOTE : Over-sample using SVM-SMOTE variant.\n",
    "\n",
    "    BorderlineSMOTE : Over-sample using Borderline-SMOTE variant.\n",
    "\n",
    "    ADASYN : Over-sample using ADASYN.\n",
    "\n",
    "    KMeansSMOTE : Over-sample applying a clustering before to oversample using\n",
    "        SMOTE.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See the original paper [1]_ for more details.\n",
    "\n",
    "    Supports multi-class resampling. A one-vs.-rest scheme is used as\n",
    "    originally proposed in [1]_.\n",
    "\n",
    "    See\n",
    "    :ref:`sphx_glr_auto_examples_over-sampling_plot_comparison_over_sampling.py`,\n",
    "    and\n",
    "    :ref:`sphx_glr_auto_examples_over-sampling_plot_illustration_generation_sample.py`.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, \"SMOTE:\n",
    "       synthetic minority over-sampling technique,\" Journal of artificial\n",
    "       intelligence research, 321-357, 2002.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from collections import Counter\n",
    "    >>> from numpy.random import RandomState\n",
    "    >>> from sklearn.datasets import make_classification\n",
    "    >>> from imblearn.over_sampling import SMOTENC\n",
    "    >>> X, y = make_classification(n_classes=2, class_sep=2,\n",
    "    ... weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "    ... n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "    >>> print(f'Original dataset shape {{X.shape}}')\n",
    "    Original dataset shape (1000, 20)\n",
    "    >>> print(f'Original dataset samples per class {{Counter(y)}}')\n",
    "    Original dataset samples per class Counter({{1: 900, 0: 100}})\n",
    "    >>> # simulate the 2 last columns to be categorical features\n",
    "    >>> X[:, -2:] = RandomState(10).randint(0, 4, size=(1000, 2))\n",
    "    >>> sm = SMOTENC(random_state=42, categorical_features=[18, 19])\n",
    "    >>> X_res, y_res = sm.fit_resample(X, y)\n",
    "    >>> print(f'Resampled dataset samples per class {{Counter(y_res)}}')\n",
    "    Resampled dataset samples per class Counter({{0: 900, 1: 900}})\n",
    "    \"\"\"\n",
    "\n",
    "    _required_parameters = [\"categorical_features\"]\n",
    "\n",
    "    _parameter_constraints: dict = {\n",
    "        **SMOTE._parameter_constraints,\n",
    "        \"categorical_features\": [\"array-like\", StrOptions({\"auto\"})],\n",
    "        \"categorical_encoder\": [\n",
    "            HasMethods([\"fit_transform\", \"inverse_transform\"]),\n",
    "            None,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_features,\n",
    "        *,\n",
    "        categorical_encoder=None,\n",
    "        sampling_strategy=\"auto\",\n",
    "        random_state=None,\n",
    "        k_neighbors=5,\n",
    "        n_jobs=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            sampling_strategy=sampling_strategy,\n",
    "            random_state=random_state,\n",
    "            k_neighbors=k_neighbors,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "        self.categorical_features = categorical_features\n",
    "        self.categorical_encoder = categorical_encoder\n",
    "\n",
    "    def _check_X_y(self, X, y):\n",
    "        \"\"\"Overwrite the checking to let pass some string for categorical\n",
    "        features.\n",
    "        \"\"\"\n",
    "        y, binarize_y = check_target_type(y, indicate_one_vs_all=True)\n",
    "        X = _check_X(X)\n",
    "        self._check_n_features(X, reset=True)\n",
    "        self._check_feature_names(X, reset=True)\n",
    "        return X, y, binarize_y\n",
    "\n",
    "    def _validate_column_types(self, X):\n",
    "        \"\"\"Compute the indices of the categorical and continuous features.\"\"\"\n",
    "        if self.categorical_features == \"auto\":\n",
    "            if not _is_pandas_df(X):\n",
    "                raise ValueError(\n",
    "                    \"When `categorical_features='auto'`, the input data \"\n",
    "                    f\"should be a pandas.DataFrame. Got {type(X)} instead.\"\n",
    "                )\n",
    "            import pandas as pd  # safely import pandas now\n",
    "\n",
    "            are_columns_categorical = np.array(\n",
    "                [isinstance(col_dtype, pd.CategoricalDtype) for col_dtype in X.dtypes]\n",
    "            )\n",
    "            self.categorical_features_ = np.flatnonzero(are_columns_categorical)\n",
    "            self.continuous_features_ = np.flatnonzero(~are_columns_categorical)\n",
    "        else:\n",
    "            self.categorical_features_ = np.array(\n",
    "                _get_column_indices(X, self.categorical_features)\n",
    "            )\n",
    "            self.continuous_features_ = np.setdiff1d(\n",
    "                np.arange(self.n_features_), self.categorical_features_\n",
    "            )\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        super()._validate_estimator()\n",
    "        if self.categorical_features_.size == self.n_features_in_:\n",
    "            raise ValueError(\n",
    "                \"SMOTE-NC is not designed to work only with categorical \"\n",
    "                \"features. It requires some numerical features.\"\n",
    "            )\n",
    "        elif self.categorical_features_.size == 0:\n",
    "            raise ValueError(\n",
    "                \"SMOTE-NC is not designed to work only with numerical \"\n",
    "                \"features. It requires some categorical features.\"\n",
    "            )\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        # FIXME: to be removed in 0.12\n",
    "        if self.n_jobs is not None:\n",
    "            warnings.warn(\n",
    "                \"The parameter `n_jobs` has been deprecated in 0.10 and will be \"\n",
    "                \"removed in 0.12. You can pass an nearest neighbors estimator where \"\n",
    "                \"`n_jobs` is already set instead.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "\n",
    "        self.n_features_ = _num_features(X)\n",
    "        self._validate_column_types(X)\n",
    "        self._validate_estimator()\n",
    "\n",
    "        X_continuous = _safe_indexing(X, self.continuous_features_, axis=1)\n",
    "        X_continuous = check_array(X_continuous, accept_sparse=[\"csr\", \"csc\"])\n",
    "        X_categorical = _safe_indexing(X, self.categorical_features_, axis=1)\n",
    "        if X_continuous.dtype.name != \"object\":\n",
    "            dtype_ohe = X_continuous.dtype\n",
    "        else:\n",
    "            dtype_ohe = np.float64\n",
    "\n",
    "        if self.categorical_encoder is None:\n",
    "            self.categorical_encoder_ = OneHotEncoder(\n",
    "                handle_unknown=\"ignore\", dtype=dtype_ohe\n",
    "            )\n",
    "        else:\n",
    "            self.categorical_encoder_ = clone(self.categorical_encoder)\n",
    "\n",
    "        # the input of the OneHotEncoder needs to be dense\n",
    "        X_ohe = self.categorical_encoder_.fit_transform(\n",
    "            X_categorical.toarray() if sparse.issparse(X_categorical) else X_categorical\n",
    "        )\n",
    "        if not sparse.issparse(X_ohe):\n",
    "            X_ohe = sparse.csr_matrix(X_ohe, dtype=dtype_ohe)\n",
    "\n",
    "        X_encoded = sparse.hstack((X_continuous, X_ohe), format=\"csr\", dtype=dtype_ohe)\n",
    "        X_resampled = [X_encoded.copy()]\n",
    "        y_resampled = [y.copy()]\n",
    "\n",
    "        # SMOTE resampling starts here\n",
    "        self.median_std_ = {}\n",
    "        for class_sample, n_samples in self.sampling_strategy_.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = _safe_indexing(X_encoded, target_class_indices)\n",
    "\n",
    "            _, var = csr_mean_variance_axis0(\n",
    "                X_class[:, : self.continuous_features_.size]\n",
    "            )\n",
    "            self.median_std_[class_sample] = np.median(np.sqrt(var))\n",
    "\n",
    "            # In the edge case where the median of the std is equal to 0, the 1s\n",
    "            # entries will be also nullified. In this case, we store the original\n",
    "            # categorical encoding which will be later used for inverting the OHE\n",
    "            if math.isclose(self.median_std_[class_sample], 0):\n",
    "                # This variable will be used when generating data\n",
    "                self._X_categorical_minority_encoded = X_class[\n",
    "                    :, self.continuous_features_.size :\n",
    "                ].toarray()\n",
    "\n",
    "            # we can replace the 1 entries of the categorical features with the\n",
    "            # median of the standard deviation. It will ensure that whenever\n",
    "            # distance is computed between 2 samples, the difference will be equal\n",
    "            # to the median of the standard deviation as in the original paper.\n",
    "            X_class_categorical = X_class[:, self.continuous_features_.size :]\n",
    "            # With one-hot encoding, the median will be repeated twice. We need\n",
    "            # to divide by sqrt(2) such that we only have one median value\n",
    "            # contributing to the Euclidean distance\n",
    "            X_class_categorical.data[:] = self.median_std_[class_sample] / np.sqrt(2)\n",
    "            X_class[:, self.continuous_features_.size :] = X_class_categorical\n",
    "\n",
    "            self.nn_k_.fit(X_class)\n",
    "            nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "            X_new, y_new = self._make_samples(\n",
    "                X_class, y.dtype, class_sample, X_class, nns, n_samples, 1.0\n",
    "            )\n",
    "            X_resampled.append(X_new)\n",
    "            y_resampled.append(y_new)\n",
    "\n",
    "        X_resampled = sparse.vstack(X_resampled, format=X_encoded.format)\n",
    "        y_resampled = np.hstack(y_resampled)\n",
    "        # SMOTE resampling ends here\n",
    "\n",
    "        # reverse the encoding of the categorical features\n",
    "        X_res_cat = X_resampled[:, self.continuous_features_.size :]\n",
    "        X_res_cat.data = np.ones_like(X_res_cat.data)\n",
    "        X_res_cat_dec = self.categorical_encoder_.inverse_transform(X_res_cat)\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            X_resampled = sparse.hstack(\n",
    "                (\n",
    "                    X_resampled[:, : self.continuous_features_.size],\n",
    "                    X_res_cat_dec,\n",
    "                ),\n",
    "                format=\"csr\",\n",
    "            )\n",
    "        else:\n",
    "            X_resampled = np.hstack(\n",
    "                (\n",
    "                    X_resampled[:, : self.continuous_features_.size].toarray(),\n",
    "                    X_res_cat_dec,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        indices_reordered = np.argsort(\n",
    "            np.hstack((self.continuous_features_, self.categorical_features_))\n",
    "        )\n",
    "        if sparse.issparse(X_resampled):\n",
    "            # the matrix is supposed to be in the CSR format after the stacking\n",
    "            col_indices = X_resampled.indices.copy()\n",
    "            for idx, col_idx in enumerate(indices_reordered):\n",
    "                mask = X_resampled.indices == col_idx\n",
    "                col_indices[mask] = idx\n",
    "            X_resampled.indices = col_indices\n",
    "        else:\n",
    "            X_resampled = X_resampled[:, indices_reordered]\n",
    "\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "    def _generate_samples(self, X, nn_data, nn_num, rows, cols, steps, y_type, y=None):\n",
    "        \"\"\"Generate a synthetic sample with an additional steps for the\n",
    "        categorical features.\n",
    "\n",
    "        Each new sample is generated the same way than in SMOTE. However, the\n",
    "        categorical features are mapped to the most frequent nearest neighbors\n",
    "        of the majority class.\n",
    "        \"\"\"\n",
    "        rng = check_random_state(self.random_state)\n",
    "        X_new = super()._generate_samples(X, nn_data, nn_num, rows, cols, steps)\n",
    "        # change in sparsity structure more efficient with LIL than CSR\n",
    "        X_new = X_new.tolil() if sparse.issparse(X_new) else X_new\n",
    "\n",
    "        # convert to dense array since scipy.sparse doesn't handle 3D\n",
    "        nn_data = nn_data.toarray() if sparse.issparse(nn_data) else nn_data\n",
    "\n",
    "        # In the case that the median std was equal to zeros, we have to\n",
    "        # create non-null entry based on the encoded of OHE\n",
    "        if math.isclose(self.median_std_[y_type], 0):\n",
    "            nn_data[\n",
    "                :, self.continuous_features_.size :\n",
    "            ] = self._X_categorical_minority_encoded\n",
    "\n",
    "        all_neighbors = nn_data[nn_num[rows]]\n",
    "\n",
    "        categories_size = [self.continuous_features_.size] + [\n",
    "            cat.size for cat in self.categorical_encoder_.categories_\n",
    "        ]\n",
    "\n",
    "        for start_idx, end_idx in zip(\n",
    "            np.cumsum(categories_size)[:-1], np.cumsum(categories_size)[1:]\n",
    "        ):\n",
    "            col_maxs = all_neighbors[:, :, start_idx:end_idx].sum(axis=1)\n",
    "            # tie breaking argmax\n",
    "            is_max = np.isclose(col_maxs, col_maxs.max(axis=1, keepdims=True))\n",
    "            max_idxs = rng.permutation(np.argwhere(is_max))\n",
    "            xs, idx_sels = np.unique(max_idxs[:, 0], return_index=True)\n",
    "            col_sels = max_idxs[idx_sels, 1]\n",
    "\n",
    "            ys = start_idx + col_sels\n",
    "            X_new[:, start_idx:end_idx] = 0\n",
    "            X_new[xs, ys] = 1\n",
    "\n",
    "        return X_new\n",
    "\n",
    "    @property\n",
    "    def ohe_(self):\n",
    "        \"\"\"One-hot encoder used to encode the categorical features.\"\"\"\n",
    "        warnings.warn(\n",
    "            \"'ohe_' attribute has been deprecated in 0.11 and will be removed \"\n",
    "            \"in 0.13. Use 'categorical_encoder_' instead.\",\n",
    "            FutureWarning,\n",
    "        )\n",
    "        return self.categorical_encoder_\n",
    "\n",
    "\n",
    "@Substitution(\n",
    "    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,\n",
    "    n_jobs=_n_jobs_docstring,\n",
    "    random_state=_random_state_docstring,\n",
    ")\n",
    "class SMOTEN(SMOTE):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
