{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab12ef8-29cf-4cf0-9165-2a38f7278dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, put this prompt: \"conda install -c conda-forge py-xgboost\" in anaconda to download xgboost package\n",
    "# install xgboost in jupyter\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6e2c2b1-797d-4f7e-ad25-0e3a0045bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 2 2 2 1 0 2 2 2 2 2 0 2 2 0 2 2 0 1 0 1 1 2 2 1 1 2 1]\n",
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# import the classifier from the xgboost package\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# import splitting function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "# read data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# load the data and save as a variable\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# split the dataset into training and testing data\n",
    "# test_size: this is the percentage of data used for testing (20% in this case), so the rest is used for training data (80% in this case)\n",
    "# random_state: this is a random number chosen that should be used each time to ensure we get the same data split each time\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=.2, random_state=52)\n",
    "\n",
    "# create model instance\n",
    "# n_estimators: number of trees(estimators) the model uses --> the more used, the more accurate the model is\n",
    "# max_depth: maximum depth of tree --> higher number makes model more complex, but too high can cause overfitting\n",
    "# learning_rate: quantifies each tree's contribution to total prediction --> lower number takes longer, but can lead to better generalization\n",
    "# objective: binary:logistic outputs probabilities. if classification is wanted, use binary:hinge\n",
    "bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "\n",
    "# fit model with the training data\n",
    "bst.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for the test dataset\n",
    "preds = bst.predict(X_test)\n",
    "\n",
    "# print predictions\n",
    "print(preds)\n",
    "\n",
    "# print model Accuracy (how often the classifier is correct)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e512e-24e0-4cd3-8901-033d3ecbb707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
