{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0d46db-653f-4d57-8c27-03c90b6d22cb",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53352d8-22d2-4075-b31f-d0ef01c764f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c78482-a1c3-4fc3-9a32-422305278d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.loadtxt('/Users/elleemortensen/Documents/GitHub/BP24/Ellee/Data/gaussian_large_d_1.tex')\n",
    "# data = np.loadtxt('/Users/elleemortensen/Documents/GitHub/BP24/Ellee/Data/gaussian_small_d_1.tex')\n",
    "# data = np.loadtxt('/Users/elleemortensen/Documents/GitHub/BP24/Ellee/uniform_large_d_1.tex')\n",
    "# data = np.loadtxt('/Users/elleemortensen/Documents/GitHub/BP24/Ellee/uniform_small_d_1.tex')\n",
    "data = pd.read_csv('/Users/elleemortensen/Documents/GitHub/BP24/Ellee/Sanity Checks/Demos/stacked_all.csv')\n",
    "# data = pd.read_csv('/Users/elleemortensen/Documents/GitHub/BP24/Ellee/Sanity Checks/Demos/gaussmall.csv', header=None)\n",
    "# data = pd.read_csv('/Users/elleemortensen/Documents/GitHub/BP24/Ellee/Sanity Checks/Demos/unismall.csv', header=None)\n",
    "array = np.array(data)\n",
    "df = pd.DataFrame(array)\n",
    "\n",
    "# df = pd.read_excel('/Users/elleemortensen/Downloads/D3Softball.xlsx', index_col=[0])\n",
    "# df.replace('---', np.nan, inplace=True)\n",
    "# df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67a91c-b14a-4ce4-bdda-b64f6c32c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(25):\n",
    "#     df.iloc[:,i] = df.iloc[:,i].round()\n",
    "#     df.iloc[:,i] = df.iloc[:,i].astype(int)\n",
    "#     df.iloc[:,i] = df.iloc[:,i].astype(\"category\")\n",
    "# df.iloc[:,150] = df.iloc[:,150].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924e959-5c63-41bc-ac61-bf285cc1e4b9",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a914886-1f4b-4340-84c7-3a8e6f506d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing data\n",
    "# test_size: this is the percentage of data used for testing (20% in this case), so the rest is used for training data (80% in this case)\n",
    "# random_state: this is a random number chosen that should be used each time to ensure we get the same data split each time\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d63487-3dd7-49ed-bdd2-3b4432a519ef",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868df05f-7458-4286-bef6-62df87febe7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# columns_to_convert = [\n",
    "#     'G', 'AB', 'H (Offensive)', 'H/G', 'BA', '2B (Offensive)', '2B/G',\n",
    "#     '3B (Offensive)', '3B/G', 'Innings Pitched', 'K (Def)', 'K/G (Def)',\n",
    "#     'BB Allowed', 'BB/G (Def)', 'K/BB', 'HA', 'HA/G', 'Runs Allowed',\n",
    "#     'Runs Allowed/G', 'ER Allowed', 'ERA', 'WHIP', 'PO', 'A', 'E', 'E/G',\n",
    "#     'FPCT', 'HBP (Offensive)', 'HBP/G', 'BB (Offensive)', 'BB/G (Off)',\n",
    "#     'SF (Offensive)', 'SH (Offensive)', 'OBP', 'SHO', 'SHO %', 'SB',\n",
    "#     'SB/G', 'TB', 'TB/G', 'SLG PCT', 'R', 'R/G', 'DP', 'DP/G',\n",
    "#     'K (Off)', 'K/G (Off)'\n",
    "# ]\n",
    "\n",
    "# # Convert the columns to float\n",
    "# df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2c2b1-797d-4f7e-ad25-0e3a0045bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model instance\n",
    "# n_estimators: number of trees(estimators) the model uses --> the more used, the more accurate the model is\n",
    "# max_depth: maximum depth of tree --> higher number makes model more complex, but too high can cause overfitting\n",
    "# learning_rate: quantifies each tree's contribution to total prediction --> lower number takes longer, but can lead to better generalization\n",
    "# objective: binary:logistic outputs probabilities. if classification is wanted, use binary:hinge\n",
    "# bst = XGBClassifier(n_estimators = 2, max_depth = 2, learning_rate = 1, objective = 'binary:logistic', enable_categorical = True)\n",
    "\n",
    "# # fit model with the training data\n",
    "# bst.fit(X_train, y_train)\n",
    "\n",
    "# # make predictions for the test dataset\n",
    "# preds = bst.predict(X_test)\n",
    "\n",
    "# # print predictions\n",
    "# print(preds)\n",
    "\n",
    "# # print model Accuracy (how often the classifier is correct)\n",
    "# print(\"Accuracy:\", metrics.accuracy_score(y_test, preds))\n",
    "\n",
    "# Create XGBoost model instance\n",
    "bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic', enable_categorical=True)\n",
    "\n",
    "# Fit model with the training data\n",
    "bst.fit(X_train_corner, y_train_corner)\n",
    "\n",
    "# Make predictions for the test dataset\n",
    "preds = bst.predict(X_test_corner)\n",
    "\n",
    "# Print predictions\n",
    "print(\"Predictions:\", preds)\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1 = metrics.f1_score(y_test_corner, preds)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d3bfc-09bc-416e-b7c7-1f8c8370f16a",
   "metadata": {},
   "source": [
    "### FIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16d03cd9-1b77-4591-adc5-a5ac9e6fa031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureClassifier:\n",
    "  def __init__(self,reqAcc = 0.01, classifier = 'DecisionTree', bias = [], control = None, n_jobs = None, random_state = 42):\n",
    "    self.featureClassifiers=[] #list of all the classifiers of all the selected features\n",
    "    self.reqAcc=reqAcc #user specified cutoff value\n",
    "    self.indexLs=[] # list of mapped index values to featureClassifiers\n",
    "    self.flag=0\n",
    "    self.bias=bias # list of biases for each and every label\n",
    "    self.control=control #overfitting control for decision trees\n",
    "    self.classifier=classifier #the classifier which is preferred by the user\n",
    "    self.dic={'DecisionTree':0,'LinearRegression':1,'SVM':2,'LogisticRegression':3} #a dictionary which maps the classifier to its index\n",
    "    self.n_jobs=n_jobs\n",
    "    self.random_state=random_state\n",
    "    self.num_lables = None\n",
    "\n",
    "  def finIndex(self):\n",
    "    #finds the index where the reqAcc condition fails and also created the indexLs[] for mapping\n",
    "    for i in range(len(self.featureClassifiers)):\n",
    "      if self.featureClassifiers[i][1] < self.reqAcc:\n",
    "        return i\n",
    "      self.indexLs.append(self.featureClassifiers[i][2])\n",
    "    self.flag=1\n",
    "    return i\n",
    "  def fit(self,x,y):\n",
    "    #applied the model to the dataset. The model is trained and saved for further prediction\n",
    "    self.num_lables=len(set(y.flatten()))\n",
    "    bestfeatures = SelectKBest(score_func=chi2,k=1)\n",
    "    print(x,y)\n",
    "    fit = bestfeatures.fit(x,y)\n",
    "\n",
    "    for i in range(len(x[0])):\n",
    "      clf=[DecisionTreeClassifier(max_depth=self.control,random_state=self.random_state),LinearRegression(n_jobs=self.n_jobs),SVC(gamma=self.control,random_state=self.random_state), LogisticRegression(penalty=self.control,random_state=self.random_state)][self.dic[self.classifier]]\n",
    "      X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33,random_state=self.random_state)\n",
    "      clf.fit(X_train[:,i:i+1],y_train)\n",
    "      self.featureClassifiers.append((clf,fit.scores_[i],i))\n",
    "    self.featureClassifiers.sort(key=lambda x:x[1],reverse=True)\n",
    "    index=self.finIndex()\n",
    "    if self.flag==0:\n",
    "      self.featureClassifiers=self.featureClassifiers[:index]\n",
    "    return\n",
    "\n",
    "  def predict(self,x):\n",
    "    #given a list of inputs, predicts the possible outputs\n",
    "    if not self.bias:\n",
    "      self.bias=np.zeros(self.num_lables)\n",
    "    if len(self.bias)<self.num_lables:\n",
    "      raise AttributeError('Please check the lenth of bias list')\n",
    "    yPred=[]\n",
    "    for i in range(len(x)):\n",
    "      pred_arr=np.zeros(self.num_lables)\n",
    "      for j in range(len(self.indexLs)):\n",
    "        pred=np.round(self.featureClassifiers[j][0].predict([[x[i][self.indexLs[j]]]]))\n",
    "        pred_arr[pred]+=self.featureClassifiers[j][1]+self.bias[pred[0]]\n",
    "      yPred.append(np.argmax(pred_arr))\n",
    "    return yPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009d4fe-6bf7-47fd-bd66-f67858d1504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model using the training sets\n",
    "clf1=FeatureClassifier(0, classifier='DecisionTree', control=3, random_state=42)\n",
    "# clf1.fit(X_train,y_train.reshape(-1,1))\n",
    "clf1.fit(np.array(X_train), np.array(y_train)[:,np.newaxis].astype(int))\n",
    "\n",
    "#predict the response for the test dataset\n",
    "#model accuracy (how often the classifier is correct)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(np.array(y_test).astype(int),clf1.predict(np.array(X_test))))\n",
    "\n",
    "print(clf1.indexLs)\n",
    "clf1.featureClassifiers\n",
    "print(clf1.indexLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cae97d84-13e6-4d02-971d-5e622aa4d574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.91084413  0.7977536   3.25609559 ...  0.          0.\n",
      "   1.        ]\n",
      " [ 0.61496164  1.30631956 -0.15622361 ...  1.          1.\n",
      "   0.        ]\n",
      " [-0.07089384  0.84888389  0.02390265 ...  0.          1.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.37466714  3.64690773  2.03829623 ...  2.          1.\n",
      "   0.        ]\n",
      " [ 0.02271092  0.84125244  1.17746719 ...  0.          0.\n",
      "   2.        ]\n",
      " [ 1.04994748  0.38292255  1.17680361 ...  0.          0.\n",
      "   1.        ]] [1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0\n",
      " 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 0 0 1\n",
      " 0 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1\n",
      " 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1\n",
      " 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m clf1 \u001b[38;5;241m=\u001b[39m FeatureClassifier(\u001b[38;5;241m0\u001b[39m, classifier\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecisionTree\u001b[39m\u001b[38;5;124m'\u001b[39m, control\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fit model with the training data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m clf1\u001b[38;5;241m.\u001b[39mfit(np\u001b[38;5;241m.\u001b[39marray(X_train_corner), np\u001b[38;5;241m.\u001b[39marray(y_train_corner)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Predict the response for the test dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf1\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray(X_test_corner))\n",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m, in \u001b[0;36mFeatureClassifier.fit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     26\u001b[0m bestfeatures \u001b[38;5;241m=\u001b[39m SelectKBest(score_func\u001b[38;5;241m=\u001b[39mchi2,k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(x,y)\n\u001b[0;32m---> 28\u001b[0m fit \u001b[38;5;241m=\u001b[39m bestfeatures\u001b[38;5;241m.\u001b[39mfit(x,y)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;241m0\u001b[39m])):\n\u001b[1;32m     31\u001b[0m   clf\u001b[38;5;241m=\u001b[39m[DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state),LinearRegression(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs),SVC(gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state), LogisticRegression(penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdic[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier]]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:472\u001b[0m, in \u001b[0;36m_BaseFilter.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    467\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    468\u001b[0m     X, y, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m], multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    469\u001b[0m )\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(X, y)\n\u001b[0;32m--> 472\u001b[0m score_func_ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_func(X, y)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(score_func_ret, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpvalues_ \u001b[38;5;241m=\u001b[39m score_func_ret\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:217\u001b[0m, in \u001b[0;36mchi2\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    215\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m(np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many((X\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;28;01melse\u001b[39;00m X) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput X must be non-negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Use a sparse representation for Y by default to reduce memory usage when\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# y has many unique classes.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m Y \u001b[38;5;241m=\u001b[39m LabelBinarizer(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfit_transform(y)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative."
     ]
    }
   ],
   "source": [
    "# Assuming FeatureClassifier is correctly implemented for DecisionTreeClassifier\n",
    "clf1 = FeatureClassifier(0, classifier='DecisionTree', control=3, random_state=42)\n",
    "\n",
    "# Fit model with the training data\n",
    "clf1.fit(np.array(X_train_corner), np.array(y_train_corner).astype(int))\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "y_pred = clf1.predict(np.array(X_test_corner))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1 = metrics.f1_score(np.array(y_test_corner).astype(int), y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Print other relevant outputs for debugging\n",
    "print(clf1.indexLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56db6b0e-18c5-48f7-8302-8aa13dbd3228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sum(y_test_corner<0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc35e1-caed-4cdf-a018-bae406111104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with any negative values\n",
    "columns_with_negatives = df.columns[(df < 0).any()]\n",
    "\n",
    "# Drop those columns\n",
    "df = df.drop(columns=columns_with_negatives)\n",
    "df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5fc2c-2542-4fd9-89a6-1b26d1e475e7",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51113697-9559-443b-add0-bb9cff71f5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a knn classifier\n",
    "#n_neighbors: predicting the label of the data point by looking at the 3 closest data points and getting them to \"vote\"\n",
    "#algorithm: we may need to look at this if it misbehaves\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "#train the model using the training sets\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "#predict the response for the test dataset\n",
    "y_pred = neigh.predict(X_test)\n",
    "\n",
    "#print predictions\n",
    "print(y_pred)\n",
    "\n",
    "#model accuracy (how often the classifier is correct)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851f6f4-1fbd-487d-a348-96411817e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN classifier instance\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model using the training sets\n",
    "neigh.fit(X_train_corner, y_train_corner)\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "y_pred = neigh.predict(X_test_corner)\n",
    "\n",
    "# Print predictions\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1 = metrics.f1_score(y_test_corner, y_pred)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4427ca7-8d35-4f5e-be7a-bb6759c9047f",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc012a82-360d-447c-8b48-1c48d3ee7ba9",
   "metadata": {},
   "source": [
    "#### SVM Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3390b-b028-4d22-b67d-aa7792b31d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a svm Classifier\n",
    "# kernel: options for kernel include linear, poly, rbf, sigmoid\n",
    "    # linear: use this when data can be split by a linear function\n",
    "    # poly (polynomial): use this when data can be split by a polynomial function\n",
    "    # rbf (radial basis function): use this when there are clusters of one class inside another\n",
    "    # sigmoid: use this when the split between classes is curved and irregular\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# print predictions\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# print model Accuracy (how often the classifier is correct)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce6e36-a3b5-46b6-8d1e-669d044a787d",
   "metadata": {},
   "source": [
    "#### SVM F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e171592c-ad1c-4b30-92f4-c3a6f748afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked\n",
    "X_train_corner = df.iloc[:168, :23]\n",
    "X_test_corner = df.iloc[168:241, :23]\n",
    "y_train_corner =  df.iloc[:168, 24]\n",
    "y_test_corner = df.iloc[168:241, 24]\n",
    "\n",
    "# Gaussian\n",
    "# X_train_corner = df.iloc[:120, :12]\n",
    "# X_test_corner = df.iloc[120:, :12]\n",
    "# y_train_corner =  df.iloc[:120, 12]\n",
    "# y_test_corner = df.iloc[120:, 12]\n",
    "\n",
    "# Uniform\n",
    "# X_train_corner = df.iloc[:240, :24]\n",
    "# X_test_corner = df.iloc[240:, :24]\n",
    "# y_train_corner =  df.iloc[:240, 24]\n",
    "# y_test_corner = df.iloc[240:, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d28c3d-9822-4ec4-9386-758da72454e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVM classifier instance with linear kernel\n",
    "clf = svm.SVC(kernel='linear', random_state = 42)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train_corner.iloc[:,0:], y_train_corner)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test_corner.iloc[:,0:])\n",
    "\n",
    "# Print predictions\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# Calculate F1 score\n",
    "print(\"F1 Score:\", metrics.f1_score(y_test_corner, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0b62c-bd3d-48e6-a39d-1a28736f24e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
