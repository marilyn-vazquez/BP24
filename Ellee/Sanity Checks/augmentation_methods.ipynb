{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde92cb5-f744-48b6-8f54-04af24047e43",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47e8ec89-6557-4886-ba85-d89a18151aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of            0         1         2         3         4         5         6   \\\n",
       "0   -0.523644 -0.273276 -0.116495  0.405252  1.197326  0.596777  0.538033   \n",
       "1    1.910844  0.797754  3.256096  1.803028 -1.190719  0.792517  1.187202   \n",
       "2    0.614962  1.306320 -0.156224  0.208113 -0.142056  0.331569 -0.139525   \n",
       "3   -0.070894  0.848884  0.023903  0.232592  1.261394  0.678807  0.218641   \n",
       "4    0.493572  0.730451  0.669870  0.671474  0.927129 -0.167649  0.735716   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "235  0.794388  0.274516  0.180763  0.435718  1.275335  0.705205  1.317206   \n",
       "236  0.520387 -1.083603  0.621607  0.629536  0.751939 -0.883465  0.337003   \n",
       "237  3.168389  1.261369  2.362146  1.498775  3.540290 -0.528903  0.764048   \n",
       "238  3.957235  2.410099  0.698402 -1.049969  0.876141 -0.665939  0.346620   \n",
       "239  1.903798  0.637188  0.683087  0.521806  0.537816  0.756112  0.143213   \n",
       "\n",
       "           7         8         9   ...        15   16   17   18   19   20  \\\n",
       "0    0.354814  0.309177  0.145222  ...  0.173969  0.0  6.0  2.0  2.0  4.0   \n",
       "1    1.193782  0.342648  0.392967  ...  0.245511  1.0  1.0  2.0  0.0  0.0   \n",
       "2    1.683484  0.249680  0.290193  ...  0.261832  1.0  0.0  0.0  0.0  1.0   \n",
       "3    0.276284  0.219636  0.111468  ...  0.069706  1.0  1.0  0.0  0.0  0.0   \n",
       "4    0.356140  0.188136  0.131791  ...  0.101293  0.0  0.0  2.0  1.0  0.0   \n",
       "..        ...       ...       ...  ...       ...  ...  ...  ...  ...  ...   \n",
       "235  0.945085  0.304787  0.123680  ...  0.204098  0.0  0.0  0.0  1.0  1.0   \n",
       "236 -0.412355  0.208957  0.188374  ...  0.087250  0.0  1.0  1.0  1.0  1.0   \n",
       "237 -0.492612  0.197433  0.415921  ...  0.152163  0.0  1.0  2.0  0.0  0.0   \n",
       "238  0.034450  0.256384  0.448714  ...  0.257884  1.0  2.0  0.0  0.0  1.0   \n",
       "239  0.374679  0.266861  0.335356  ...  0.109969  1.0  2.0  3.0  0.0  0.0   \n",
       "\n",
       "      21   22   23   24  \n",
       "0    1.0  0.0  1.0  0.0  \n",
       "1    0.0  1.0  0.0  1.0  \n",
       "2    1.0  0.0  0.0  0.0  \n",
       "3    1.0  0.0  1.0  0.0  \n",
       "4    0.0  1.0  2.0  0.0  \n",
       "..   ...  ...  ...  ...  \n",
       "235  0.0  3.0  1.0  0.0  \n",
       "236  0.0  0.0  1.0  0.0  \n",
       "237  0.0  0.0  0.0  1.0  \n",
       "238  1.0  0.0  1.0  1.0  \n",
       "239  2.0  0.0  0.0  0.0  \n",
       "\n",
       "[240 rows x 25 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "df = pd.read_csv(\"/Users/elleemortensen/Documents/GitHub/BP24/Ellee/Data/Stacked/stacked_orig.csv\", header=None)\n",
    "df.info\n",
    "\n",
    "# # Creating NumPy array\n",
    "# array = np.array(data)\n",
    "\n",
    "# # Converting to Pandas DataFrame\n",
    "# df = pd.DataFrame(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be45cd6-ec01-4103-b03a-87fbb8c40d10",
   "metadata": {},
   "source": [
    "# Convert 25/150 Columns into NON-CONSECUTIVE Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17e27c-9c21-4514-93cf-ccdbb20c584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-5,0):\n",
    "    df.iloc[:,i] = df.iloc[:,i].astype(int) # Integer\n",
    "    df.iloc[:,i] = df.iloc[:,i].astype('category') # Categories\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd66f6-7631-473d-a38a-79fd03432bd7",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f061a89d-9580-42e4-b535-fd1f08aacb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# # Split dataset into X_train and y_train\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = df.iloc[:168,:-1]\n",
    "X_test = df.iloc[168:,:-1]\n",
    "y_train = df.iloc[:168,-1]\n",
    "y_test = df.iloc[168:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9103efbf-c514-43ce-bbcf-1461f06720e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected columns: [16, 17, 18, 19, 20, 21, 22, 23]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      float64\n",
       "1      float64\n",
       "2      float64\n",
       "3      float64\n",
       "4      float64\n",
       "5      float64\n",
       "6      float64\n",
       "7      float64\n",
       "8      float64\n",
       "9      float64\n",
       "10     float64\n",
       "11     float64\n",
       "12     float64\n",
       "13     float64\n",
       "14     float64\n",
       "15     float64\n",
       "16    category\n",
       "17    category\n",
       "18    category\n",
       "19    category\n",
       "20    category\n",
       "21    category\n",
       "22    category\n",
       "23    category\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_columns = [16,17,18,19,20,21,22,23]\n",
    "print(\"Randomly selected columns:\", random_columns)\n",
    "\n",
    "# Create a copy of the saved_X_train variable\n",
    "cat_X_train = X_train.copy()\n",
    "\n",
    "# Process the randomly selected columns into categorical\n",
    "for col in random_columns:\n",
    "    cat_X_train[col] = X_train[col].round().astype(int).astype(\"category\")\n",
    "cat_X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23033ddb-5ee9-403b-ba3d-aac56ba00f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.523644</td>\n",
       "      <td>-0.273276</td>\n",
       "      <td>-0.116495</td>\n",
       "      <td>0.405252</td>\n",
       "      <td>1.197326</td>\n",
       "      <td>0.596777</td>\n",
       "      <td>0.538033</td>\n",
       "      <td>0.354814</td>\n",
       "      <td>0.309177</td>\n",
       "      <td>0.145222</td>\n",
       "      <td>0.231606</td>\n",
       "      <td>0.295207</td>\n",
       "      <td>0.234904</td>\n",
       "      <td>0.302261</td>\n",
       "      <td>0.069002</td>\n",
       "      <td>0.173969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.910844</td>\n",
       "      <td>0.797754</td>\n",
       "      <td>3.256096</td>\n",
       "      <td>1.803028</td>\n",
       "      <td>-1.190719</td>\n",
       "      <td>0.792517</td>\n",
       "      <td>1.187202</td>\n",
       "      <td>1.193782</td>\n",
       "      <td>0.342648</td>\n",
       "      <td>0.392967</td>\n",
       "      <td>0.252662</td>\n",
       "      <td>0.279862</td>\n",
       "      <td>0.483900</td>\n",
       "      <td>0.150756</td>\n",
       "      <td>0.269909</td>\n",
       "      <td>0.245511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.614962</td>\n",
       "      <td>1.306320</td>\n",
       "      <td>-0.156224</td>\n",
       "      <td>0.208113</td>\n",
       "      <td>-0.142056</td>\n",
       "      <td>0.331569</td>\n",
       "      <td>-0.139525</td>\n",
       "      <td>1.683484</td>\n",
       "      <td>0.249680</td>\n",
       "      <td>0.290193</td>\n",
       "      <td>0.046579</td>\n",
       "      <td>0.334307</td>\n",
       "      <td>0.276650</td>\n",
       "      <td>0.253384</td>\n",
       "      <td>0.387958</td>\n",
       "      <td>0.261832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.070894</td>\n",
       "      <td>0.848884</td>\n",
       "      <td>0.023903</td>\n",
       "      <td>0.232592</td>\n",
       "      <td>1.261394</td>\n",
       "      <td>0.678807</td>\n",
       "      <td>0.218641</td>\n",
       "      <td>0.276284</td>\n",
       "      <td>0.219636</td>\n",
       "      <td>0.111468</td>\n",
       "      <td>0.320295</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.289185</td>\n",
       "      <td>0.236309</td>\n",
       "      <td>0.126251</td>\n",
       "      <td>0.069706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.493572</td>\n",
       "      <td>0.730451</td>\n",
       "      <td>0.669870</td>\n",
       "      <td>0.671474</td>\n",
       "      <td>0.927129</td>\n",
       "      <td>-0.167649</td>\n",
       "      <td>0.735716</td>\n",
       "      <td>0.356140</td>\n",
       "      <td>0.188136</td>\n",
       "      <td>0.131791</td>\n",
       "      <td>0.024936</td>\n",
       "      <td>0.044220</td>\n",
       "      <td>0.276723</td>\n",
       "      <td>0.076487</td>\n",
       "      <td>0.334012</td>\n",
       "      <td>0.101293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.523644 -0.273276 -0.116495  0.405252  1.197326  0.596777  0.538033   \n",
       "1  1.910844  0.797754  3.256096  1.803028 -1.190719  0.792517  1.187202   \n",
       "2  0.614962  1.306320 -0.156224  0.208113 -0.142056  0.331569 -0.139525   \n",
       "3 -0.070894  0.848884  0.023903  0.232592  1.261394  0.678807  0.218641   \n",
       "4  0.493572  0.730451  0.669870  0.671474  0.927129 -0.167649  0.735716   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.354814  0.309177  0.145222  0.231606  0.295207  0.234904  0.302261   \n",
       "1  1.193782  0.342648  0.392967  0.252662  0.279862  0.483900  0.150756   \n",
       "2  1.683484  0.249680  0.290193  0.046579  0.334307  0.276650  0.253384   \n",
       "3  0.276284  0.219636  0.111468  0.320295  0.153200  0.289185  0.236309   \n",
       "4  0.356140  0.188136  0.131791  0.024936  0.044220  0.276723  0.076487   \n",
       "\n",
       "         14        15  \n",
       "0  0.069002  0.173969  \n",
       "1  0.269909  0.245511  \n",
       "2  0.387958  0.261832  \n",
       "3  0.126251  0.069706  \n",
       "4  0.334012  0.101293  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_X_train = cat_X_train.drop(labels=random_columns, axis=1)\n",
    "drop_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bba622d4-3e38-4d37-bbbd-dd8e145cd16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      float64\n",
      "1      float64\n",
      "2      float64\n",
      "3      float64\n",
      "4      float64\n",
      "5      float64\n",
      "6      float64\n",
      "7      float64\n",
      "8      float64\n",
      "9      float64\n",
      "10     float64\n",
      "11     float64\n",
      "12     float64\n",
      "13     float64\n",
      "14     float64\n",
      "15     float64\n",
      "16    category\n",
      "17    category\n",
      "18    category\n",
      "19    category\n",
      "20    category\n",
      "21    category\n",
      "22    category\n",
      "23    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the saved_X_test variable\n",
    "cat_X_test = X_test.copy()\n",
    "\n",
    "# Process the randomly selected columns\n",
    "for col in random_columns:\n",
    "    cat_X_test[col] = X_test[col].round().astype(int).astype(\"category\")\n",
    "\n",
    "# Check if 4 columns were converted to categorical\n",
    "print(cat_X_test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba53f46-38a3-4f34-8010-9892db9346f1",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35006063-2363-4a1e-bb9f-0fad6cd12d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(X_train.iloc[:,6], X_train.iloc[:,7], c=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03fa29-f69a-467f-94ef-efa338c5b518",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af1e90-3885-40b6-8e13-29f11d277850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/aceme/OneDrive/Documents/GitHub/BP24/IRIS.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5cca29-1ae7-4a48-82da-af6f916bb2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "x=df.drop(['species'],axis=1)[1:70] # 150 rows -> 70 rows to match with created imbalance\n",
    "y=df['species'][1:70] # creating imbalance\n",
    "count_class=df['species'][1:70].value_counts()\n",
    "bar_colors = ['tab:blue', 'tab:red']\n",
    "plt.bar(count_class.index, count_class.values, color=bar_colors)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(count_class.index, ['Iris-setosa', 'Iris-versicolor'])\n",
    "plt.show()\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c9e3f9-13f0-4aa7-a449-db175e42aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE to balance species classes\n",
    "smote=SMOTE(sampling_strategy='minority')  # generating synthetic samples for minority class\n",
    "x,y=smote.fit_resample(x,y) # re-sampling\n",
    "y.value_counts() # Looking at counts post-SMOTE algoirthm\n",
    "\n",
    "count_class=y.value_counts()\n",
    "bar_colors = ['tab:blue', 'tab:red']\n",
    "plt.bar(count_class.index, count_class.values, color=bar_colors)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(count_class.index, ['Iris-setosa', 'Iris-versicolor'])\n",
    "plt.show()\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e5730-941f-4cb6-9985-76cfa13e672d",
   "metadata": {},
   "source": [
    "# Edited function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53ad3c-aef3-460f-97f2-b45f4c229ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr 23 13:45:56 2024\n",
    "\n",
    "@author: cdiet\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def betterApplyAugmentationMethods(X_train, method, nrows, nvalues=None, unit=None, noise=None):\n",
    "    # If nvalues not specified, entire column is selected\n",
    "    if nvalues is None:\n",
    "        nvalues = X_train.shape[1] - 1\n",
    "\n",
    "    if str(method).lower() == 'pmone':\n",
    "        # Creates empty dataframe to store augmented data\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects nvalues amount of unique column indexes\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Iterates through augmentedData and applies plus or minus to randCols indexes\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in randCols:\n",
    "                if (random.randint(0, 1) == 0):\n",
    "                    augmentedDf.iloc[i, col] += unit\n",
    "                else:\n",
    "                    augmentedDf.iloc[i, col] -= unit\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'modpmone':\n",
    "        # Creates empty dataframe to store augmented data\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects nvalues amount of unique column indexes\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Iterates through augmentedData and applies plus or minus to randCols indexes\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in randCols:\n",
    "                colMax = X_train.iloc[:, col].max()\n",
    "                colMin = X_train.iloc[:, col].min()\n",
    "\n",
    "                if (augmentedDf.iloc[i, col] + unit < colMax and augmentedDf.iloc[i, col] - unit > colMin):\n",
    "                    if (random.randint(0, 1) == 0):\n",
    "                        if (augmentedDf.iloc[i, col] + unit <= colMax):\n",
    "                            augmentedDf.iloc[i, col] += unit\n",
    "                        else:\n",
    "                            augmentedDf.iloc[i, col] -= unit\n",
    "                    else:\n",
    "                        if (augmentedDf.iloc[i, col] - unit >= colMin):\n",
    "                            augmentedDf.iloc[i, col] -= unit\n",
    "                        else:\n",
    "                            augmentedDf.iloc[i, col] += unit\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'gausnoise':\n",
    "        # Creates empty dataframe to hold augmented rows\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Selects random rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects random unique column index\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Applies Gaussian noise to randCols values stored in array\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for cols in randCols:\n",
    "                augmentedDf.iloc[i, cols] += np.random.normal(0, noise)\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'modgausnoise':\n",
    "        # Creates an empty dataframe to hold augmented observations\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects unique column indexs from data\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Appends randomly selected rows from data to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Generates Gaussian distribution based on columns summary statistics\n",
    "        # Swaps value with random value in generated Gaussian distribution\n",
    "        for col in randCols:\n",
    "            for i in range(augmentedDf.shape[0]):\n",
    "                mean = augmentedDf[col].mean()\n",
    "                stDev = augmentedDf[col].std()\n",
    "\n",
    "                augmentedDf.iloc[i, col] = np.random.normal(mean, stDev)\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'randswap':\n",
    "        # Creates empty dataframe to store augmented rows\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Copies nrows from original data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Picks UNIQUE column indexes to swap\n",
    "        columnIndexSwaps = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Swaps augmentedDf column value from same column in data\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in columnIndexSwaps:\n",
    "                randValue = X_train.iloc[random.randint(0, X_train.shape[0] - 1), col]\n",
    "\n",
    "                augmentedDf.iloc[i, col] = randValue\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    else:\n",
    "        print(\"Method not found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generateLabels(X_train, Y_train, augmented):\n",
    "    # import the class\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # instantiate the model (using the default parameters)\n",
    "    # random.seed(1)\n",
    "    logreg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "    # fit the model with data\n",
    "\n",
    "    # print(y_train)\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    # create the prediction\n",
    "    augmented_labels = pd.DataFrame(logreg.predict(augmented))\n",
    "\n",
    "    Y_train = pd.concat([Y_train, augmented_labels], axis=0, ignore_index=True)\n",
    "\n",
    "    X_train = pd.concat([X_train, augmented], axis=0, ignore_index=True)\n",
    "\n",
    "    return X_train, Y_train\n",
    "\n",
    "\n",
    "def runClassifier(X_train, Y_train, X_test, Y_test, classifier):\n",
    "    # Creates accuracy table dataframe\n",
    "    results_df = pd.DataFrame(columns=[\"Accuracy\", \"Mean Absolute Error\", \"Rooted Mean Square Error\", \"F1 Score\"])\n",
    "\n",
    "    if classifier == \"kNN\":\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "\n",
    "        knn.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "        # Predict on dataset which model has not seen before\n",
    "        predicted_values = knn.predict(X_test)\n",
    "\n",
    "    elif classifier == \"D_tree\":\n",
    "        clf_gini = DecisionTreeClassifier(criterion=\"gini\",\n",
    "                                          random_state=100, max_depth=3, min_samples_leaf=5)\n",
    "\n",
    "        clf_gini.fit(X_train, Y_train)\n",
    "\n",
    "        predicted_values = clf_gini.predict(X_test)\n",
    "\n",
    "\n",
    "    elif classifier == \"Naive_bayes\":\n",
    "\n",
    "        classifier = GaussianNB()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "\n",
    "        predicted_values = classifier.predict(X_test)\n",
    "\n",
    "    elif classifier == \"ANN\":\n",
    "        # Performing Feature Scaling\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "\n",
    "        # Initializing Artificial Neural Network\n",
    "        ann = tf.keras.models.Sequential()\n",
    "\n",
    "        # Adding Hidden Layers\n",
    "        ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))\n",
    "        ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))\n",
    "\n",
    "        # Adding output layers\n",
    "        ann.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "        # compiling the Artificial Neural Network\n",
    "        ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "        # Fitting the Artificial Neural Network\n",
    "        ann.fit(X_train, Y_train, batch_size=32, epochs=100)\n",
    "\n",
    "        # Generate the predicted labels\n",
    "        first_predicted_values = ann.predict(X_test)\n",
    "        second_predicted_labels = first_predicted_values > .5\n",
    "        final_predicted_labels = second_predicted_labels * 1\n",
    "        predicted_values = final_predicted_labels\n",
    "\n",
    "    # SVM\n",
    "    elif classifier == \"SVM\":\n",
    "        # random.seed(1)\n",
    "        svm = SVC(gamma=2, C=1, kernel='linear', max_iter=1000000, random_state=0)\n",
    "\n",
    "        # fit the model with data\n",
    "        # svm.fit(X_train,y_train)\n",
    "        svm.fit(X_train, Y_train)\n",
    "        predicted_values = svm.predict(X_test)\n",
    "\n",
    "        # SVM\n",
    "    elif classifier == \"xgboost\":\n",
    "        # Create model instance\n",
    "        bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic', enable_categorical=True)\n",
    "        \n",
    "        # fit model with the training data\n",
    "        bst.fit(X_train, y_train)\n",
    "        \n",
    "        # make predictions for the test dataset\n",
    "        predicted_values = bst.predict(X_test)\n",
    "\n",
    "    else:\n",
    "        print(\"Unknown classifier\")\n",
    "        return None\n",
    "\n",
    "    # Accuracy\n",
    "    acc = skm.accuracy_score(Y_test, predicted_values)\n",
    "    mae_accuracy = skm.mean_absolute_error(Y_test, predicted_values)\n",
    "    #rmse_accuracy = skm.root_mean_squared_error(Y_test, predicted_values)\n",
    "    f1_accuracy = skm.f1_score(Y_test, predicted_values)\n",
    "\n",
    "    # Appends accuracies to accuracy table\n",
    "    results_df.loc[1, 'Accuracy'] = acc\n",
    "    results_df.loc[1, 'Mean Absolute Error'] = mae_accuracy\n",
    "    #results_df.loc[1, 'Rooted Mean Square Error'] = rmse_accuracy\n",
    "    results_df.loc[1, 'F1 Score'] = f1_accuracy\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "generatedGaussianDistrubutions Inputs\n",
    "\n",
    "nrows: Number of rows\n",
    "ncolumns: Number of columns\n",
    "median1: First Gaussian distribution median (center)\n",
    "median2: Second Gaussian distribution median (center)\n",
    "spread1: First Gaussian distrbiution spread\n",
    "spread2: Second Gaussian distribution spread\n",
    "\n",
    "Note:\n",
    "if label == 0, first Gaussian distribution\n",
    "if label == 1, second Gaussian distribution\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generateGaussianDistributions(nrows, ncolumns, median1, median2, spread1, spread2):\n",
    "    # Creates first Gaussian distribution\n",
    "    label1 = pd.DataFrame(np.random.normal(median1, spread1, size=(int(nrows / 2), ncolumns)))\n",
    "    # Adds new column for label\n",
    "    label1['label'] = 0\n",
    "\n",
    "    # Creates second Gaussian distribution\n",
    "    label2 = pd.DataFrame(np.random.normal(median2, spread2, size=(int(nrows / 2), ncolumns)))\n",
    "    # Adds new column for label\n",
    "    label2['label'] = 1\n",
    "\n",
    "    # Combines both Gaussian distributions\n",
    "    df = pd.concat([label1, label2])\n",
    "\n",
    "    # Shuffles Gaussian distributions\n",
    "    shuffled_df = pd.DataFrame(np.random.permutation(df))\n",
    "\n",
    "    return shuffled_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "superFunction applies all methods from the flowchart: augmentation, \n",
    "interpretable/uninterpretable classifiers, and accuracy, taking all inputs from\n",
    "these functions and outputs the accuracy of the augmented data.\n",
    "\n",
    "Inputs:\n",
    "    data: A dataframe where the labels are the LAST column\n",
    "    method: The augmentation method the user wants to use for the data\n",
    "    nrows: How many output augmentation rows are wanted\n",
    "    nvalues: The number of values in each row that need to be augmented\n",
    "    classifier: The classifier the user wants to use\n",
    "    unit(optional): Only for the pmOne augmentation method and is the unit the \n",
    "    augmented data will differ from original data by\n",
    "    noise(optional): Only for the gausNoise augmentation method and denotes the\n",
    "    percent by which the augmented data varies from original data\n",
    "\n",
    "\n",
    "Outputs:\n",
    "    Returns two dataframes of original and augmented data accuracy measures\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def superFunction(data, method, nrows, nvalues, classifier, unit=None, noise=None):\n",
    "    X = data.drop(data.shape[1] - 1, axis=1)\n",
    "    Y = data[data.shape[1] - 1]\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    original_accuracies = runClassifier(X_train, Y_train, X_test, Y_test, classifier)\n",
    "\n",
    "    # Applies augmentation method to X_train\n",
    "    augmented = betterApplyAugmentationMethods(X_train, method, nrows, nvalues, unit=unit, noise=noise)\n",
    "\n",
    "    # Generates labels and concat to X_train and Y_train\n",
    "    X_train, Y_train = generateLabels(X_train, Y_train, augmented)\n",
    "\n",
    "    augmented_accuracies = runClassifier(X_train, Y_train, X_test, Y_test, classifier)\n",
    "\n",
    "    return original_accuracies, augmented_accuracies\n",
    "\n",
    "\n",
    "df = generateGaussianDistributions(500, 150, 0, 0.25, 1, 1)\n",
    "\n",
    "# Prints the F1 Score of the augmented data for each augmentation method\n",
    "# print('pmOne:', superFunction(df, \"pmOne\", 200, 15, \"kNN\", unit=0.1)[1]['F1 Score'][1])\n",
    "# print('randSwap:', superFunction(df, \"randSwap\", 200, 15, \"kNN\")[1]['F1 Score'][1])\n",
    "print('gausNoise:', superFunction(df, \"gausNoise\", 200, 15, \"kNN\", noise=0.1)[1]['F1 Score'][1])\n",
    "# print('modPmOne:', superFunction(df, \"modpmone\", 200, 15, \"kNN\", unit=0.1)[1]['F1 Score'][1])\n",
    "# print('modGausNoise:', superFunction(df, \"modgausnoise\", 200, 15, \"kNN\")[1]['F1 Score'][1])\n",
    "\n",
    "\n",
    "orig, new = superFunction(df, \"pmOne\", 200, 15, \"kNN\", unit=0.1)\n",
    "print('No Augmentation:', orig['F1 Score'][1])\n",
    "print('w/ Augmentation:', new['F1 Score'][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38819c3b-78e3-4e16-bf47-4bc76b6cc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betterApplyAugmentationMethods(X_train, method, nrows, nvalues=None, unit=None, noise=None):\n",
    "    # If nvalues not specified, entire column is selected\n",
    "    if nvalues is None:\n",
    "        nvalues = X_train.shape[1] - 1\n",
    "\n",
    "    if str(method).lower() == 'gausnoise':\n",
    "        # Creates empty dataframe to hold augmented rows\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Selects random rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects random unique column index\n",
    "        rng = np.random.default_rng(42672244757883671377369755671764847711)\n",
    "        randCols = rng.choice(np.arange(X_train.shape[1] - 1), size=nvalues)\n",
    "\n",
    "        # Applies Gaussian noise to randCols values stored in array\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for cols in randCols:\n",
    "                augmentedDf.iloc[i, cols] += np.random.normal(0, noise)\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    else:\n",
    "        print(\"Method not found\")\n",
    "        return None\n",
    "        \n",
    "stacked_aug = betterApplyAugmentationMethods(drop_X_train, 'gausnoise', 240, nvalues=24, noise=0.1)\n",
    "stacked_aug\n",
    "# np.savetxt('stacked_aug.csv', stacked_aug, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6b330-472c-4167-a8d1-a83072ab2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES: \n",
    "# nrows; # of rows you want created \n",
    "# nvalues: # of values in column to change\n",
    "# unit: unit you want to change your values by \n",
    "\n",
    "# Converting the string column to int\n",
    "species_mapping = {\n",
    "    'Iris-setosa': 1,\n",
    "    'Iris-versicolor': 2,\n",
    "    'Iris-virginica': 3\n",
    "}\n",
    "df['species'] = df['species'].map(species_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcde0c3-12f5-44b6-b169-3fd837c3eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['species'], axis=1)\n",
    "y = df['species']\n",
    "\n",
    "# # Train test split \n",
    "# X_train, X_test, y_train, y_test = train_test_split(x,y, \n",
    "#                                                     random_state=0,\n",
    "#                                                     test_size=0.25) \n",
    "\n",
    "# pmone\n",
    "aug_dat1 = betterApplyAugmentationMethods(X_train, method='pmone', nrows=20, nvalues=None, unit=1, noise=None)\n",
    "print(aug_dat1)\n",
    "\n",
    "# modpmone\n",
    "aug_dat2 = betterApplyAugmentationMethods(X_train, method='modpmone', nrows=20, nvalues=None, unit=1, noise=None)\n",
    "print(aug_dat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d613336-d977-4810-b464-2164f82cbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "superFunction(data, method='gausnoise', nrows=10, nvalues=10, classifier='xgboost', unit=None, noise=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
