{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde92cb5-f744-48b6-8f54-04af24047e43",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47e8ec89-6557-4886-ba85-d89a18151aa7",
   "metadata": {},
       "4    0.820657  0.338822  1.0  0.068901  0.803221  0.408169  0.542404  0.0   \n",
   "outputs": [],
   "source": [
    "###### import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv(\"/Users/elleemortensen/Documents/GitHub/BP24/Ellee/Data/Stacked/stacked_orig.csv\", header=None)\n",
    "\n",
    "# # Creating NumPy array\n",
    "# array = np.array(data)\n",
    "\n",
    "# # Converting to Pandas DataFrame\n",
    "# df = pd.DataFrame(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be45cd6-ec01-4103-b03a-87fbb8c40d10",
   "metadata": {},
   "source": [
    "# Convert 25/150 Columns into NON-CONSECUTIVE Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17e27c-9c21-4514-93cf-ccdbb20c584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(-5,0):\n",
    "#     df.iloc[:,i] = df.iloc[:,i].astype(int) # Integer\n",
    "#     df.iloc[:,i] = df.iloc[:,i].astype('category') # Categories\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd66f6-7631-473d-a38a-79fd03432bd7",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f061a89d-9580-42e4-b535-fd1f08aacb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# # Split dataset into X_train and y_train\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = df.iloc[:168,:-1]\n",
    "X_test = df.iloc[168:,:-1]\n",
    "y_train = df.iloc[:168,-1]\n",
    "y_test = df.iloc[168:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9103efbf-c514-43ce-bbcf-1461f06720e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_columns = [16,17,18,19,20,21,22,23]\n",
    "\n",
    "# Create a copy of the saved_X_train variable\n",
    "cat_X_train = X_train.copy()\n",
    "\n",
    "# Process the randomly selected columns into categorical\n",
    "for col in random_columns:\n",
    "    cat_X_train[col] = X_train[col].round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba53f46-38a3-4f34-8010-9892db9346f1",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35006063-2363-4a1e-bb9f-0fad6cd12d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(X_train.iloc[:,6], X_train.iloc[:,7], c=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03fa29-f69a-467f-94ef-efa338c5b518",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af1e90-3885-40b6-8e13-29f11d277850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/aceme/OneDrive/Documents/GitHub/BP24/IRIS.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5cca29-1ae7-4a48-82da-af6f916bb2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "x=df.drop(['species'],axis=1)[1:70] # 150 rows -> 70 rows to match with created imbalance\n",
    "y=df['species'][1:70] # creating imbalance\n",
    "count_class=df['species'][1:70].value_counts()\n",
    "bar_colors = ['tab:blue', 'tab:red']\n",
    "plt.bar(count_class.index, count_class.values, color=bar_colors)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(count_class.index, ['Iris-setosa', 'Iris-versicolor'])\n",
    "plt.show()\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c9e3f9-13f0-4aa7-a449-db175e42aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE to balance species classes\n",
    "smote=SMOTE(sampling_strategy='minority')  # generating synthetic samples for minority class\n",
    "x,y=smote.fit_resample(x,y) # re-sampling\n",
    "y.value_counts() # Looking at counts post-SMOTE algoirthm\n",
    "\n",
    "count_class=y.value_counts()\n",
    "bar_colors = ['tab:blue', 'tab:red']\n",
    "plt.bar(count_class.index, count_class.values, color=bar_colors)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(count_class.index, ['Iris-setosa', 'Iris-versicolor'])\n",
    "plt.show()\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e5730-941f-4cb6-9985-76cfa13e672d",
   "metadata": {},
   "source": [
    "# Edited function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53ad3c-aef3-460f-97f2-b45f4c229ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr 23 13:45:56 2024\n",
    "\n",
    "@author: cdiet\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def betterApplyAugmentationMethods(X_train, method, nrows, nvalues=None, unit=None, noise=None):\n",
    "    # If nvalues not specified, entire column is selected\n",
    "    if nvalues is None:\n",
    "        nvalues = X_train.shape[1] - 1\n",
    "\n",
    "    if str(method).lower() == 'pmone':\n",
    "        # Creates empty dataframe to store augmented data\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects nvalues amount of unique column indexes\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Iterates through augmentedData and applies plus or minus to randCols indexes\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in randCols:\n",
    "                if (random.randint(0, 1) == 0):\n",
    "                    augmentedDf.iloc[i, col] += unit\n",
    "                else:\n",
    "                    augmentedDf.iloc[i, col] -= unit\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'modpmone':\n",
    "        # Creates empty dataframe to store augmented data\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects nvalues amount of unique column indexes\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Iterates through augmentedData and applies plus or minus to randCols indexes\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in randCols:\n",
    "                colMax = X_train.iloc[:, col].max()\n",
    "                colMin = X_train.iloc[:, col].min()\n",
    "\n",
    "                if (augmentedDf.iloc[i, col] + unit < colMax and augmentedDf.iloc[i, col] - unit > colMin):\n",
    "                    if (random.randint(0, 1) == 0):\n",
    "                        if (augmentedDf.iloc[i, col] + unit <= colMax):\n",
    "                            augmentedDf.iloc[i, col] += unit\n",
    "                        else:\n",
    "                            augmentedDf.iloc[i, col] -= unit\n",
    "                    else:\n",
    "                        if (augmentedDf.iloc[i, col] - unit >= colMin):\n",
    "                            augmentedDf.iloc[i, col] -= unit\n",
    "                        else:\n",
    "                            augmentedDf.iloc[i, col] += unit\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'gausnoise':\n",
    "        # Creates empty dataframe to hold augmented rows\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Selects random rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects random unique column index\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Applies Gaussian noise to randCols values stored in array\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for cols in randCols:\n",
    "                augmentedDf.iloc[i, cols] += np.random.normal(0, noise)\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'modgausnoise':\n",
    "        # Creates an empty dataframe to hold augmented observations\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects unique column indexs from data\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Appends randomly selected rows from data to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Generates Gaussian distribution based on columns summary statistics\n",
    "        # Swaps value with random value in generated Gaussian distribution\n",
    "        for col in randCols:\n",
    "            for i in range(augmentedDf.shape[0]):\n",
    "                mean = augmentedDf[col].mean()\n",
    "                stDev = augmentedDf[col].std()\n",
    "\n",
    "                augmentedDf.iloc[i, col] = np.random.normal(mean, stDev)\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'randswap':\n",
    "        # Creates empty dataframe to store augmented rows\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Copies nrows from original data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Picks UNIQUE column indexes to swap\n",
    "        columnIndexSwaps = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Swaps augmentedDf column value from same column in data\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in columnIndexSwaps:\n",
    "                randValue = X_train.iloc[random.randint(0, X_train.shape[0] - 1), col]\n",
    "\n",
    "                augmentedDf.iloc[i, col] = randValue\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    else:\n",
    "        print(\"Method not found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generateLabels(X_train, Y_train, augmented):\n",
    "    # import the class\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # instantiate the model (using the default parameters)\n",
    "    # random.seed(1)\n",
    "    logreg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "    # fit the model with data\n",
    "\n",
    "    # print(y_train)\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    # create the prediction\n",
    "    augmented_labels = pd.DataFrame(logreg.predict(augmented))\n",
    "\n",
    "    Y_train = pd.concat([Y_train, augmented_labels], axis=0, ignore_index=True)\n",
    "\n",
    "    X_train = pd.concat([X_train, augmented], axis=0, ignore_index=True)\n",
    "\n",
    "    return X_train, Y_train\n",
    "\n",
    "\n",
    "def runClassifier(X_train, Y_train, X_test, Y_test, classifier):\n",
    "    # Creates accuracy table dataframe\n",
    "    results_df = pd.DataFrame(columns=[\"Accuracy\", \"Mean Absolute Error\", \"Rooted Mean Square Error\", \"F1 Score\"])\n",
    "\n",
    "    if classifier == \"kNN\":\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "\n",
    "        knn.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "        # Predict on dataset which model has not seen before\n",
    "        predicted_values = knn.predict(X_test)\n",
    "\n",
    "    elif classifier == \"D_tree\":\n",
    "        clf_gini = DecisionTreeClassifier(criterion=\"gini\",\n",
    "                                          random_state=100, max_depth=3, min_samples_leaf=5)\n",
    "\n",
    "        clf_gini.fit(X_train, Y_train)\n",
    "\n",
    "        predicted_values = clf_gini.predict(X_test)\n",
    "\n",
    "\n",
    "    elif classifier == \"Naive_bayes\":\n",
    "\n",
    "        classifier = GaussianNB()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "\n",
    "        predicted_values = classifier.predict(X_test)\n",
    "\n",
    "    elif classifier == \"ANN\":\n",
    "        # Performing Feature Scaling\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "\n",
    "        # Initializing Artificial Neural Network\n",
    "        ann = tf.keras.models.Sequential()\n",
    "\n",
    "        # Adding Hidden Layers\n",
    "        ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))\n",
    "        ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))\n",
    "\n",
    "        # Adding output layers\n",
    "        ann.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "        # compiling the Artificial Neural Network\n",
    "        ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "        # Fitting the Artificial Neural Network\n",
    "        ann.fit(X_train, Y_train, batch_size=32, epochs=100)\n",
    "\n",
    "        # Generate the predicted labels\n",
    "        first_predicted_values = ann.predict(X_test)\n",
    "        second_predicted_labels = first_predicted_values > .5\n",
    "        final_predicted_labels = second_predicted_labels * 1\n",
    "        predicted_values = final_predicted_labels\n",
    "\n",
    "    # SVM\n",
    "    elif classifier == \"SVM\":\n",
    "        # random.seed(1)\n",
    "        svm = SVC(gamma=2, C=1, kernel='linear', max_iter=1000000, random_state=0)\n",
    "\n",
    "        # fit the model with data\n",
    "        # svm.fit(X_train,y_train)\n",
    "        svm.fit(X_train, Y_train)\n",
    "        predicted_values = svm.predict(X_test)\n",
    "\n",
    "        # SVM\n",
    "    elif classifier == \"xgboost\":\n",
    "        # Create model instance\n",
    "        bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic', enable_categorical=True)\n",
    "        \n",
    "        # fit model with the training data\n",
    "        bst.fit(X_train, y_train)\n",
    "        \n",
    "        # make predictions for the test dataset\n",
    "        predicted_values = bst.predict(X_test)\n",
    "\n",
    "    else:\n",
    "        print(\"Unknown classifier\")\n",
    "        return None\n",
    "\n",
    "    # Accuracy\n",
    "    acc = skm.accuracy_score(Y_test, predicted_values)\n",
    "    mae_accuracy = skm.mean_absolute_error(Y_test, predicted_values)\n",
    "    #rmse_accuracy = skm.root_mean_squared_error(Y_test, predicted_values)\n",
    "    f1_accuracy = skm.f1_score(Y_test, predicted_values)\n",
    "\n",
    "    # Appends accuracies to accuracy table\n",
    "    results_df.loc[1, 'Accuracy'] = acc\n",
    "    results_df.loc[1, 'Mean Absolute Error'] = mae_accuracy\n",
    "    #results_df.loc[1, 'Rooted Mean Square Error'] = rmse_accuracy\n",
    "    results_df.loc[1, 'F1 Score'] = f1_accuracy\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "generatedGaussianDistrubutions Inputs\n",
    "\n",
    "nrows: Number of rows\n",
    "ncolumns: Number of columns\n",
    "median1: First Gaussian distribution median (center)\n",
    "median2: Second Gaussian distribution median (center)\n",
    "spread1: First Gaussian distrbiution spread\n",
    "spread2: Second Gaussian distribution spread\n",
    "\n",
    "Note:\n",
    "if label == 0, first Gaussian distribution\n",
    "if label == 1, second Gaussian distribution\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generateGaussianDistributions(nrows, ncolumns, median1, median2, spread1, spread2):\n",
    "    # Creates first Gaussian distribution\n",
    "    label1 = pd.DataFrame(np.random.normal(median1, spread1, size=(int(nrows / 2), ncolumns)))\n",
    "    # Adds new column for label\n",
    "    label1['label'] = 0\n",
    "\n",
    "    # Creates second Gaussian distribution\n",
    "    label2 = pd.DataFrame(np.random.normal(median2, spread2, size=(int(nrows / 2), ncolumns)))\n",
    "    # Adds new column for label\n",
    "    label2['label'] = 1\n",
    "\n",
    "    # Combines both Gaussian distributions\n",
    "    df = pd.concat([label1, label2])\n",
    "\n",
    "    # Shuffles Gaussian distributions\n",
    "    shuffled_df = pd.DataFrame(np.random.permutation(df))\n",
    "\n",
    "    return shuffled_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "superFunction applies all methods from the flowchart: augmentation, \n",
    "interpretable/uninterpretable classifiers, and accuracy, taking all inputs from\n",
    "these functions and outputs the accuracy of the augmented data.\n",
    "\n",
    "Inputs:\n",
    "    data: A dataframe where the labels are the LAST column\n",
    "    method: The augmentation method the user wants to use for the data\n",
    "    nrows: How many output augmentation rows are wanted\n",
    "    nvalues: The number of values in each row that need to be augmented\n",
    "    classifier: The classifier the user wants to use\n",
    "    unit(optional): Only for the pmOne augmentation method and is the unit the \n",
    "    augmented data will differ from original data by\n",
    "    noise(optional): Only for the gausNoise augmentation method and denotes the\n",
    "    percent by which the augmented data varies from original data\n",
    "\n",
    "\n",
    "Outputs:\n",
    "    Returns two dataframes of original and augmented data accuracy measures\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def superFunction(data, method, nrows, nvalues, classifier, unit=None, noise=None):\n",
    "    X = data.drop(data.shape[1] - 1, axis=1)\n",
    "    Y = data[data.shape[1] - 1]\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    original_accuracies = runClassifier(X_train, Y_train, X_test, Y_test, classifier)\n",
    "\n",
    "    # Applies augmentation method to X_train\n",
    "    augmented = betterApplyAugmentationMethods(X_train, method, nrows, nvalues, unit=unit, noise=noise)\n",
    "\n",
    "    # Generates labels and concat to X_train and Y_train\n",
    "    X_train, Y_train = generateLabels(X_train, Y_train, augmented)\n",
    "\n",
    "    augmented_accuracies = runClassifier(X_train, Y_train, X_test, Y_test, classifier)\n",
    "\n",
    "    return original_accuracies, augmented_accuracies\n",
    "\n",
    "\n",
    "df = generateGaussianDistributions(500, 150, 0, 0.25, 1, 1)\n",
    "\n",
    "# Prints the F1 Score of the augmented data for each augmentation method\n",
    "# print('pmOne:', superFunction(df, \"pmOne\", 200, 15, \"kNN\", unit=0.1)[1]['F1 Score'][1])\n",
    "# print('randSwap:', superFunction(df, \"randSwap\", 200, 15, \"kNN\")[1]['F1 Score'][1])\n",
    "print('gausNoise:', superFunction(df, \"gausNoise\", 200, 15, \"kNN\", noise=0.1)[1]['F1 Score'][1])\n",
    "# print('modPmOne:', superFunction(df, \"modpmone\", 200, 15, \"kNN\", unit=0.1)[1]['F1 Score'][1])\n",
    "# print('modGausNoise:', superFunction(df, \"modgausnoise\", 200, 15, \"kNN\")[1]['F1 Score'][1])\n",
    "\n",
    "\n",
    "orig, new = superFunction(df, \"pmOne\", 200, 15, \"kNN\", unit=0.1)\n",
    "print('No Augmentation:', orig['F1 Score'][1])\n",
    "print('w/ Augmentation:', new['F1 Score'][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38819c3b-78e3-4e16-bf47-4bc76b6cc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betterApplyAugmentationMethods(X_train, method, nrows, nvalues=None, unit=None, noise=None):\n",
    "    # If nvalues not specified, entire column is selected\n",
    "    if nvalues is None:\n",
    "        nvalues = X_train.shape[1] - 1\n",
    "\n",
    "    if str(method).lower() == 'randswap':\n",
    "        # Creates empty dataframe to store augmented rows\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Copies nrows from original data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, cat_X_train.iloc[[random.randint(0, cat_X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Picks UNIQUE column indexes to swap\n",
    "        rng = np.random.default_rng(42672244757883671377369755671764847711)\n",
    "        columnIndexSwaps = rng.choice(range(0, cat_X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Swaps augmentedDf column value from same column in data\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in columnIndexSwaps:\n",
    "                randValue = cat_X_train.iloc[random.randint(0, cat_X_train.shape[0] - 1), col]\n",
    "\n",
    "                augmentedDf.iloc[i, col] = randValue\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    else:\n",
    "        print(\"Method not found\")\n",
    "        return None\n",
    "\n",
    "def generateLabels(cat_X_train, y_train, augmented):\n",
    "    # instantiate the model (using the default parameters)\n",
    "    # random.seed(1)\n",
    "    logreg = LogisticRegression(max_iter=10000)\n",
    "    \n",
    "    # fit the model with data\n",
    "    logreg.fit(cat_X_train, y_train)\n",
    "    \n",
    "    # create the prediction\n",
    "    augmented_labels = pd.DataFrame(logreg.predict(augmented))\n",
    "    \n",
    "    y_train = pd.concat([y_train, augmented_labels], axis=0, ignore_index=True)\n",
    "    \n",
    "    X_train = pd.concat([cat_X_train, augmented], axis=0, ignore_index=True)\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d4457b8-deab-4202-9eb7-17c33789eee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6   \\\n",
      "0    0.088773  0.737501  0.010419  0.848826  2.103859 -0.084637 -0.102792   \n",
      "1    0.449472  0.494923  3.618190 -0.168263  0.927165  4.822020  0.077651   \n",
      "2    1.810712  0.775600  1.293492  0.581579 -0.226484  7.152726  0.158751   \n",
      "3    1.088254 -0.239871  0.023903  1.519232  0.447489 -0.092557  0.890868   \n",
      "4    0.701442  1.184965  3.697365  5.298132  0.460860 -0.178326  0.193822   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "235  2.463306  2.193933  0.492536  0.623638  0.918059  2.909277 -0.291856   \n",
      "236  1.119396  0.414520 -0.241327  1.139724  0.617746  1.292093  3.133432   \n",
      "237  1.029102  0.204681  0.096641  0.133467  2.117463 -0.071907 -1.885890   \n",
      "238  0.663965 -0.357724  1.627623  2.537089  0.178052  1.082511  1.721368   \n",
      "239  0.217072  2.343969  2.516844  1.467715  1.438462  0.448616  0.387924   \n",
      "\n",
      "           7         8         9   ...        14        15  16  17  18  19  \\\n",
      "0    1.076791  0.347838  0.052344  ...  0.380649  0.074173   1   2   0   0   \n",
      "1    0.771103  0.007961  0.276776  ...  0.061120  0.206302   1   0   1   1   \n",
      "2    2.563062  0.326399  0.172998  ...  0.147216  0.375684   1   0   2   1   \n",
      "3    0.901937  0.217546  0.100474  ...  0.460142  0.333423   0   2   3   2   \n",
      "4    0.479532  0.337822  0.372134  ...  0.423306  0.160901   1   0   0   1   \n",
      "..        ...       ...       ...  ...       ...       ...  ..  ..  ..  ..   \n",
      "235  2.139564  0.482547  0.385978  ...  0.495238  0.272726   1   0   0   1   \n",
      "236  1.092391  0.127967  0.181252  ...  0.199058  0.331164   0   0   0   2   \n",
      "237 -1.098137  0.052093  0.095793  ...  0.181623  0.051570   2   0   1   0   \n",
      "238  2.703803  0.432441  0.358854  ...  0.301140  0.252321   0   1   0   0   \n",
      "239 -0.802491  0.464559  0.449139  ...  0.286223  0.448680   1   0   2   1   \n",
      "\n",
      "     20  21  22  23  \n",
      "0     0   1   0   1  \n",
      "1     2   0   3   0  \n",
      "2     0   3   2   0  \n",
      "3     0   1   2   0  \n",
      "4     0   1   3   1  \n",
      "..   ..  ..  ..  ..  \n",
      "235   1   1   1   0  \n",
      "236   3   0   1   0  \n",
      "237   2   1   1   1  \n",
      "238   1   1   2   0  \n",
      "239   0   0   0   0  \n",
      "\n",
      "[240 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "stacked_aug = betterApplyAugmentationMethods(cat_X_train, 'randswap', 240, nvalues=24, noise=0.1)\n",
    "print(stacked_aug)\n",
    "np.savetxt('stacked_randswap.csv', stacked_aug , delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6b330-472c-4167-a8d1-a83072ab2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES: \n",
    "# nrows; # of rows you want created \n",
    "# nvalues: # of values in column to change\n",
    "# unit: unit you want to change your values by \n",
    "\n",
    "# Converting the string column to int\n",
    "species_mapping = {\n",
    "    'Iris-setosa': 1,\n",
    "    'Iris-versicolor': 2,\n",
    "    'Iris-virginica': 3\n",
    "}\n",
    "df['species'] = df['species'].map(species_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcde0c3-12f5-44b6-b169-3fd837c3eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['species'], axis=1)\n",
    "y = df['species']\n",
    "\n",
    "# # Train test split \n",
    "# X_train, X_test, y_train, y_test = train_test_split(x,y, \n",
    "#                                                     random_state=0,\n",
    "#                                                     test_size=0.25) \n",
    "\n",
    "# pmone\n",
    "aug_dat1 = betterApplyAugmentationMethods(X_train, method='pmone', nrows=20, nvalues=None, unit=1, noise=None)\n",
    "print(aug_dat1)\n",
    "\n",
    "# modpmone\n",
    "aug_dat2 = betterApplyAugmentationMethods(X_train, method='modpmone', nrows=20, nvalues=None, unit=1, noise=None)\n",
    "print(aug_dat2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
