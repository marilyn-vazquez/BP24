{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99eefae9-6941-40b5-b4ee-753571ceac52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _check_precomputed\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassifierMixin, _fit_context\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     ArgKminClassMode,\n\u001b[1;32m     20\u001b[0m     RadiusNeighborsClassMode,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOptions\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "\"\"\"Nearest Neighbor Classification\"\"\"\n",
    "\n",
    "# Authors: Jake Vanderplas <vanderplas@astro.washington.edu>\n",
    "#          Fabian Pedregosa <fabian.pedregosa@inria.fr>\n",
    "#          Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "#          Sparseness support by Lars Buitinck\n",
    "#          Multi-output support by Arnaud Joly <a.joly@ulg.ac.be>\n",
    "#\n",
    "# License: BSD 3 clause (C) INRIA, University of Amsterdam\n",
    "import warnings\n",
    "from numbers import Integral\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors._base import _check_precomputed\n",
    "\n",
    "from ._base import ClassifierMixin, _fit_context\n",
    "from ..metrics._pairwise_distances_reduction import (\n",
    "    ArgKminClassMode,\n",
    "    RadiusNeighborsClassMode,\n",
    ")\n",
    "from ..utils._param_validation import StrOptions\n",
    "from ..utils.arrayfuncs import _all_with_any_reduction_axis_1\n",
    "from ..utils.extmath import weighted_mode\n",
    "from ..utils.fixes import _mode\n",
    "from ..utils.validation import _is_arraylike, _num_samples, check_is_fitted\n",
    "from ..base import KNeighborsMixin, NeighborsBase, RadiusNeighborsMixin, _get_weights\n",
    "\n",
    "\n",
    "def _adjusted_metric(metric, metric_kwargs, p=None):\n",
    "    metric_kwargs = metric_kwargs or {}\n",
    "    if metric == \"minkowski\":\n",
    "        metric_kwargs[\"p\"] = p\n",
    "        if p == 2:\n",
    "            metric = \"euclidean\"\n",
    "    return metric, metric_kwargs\n",
    "\n",
    "\n",
    "class KNeighborsClassifier(KNeighborsMixin, ClassifierMixin, NeighborsBase):\n",
    "    \"\"\"Classifier implementing the k-nearest neighbors vote.\n",
    "\n",
    "    Read more in the :ref:`User Guide <classification>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_neighbors : int, default=5\n",
    "        Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
    "\n",
    "    weights : {'uniform', 'distance'}, callable or None, default='uniform'\n",
    "        Weight function used in prediction.  Possible values:\n",
    "\n",
    "        - 'uniform' : uniform weights.  All points in each neighborhood\n",
    "          are weighted equally.\n",
    "        - 'distance' : weight points by the inverse of their distance.\n",
    "          in this case, closer neighbors of a query point will have a\n",
    "          greater influence than neighbors which are further away.\n",
    "        - [callable] : a user-defined function which accepts an\n",
    "          array of distances, and returns an array of the same shape\n",
    "          containing the weights.\n",
    "\n",
    "        Refer to the example entitled\n",
    "        :ref:`sphx_glr_auto_examples_neighbors_plot_classification.py`\n",
    "        showing the impact of the `weights` parameter on the decision\n",
    "        boundary.\n",
    "\n",
    "    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
    "        Algorithm used to compute the nearest neighbors:\n",
    "\n",
    "        - 'ball_tree' will use :class:`BallTree`\n",
    "        - 'kd_tree' will use :class:`KDTree`\n",
    "        - 'brute' will use a brute-force search.\n",
    "        - 'auto' will attempt to decide the most appropriate algorithm\n",
    "          based on the values passed to :meth:`fit` method.\n",
    "\n",
    "        Note: fitting on sparse input will override the setting of\n",
    "        this parameter, using brute force.\n",
    "\n",
    "    leaf_size : int, default=30\n",
    "        Leaf size passed to BallTree or KDTree.  This can affect the\n",
    "        speed of the construction and query, as well as the memory\n",
    "        required to store the tree.  The optimal value depends on the\n",
    "        nature of the problem.\n",
    "\n",
    "    p : float, default=2\n",
    "        Power parameter for the Minkowski metric. When p = 1, this is equivalent\n",
    "        to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n",
    "        For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\n",
    "        to be positive.\n",
    "\n",
    "    metric : str or callable, default='minkowski'\n",
    "        Metric to use for distance computation. Default is \"minkowski\", which\n",
    "        results in the standard Euclidean distance when p = 2. See the\n",
    "        documentation of `scipy.spatial.distance\n",
    "        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n",
    "        the metrics listed in\n",
    "        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n",
    "        values.\n",
    "\n",
    "        If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
    "        must be square during fit. X may be a :term:`sparse graph`, in which\n",
    "        case only \"nonzero\" elements may be considered neighbors.\n",
    "\n",
    "        If metric is a callable function, it takes two arrays representing 1D\n",
    "        vectors as inputs and must return one value indicating the distance\n",
    "        between those vectors. This works for Scipy's metrics, but is less\n",
    "        efficient than passing the metric name as a string.\n",
    "\n",
    "    metric_params : dict, default=None\n",
    "        Additional keyword arguments for the metric function.\n",
    "\n",
    "    n_jobs : int, default=None\n",
    "        The number of parallel jobs to run for neighbors search.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "        Doesn't affect :meth:`fit` method.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape (n_classes,)\n",
    "        Class labels known to the classifier\n",
    "\n",
    "    effective_metric_ : str or callble\n",
    "        The distance metric used. It will be same as the `metric` parameter\n",
    "        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
    "        'minkowski' and `p` parameter set to 2.\n",
    "\n",
    "    effective_metric_params_ : dict\n",
    "        Additional keyword arguments for the metric function. For most metrics\n",
    "        will be same with `metric_params` parameter, but may also contain the\n",
    "        `p` parameter value if the `effective_metric_` attribute is set to\n",
    "        'minkowski'.\n",
    "\n",
    "    n_features_in_ : int\n",
    "        Number of features seen during :term:`fit`.\n",
    "\n",
    "        .. versionadded:: 0.24\n",
    "\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during :term:`fit`. Defined only when `X`\n",
    "        has feature names that are all strings.\n",
    "\n",
    "        .. versionadded:: 1.0\n",
    "\n",
    "    n_samples_fit_ : int\n",
    "        Number of samples in the fitted data.\n",
    "\n",
    "    outputs_2d_ : bool\n",
    "        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
    "        otherwise True.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\n",
    "    KNeighborsRegressor: Regression based on k-nearest neighbors.\n",
    "    RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\n",
    "    NearestNeighbors: Unsupervised learner for implementing neighbor searches.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
    "    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
    "\n",
    "    .. warning::\n",
    "\n",
    "       Regarding the Nearest Neighbors algorithms, if it is found that two\n",
    "       neighbors, neighbor `k+1` and `k`, have identical distances\n",
    "       but different labels, the results will depend on the ordering of the\n",
    "       training data.\n",
    "\n",
    "    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> X = [[0], [1], [2], [3]]\n",
    "    >>> y = [0, 0, 1, 1]\n",
    "    >>> from sklearn.neighbors import KNeighborsClassifier\n",
    "    >>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "    >>> neigh.fit(X, y)\n",
    "    KNeighborsClassifier(...)\n",
    "    >>> print(neigh.predict([[1.1]]))\n",
    "    [0]\n",
    "    >>> print(neigh.predict_proba([[0.9]]))\n",
    "    [[0.666... 0.333...]]\n",
    "    \"\"\"\n",
    "\n",
    "    _parameter_constraints: dict = {**NeighborsBase._parameter_constraints}\n",
    "    _parameter_constraints.pop(\"radius\")\n",
    "    _parameter_constraints.update(\n",
    "        {\"weights\": [StrOptions({\"uniform\", \"distance\"}), callable, None]}\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_neighbors=5,\n",
    "        *,\n",
    "        weights=\"uniform\",\n",
    "        algorithm=\"auto\",\n",
    "        leaf_size=30,\n",
    "        p=2,\n",
    "        metric=\"minkowski\",\n",
    "        metric_params=None,\n",
    "        n_jobs=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_neighbors=n_neighbors,\n",
    "            algorithm=algorithm,\n",
    "            leaf_size=leaf_size,\n",
    "            metric=metric,\n",
    "            p=p,\n",
    "            metric_params=metric_params,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "        self.weights = weights\n",
    "\n",
    "    @_fit_context(\n",
    "        # KNeighborsClassifier.metric is not validated yet\n",
    "        prefer_skip_nested_validation=False\n",
    "    )\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the k-nearest neighbors classifier from the training dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n",
    "                (n_samples, n_samples) if metric='precomputed'\n",
    "            Training data.\n",
    "\n",
    "        y : {array-like, sparse matrix} of shape (n_samples,) or \\\n",
    "                (n_samples, n_outputs)\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : KNeighborsClassifier\n",
    "            The fitted k-nearest neighbors classifier.\n",
    "        \"\"\"\n",
    "        return self._fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the class labels for the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_queries, n_features), \\\n",
    "                or (n_queries, n_indexed) if metric == 'precomputed'\n",
    "            Test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n",
    "            Class labels for each data sample.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"_fit_method\")\n",
    "        if self.weights == \"uniform\":\n",
    "            if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
    "                X, self._fit_X, self.metric\n",
    "            ):\n",
    "                probabilities = self.predict_proba(X)\n",
    "                if self.outputs_2d_:\n",
    "                    return np.stack(\n",
    "                        [\n",
    "                            self.classes_[idx][np.argmax(probas, axis=1)]\n",
    "                            for idx, probas in enumerate(probabilities)\n",
    "                        ],\n",
    "                        axis=1,\n",
    "                    )\n",
    "                return self.classes_[np.argmax(probabilities, axis=1)]\n",
    "            # In that case, we do not need the distances to perform\n",
    "            # the weighting so we do not compute them.\n",
    "            neigh_ind = self.kneighbors(X, return_distance=False)\n",
    "            neigh_dist = None\n",
    "        else:\n",
    "            neigh_dist, neigh_ind = self.kneighbors(X)\n",
    "\n",
    "        classes_ = self.classes_\n",
    "        _y = self._y\n",
    "        if not self.outputs_2d_:\n",
    "            _y = self._y.reshape((-1, 1))\n",
    "            classes_ = [self.classes_]\n",
    "\n",
    "        n_outputs = len(classes_)\n",
    "        n_queries = _num_samples(X)\n",
    "        weights = _get_weights(neigh_dist, self.weights)\n",
    "        if weights is not None and _all_with_any_reduction_axis_1(weights, value=0):\n",
    "            raise ValueError(\n",
    "                \"All neighbors of some sample is getting zero weights. \"\n",
    "                \"Please modify 'weights' to avoid this case if you are \"\n",
    "                \"using a user-defined function.\"\n",
    "            )\n",
    "\n",
    "        y_pred = np.empty((n_queries, n_outputs), dtype=classes_[0].dtype)\n",
    "        for k, classes_k in enumerate(classes_):\n",
    "            if weights is None:\n",
    "                mode, _ = _mode(_y[neigh_ind, k], axis=1)\n",
    "            else:\n",
    "                mode, _ = weighted_mode(_y[neigh_ind, k], weights, axis=1)\n",
    "\n",
    "            mode = np.asarray(mode.ravel(), dtype=np.intp)\n",
    "            y_pred[:, k] = classes_k.take(mode)\n",
    "\n",
    "        if not self.outputs_2d_:\n",
    "            y_pred = y_pred.ravel()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probability estimates for the test data X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_queries, n_features), \\\n",
    "                or (n_queries, n_indexed) if metric == 'precomputed'\n",
    "            Test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : ndarray of shape (n_queries, n_classes), or a list of n_outputs \\\n",
    "                of such arrays if n_outputs > 1.\n",
    "            The class probabilities of the input samples. Classes are ordered\n",
    "            by lexicographic order.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"_fit_method\")\n",
    "        if self.weights == \"uniform\":\n",
    "            # TODO: systematize this mapping of metric for\n",
    "            # PairwiseDistancesReductions.\n",
    "            metric, metric_kwargs = _adjusted_metric(\n",
    "                metric=self.metric, metric_kwargs=self.metric_params, p=self.p\n",
    "            )\n",
    "            if (\n",
    "                self._fit_method == \"brute\"\n",
    "                and ArgKminClassMode.is_usable_for(X, self._fit_X, metric)\n",
    "                # TODO: Implement efficient multi-output solution\n",
    "                and not self.outputs_2d_\n",
    "            ):\n",
    "                if self.metric == \"precomputed\":\n",
    "                    X = _check_precomputed(X)\n",
    "                else:\n",
    "                    X = self._validate_data(\n",
    "                        X, accept_sparse=\"csr\", reset=False, order=\"C\"\n",
    "                    )\n",
    "\n",
    "                probabilities = ArgKminClassMode.compute(\n",
    "                    X,\n",
    "                    self._fit_X,\n",
    "                    k=self.n_neighbors,\n",
    "                    weights=self.weights,\n",
    "                    Y_labels=self._y,\n",
    "                    unique_Y_labels=self.classes_,\n",
    "                    metric=metric,\n",
    "                    metric_kwargs=metric_kwargs,\n",
    "                    # `strategy=\"parallel_on_X\"` has in practice be shown\n",
    "                    # to be more efficient than `strategy=\"parallel_on_Y``\n",
    "                    # on many combination of datasets.\n",
    "                    # Hence, we choose to enforce it here.\n",
    "                    # For more information, see:\n",
    "                    # https://github.com/scikit-learn/scikit-learn/pull/24076#issuecomment-1445258342  # noqa\n",
    "                    # TODO: adapt the heuristic for `strategy=\"auto\"` for\n",
    "                    # `ArgKminClassMode` and use `strategy=\"auto\"`.\n",
    "                    strategy=\"parallel_on_X\",\n",
    "                )\n",
    "                return probabilities\n",
    "\n",
    "            # In that case, we do not need the distances to perform\n",
    "            # the weighting so we do not compute them.\n",
    "            neigh_ind = self.kneighbors(X, return_distance=False)\n",
    "            neigh_dist = None\n",
    "        else:\n",
    "            neigh_dist, neigh_ind = self.kneighbors(X)\n",
    "\n",
    "        classes_ = self.classes_\n",
    "        _y = self._y\n",
    "        if not self.outputs_2d_:\n",
    "            _y = self._y.reshape((-1, 1))\n",
    "            classes_ = [self.classes_]\n",
    "\n",
    "        n_queries = _num_samples(X)\n",
    "\n",
    "        weights = _get_weights(neigh_dist, self.weights)\n",
    "        if weights is None:\n",
    "            weights = np.ones_like(neigh_ind)\n",
    "        elif _all_with_any_reduction_axis_1(weights, value=0):\n",
    "            raise ValueError(\n",
    "                \"All neighbors of some sample is getting zero weights. \"\n",
    "                \"Please modify 'weights' to avoid this case if you are \"\n",
    "                \"using a user-defined function.\"\n",
    "            )\n",
    "\n",
    "        all_rows = np.arange(n_queries)\n",
    "        probabilities = []\n",
    "        for k, classes_k in enumerate(classes_):\n",
    "            pred_labels = _y[:, k][neigh_ind]\n",
    "            proba_k = np.zeros((n_queries, classes_k.size))\n",
    "\n",
    "            # a simple ':' index doesn't work right\n",
    "            for i, idx in enumerate(pred_labels.T):  # loop is O(n_neighbors)\n",
    "                proba_k[all_rows, idx] += weights[:, i]\n",
    "\n",
    "            # normalize 'votes' into real [0,1] probabilities\n",
    "            normalizer = proba_k.sum(axis=1)[:, np.newaxis]\n",
    "            proba_k /= normalizer\n",
    "\n",
    "            probabilities.append(proba_k)\n",
    "\n",
    "        if not self.outputs_2d_:\n",
    "            probabilities = probabilities[0]\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"multilabel\": True}\n",
    "\n",
    "\n",
    "class RadiusNeighborsClassifier(RadiusNeighborsMixin, ClassifierMixin, NeighborsBase):\n",
    "    \"\"\"Classifier implementing a vote among neighbors within a given radius.\n",
    "\n",
    "    Read more in the :ref:`User Guide <classification>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    radius : float, default=1.0\n",
    "        Range of parameter space to use by default for :meth:`radius_neighbors`\n",
    "        queries.\n",
    "\n",
    "    weights : {'uniform', 'distance'}, callable or None, default='uniform'\n",
    "        Weight function used in prediction.  Possible values:\n",
    "\n",
    "        - 'uniform' : uniform weights.  All points in each neighborhood\n",
    "          are weighted equally.\n",
    "        - 'distance' : weight points by the inverse of their distance.\n",
    "          in this case, closer neighbors of a query point will have a\n",
    "          greater influence than neighbors which are further away.\n",
    "        - [callable] : a user-defined function which accepts an\n",
    "          array of distances, and returns an array of the same shape\n",
    "          containing the weights.\n",
    "\n",
    "        Uniform weights are used by default.\n",
    "\n",
    "    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
    "        Algorithm used to compute the nearest neighbors:\n",
    "\n",
    "        - 'ball_tree' will use :class:`BallTree`\n",
    "        - 'kd_tree' will use :class:`KDTree`\n",
    "        - 'brute' will use a brute-force search.\n",
    "        - 'auto' will attempt to decide the most appropriate algorithm\n",
    "          based on the values passed to :meth:`fit` method.\n",
    "\n",
    "        Note: fitting on sparse input will override the setting of\n",
    "        this parameter, using brute force.\n",
    "\n",
    "    leaf_size : int, default=30\n",
    "        Leaf size passed to BallTree or KDTree.  This can affect the\n",
    "        speed of the construction and query, as well as the memory\n",
    "        required to store the tree.  The optimal value depends on the\n",
    "        nature of the problem.\n",
    "\n",
    "    p : float, default=2\n",
    "        Power parameter for the Minkowski metric. When p = 1, this is\n",
    "        equivalent to using manhattan_distance (l1), and euclidean_distance\n",
    "        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "        This parameter is expected to be positive.\n",
    "\n",
    "    metric : str or callable, default='minkowski'\n",
    "        Metric to use for distance computation. Default is \"minkowski\", which\n",
    "        results in the standard Euclidean distance when p = 2. See the\n",
    "        documentation of `scipy.spatial.distance\n",
    "        <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n",
    "        the metrics listed in\n",
    "        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n",
    "        values.\n",
    "\n",
    "        If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
    "        must be square during fit. X may be a :term:`sparse graph`, in which\n",
    "        case only \"nonzero\" elements may be considered neighbors.\n",
    "\n",
    "        If metric is a callable function, it takes two arrays representing 1D\n",
    "        vectors as inputs and must return one value indicating the distance\n",
    "        between those vectors. This works for Scipy's metrics, but is less\n",
    "        efficient than passing the metric name as a string.\n",
    "\n",
    "    outlier_label : {manual label, 'most_frequent'}, default=None\n",
    "        Label for outlier samples (samples with no neighbors in given radius).\n",
    "\n",
    "        - manual label: str or int label (should be the same type as y)\n",
    "          or list of manual labels if multi-output is used.\n",
    "        - 'most_frequent' : assign the most frequent label of y to outliers.\n",
    "        - None : when any outlier is detected, ValueError will be raised.\n",
    "\n",
    "        The outlier label should be selected from among the unique 'Y' labels.\n",
    "        If it is specified with a different value a warning will be raised and\n",
    "        all class probabilities of outliers will be assigned to be 0.\n",
    "\n",
    "    metric_params : dict, default=None\n",
    "        Additional keyword arguments for the metric function.\n",
    "\n",
    "    n_jobs : int, default=None\n",
    "        The number of parallel jobs to run for neighbors search.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : ndarray of shape (n_classes,)\n",
    "        Class labels known to the classifier.\n",
    "\n",
    "    effective_metric_ : str or callable\n",
    "        The distance metric used. It will be same as the `metric` parameter\n",
    "        or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
    "        'minkowski' and `p` parameter set to 2.\n",
    "\n",
    "    effective_metric_params_ : dict\n",
    "        Additional keyword arguments for the metric function. For most metrics\n",
    "        will be same with `metric_params` parameter, but may also contain the\n",
    "        `p` parameter value if the `effective_metric_` attribute is set to\n",
    "        'minkowski'.\n",
    "\n",
    "    n_features_in_ : int\n",
    "        Number of features seen during :term:`fit`.\n",
    "\n",
    "        .. versionadded:: 0.24\n",
    "\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during :term:`fit`. Defined only when `X`\n",
    "        has feature names that are all strings.\n",
    "\n",
    "        .. versionadded:: 1.0\n",
    "\n",
    "    n_samples_fit_ : int\n",
    "        Number of samples in the fitted data.\n",
    "\n",
    "    outlier_label_ : int or array-like of shape (n_class,)\n",
    "        Label which is given for outlier samples (samples with no neighbors\n",
    "        on given radius).\n",
    "\n",
    "    outputs_2d_ : bool\n",
    "        False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
    "        otherwise True.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    KNeighborsClassifier : Classifier implementing the k-nearest neighbors\n",
    "        vote.\n",
    "    RadiusNeighborsRegressor : Regression based on neighbors within a\n",
    "        fixed radius.\n",
    "    KNeighborsRegressor : Regression based on k-nearest neighbors.\n",
    "    NearestNeighbors : Unsupervised learner for implementing neighbor\n",
    "        searches.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
    "    for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
    "\n",
    "    https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> X = [[0], [1], [2], [3]]\n",
    "    >>> y = [0, 0, 1, 1]\n",
    "    >>> from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "    >>> neigh = RadiusNeighborsClassifier(radius=1.0)\n",
    "    >>> neigh.fit(X, y)\n",
    "    RadiusNeighborsClassifier(...)\n",
    "    >>> print(neigh.predict([[1.5]]))\n",
    "    [0]\n",
    "    >>> print(neigh.predict_proba([[1.0]]))\n",
    "    [[0.66666667 0.33333333]]\n",
    "    \"\"\"\n",
    "\n",
    "    _parameter_constraints: dict = {\n",
    "        **NeighborsBase._parameter_constraints,\n",
    "        \"weights\": [StrOptions({\"uniform\", \"distance\"}), callable, None],\n",
    "        \"outlier_label\": [Integral, str, \"array-like\", None],\n",
    "    }\n",
    "    _parameter_constraints.pop(\"n_neighbors\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        radius=1.0,\n",
    "        *,\n",
    "        weights=\"uniform\",\n",
    "        algorithm=\"auto\",\n",
    "        leaf_size=30,\n",
    "        p=2,\n",
    "        metric=\"minkowski\",\n",
    "        outlier_label=None,\n",
    "        metric_params=None,\n",
    "        n_jobs=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            radius=radius,\n",
    "            algorithm=algorithm,\n",
    "            leaf_size=leaf_size,\n",
    "            metric=metric,\n",
    "            p=p,\n",
    "            metric_params=metric_params,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "        self.weights = weights\n",
    "        self.outlier_label = outlier_label\n",
    "\n",
    "    @_fit_context(\n",
    "        # RadiusNeighborsClassifier.metric is not validated yet\n",
    "        prefer_skip_nested_validation=False\n",
    "    )\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the radius neighbors classifier from the training dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n",
    "                (n_samples, n_samples) if metric='precomputed'\n",
    "            Training data.\n",
    "\n",
    "        y : {array-like, sparse matrix} of shape (n_samples,) or \\\n",
    "                (n_samples, n_outputs)\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : RadiusNeighborsClassifier\n",
    "            The fitted radius neighbors classifier.\n",
    "        \"\"\"\n",
    "        self._fit(X, y)\n",
    "\n",
    "        classes_ = self.classes_\n",
    "        _y = self._y\n",
    "        if not self.outputs_2d_:\n",
    "            _y = self._y.reshape((-1, 1))\n",
    "            classes_ = [self.classes_]\n",
    "\n",
    "        if self.outlier_label is None:\n",
    "            outlier_label_ = None\n",
    "\n",
    "        elif self.outlier_label == \"most_frequent\":\n",
    "            outlier_label_ = []\n",
    "            # iterate over multi-output, get the most frequent label for each\n",
    "            # output.\n",
    "            for k, classes_k in enumerate(classes_):\n",
    "                label_count = np.bincount(_y[:, k])\n",
    "                outlier_label_.append(classes_k[label_count.argmax()])\n",
    "\n",
    "        else:\n",
    "            if _is_arraylike(self.outlier_label) and not isinstance(\n",
    "                self.outlier_label, str\n",
    "            ):\n",
    "                if len(self.outlier_label) != len(classes_):\n",
    "                    raise ValueError(\n",
    "                        \"The length of outlier_label: {} is \"\n",
    "                        \"inconsistent with the output \"\n",
    "                        \"length: {}\".format(self.outlier_label, len(classes_))\n",
    "                    )\n",
    "                outlier_label_ = self.outlier_label\n",
    "            else:\n",
    "                outlier_label_ = [self.outlier_label] * len(classes_)\n",
    "\n",
    "            for classes, label in zip(classes_, outlier_label_):\n",
    "                if _is_arraylike(label) and not isinstance(label, str):\n",
    "                    # ensure the outlier label for each output is a scalar.\n",
    "                    raise TypeError(\n",
    "                        \"The outlier_label of classes {} is \"\n",
    "                        \"supposed to be a scalar, got \"\n",
    "                        \"{}.\".format(classes, label)\n",
    "                    )\n",
    "                if np.append(classes, label).dtype != classes.dtype:\n",
    "                    # ensure the dtype of outlier label is consistent with y.\n",
    "                    raise TypeError(\n",
    "                        \"The dtype of outlier_label {} is \"\n",
    "                        \"inconsistent with classes {} in \"\n",
    "                        \"y.\".format(label, classes)\n",
    "                    )\n",
    "\n",
    "        self.outlier_label_ = outlier_label_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the class labels for the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_queries, n_features), \\\n",
    "                or (n_queries, n_indexed) if metric == 'precomputed'\n",
    "            Test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n",
    "            Class labels for each data sample.\n",
    "        \"\"\"\n",
    "\n",
    "        probs = self.predict_proba(X)\n",
    "        classes_ = self.classes_\n",
    "\n",
    "        if not self.outputs_2d_:\n",
    "            probs = [probs]\n",
    "            classes_ = [self.classes_]\n",
    "\n",
    "        n_outputs = len(classes_)\n",
    "        n_queries = probs[0].shape[0]\n",
    "        y_pred = np.empty((n_queries, n_outputs), dtype=classes_[0].dtype)\n",
    "\n",
    "        for k, prob in enumerate(probs):\n",
    "            # iterate over multi-output, assign labels based on probabilities\n",
    "            # of each output.\n",
    "            max_prob_index = prob.argmax(axis=1)\n",
    "            y_pred[:, k] = classes_[k].take(max_prob_index)\n",
    "\n",
    "            outlier_zero_probs = (prob == 0).all(axis=1)\n",
    "            if outlier_zero_probs.any():\n",
    "                zero_prob_index = np.flatnonzero(outlier_zero_probs)\n",
    "                y_pred[zero_prob_index, k] = self.outlier_label_[k]\n",
    "\n",
    "        if not self.outputs_2d_:\n",
    "            y_pred = y_pred.ravel()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probability estimates for the test data X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_queries, n_features), \\\n",
    "                or (n_queries, n_indexed) if metric == 'precomputed'\n",
    "            Test samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : ndarray of shape (n_queries, n_classes), or a list of \\\n",
    "                n_outputs of such arrays if n_outputs > 1.\n",
    "            The class probabilities of the input samples. Classes are ordered\n",
    "            by lexicographic order.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"_fit_method\")\n",
    "        n_queries = _num_samples(X)\n",
    "\n",
    "        metric, metric_kwargs = _adjusted_metric(\n",
    "            metric=self.metric, metric_kwargs=self.metric_params, p=self.p\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            self.weights == \"uniform\"\n",
    "            and self._fit_method == \"brute\"\n",
    "            and not self.outputs_2d_\n",
    "            and RadiusNeighborsClassMode.is_usable_for(X, self._fit_X, metric)\n",
    "        ):\n",
    "            probabilities = RadiusNeighborsClassMode.compute(\n",
    "                X=X,\n",
    "                Y=self._fit_X,\n",
    "                radius=self.radius,\n",
    "                weights=self.weights,\n",
    "                Y_labels=self._y,\n",
    "                unique_Y_labels=self.classes_,\n",
    "                outlier_label=self.outlier_label,\n",
    "                metric=metric,\n",
    "                metric_kwargs=metric_kwargs,\n",
    "                strategy=\"parallel_on_X\",\n",
    "                # `strategy=\"parallel_on_X\"` has in practice be shown\n",
    "                # to be more efficient than `strategy=\"parallel_on_Y``\n",
    "                # on many combination of datasets.\n",
    "                # Hence, we choose to enforce it here.\n",
    "                # For more information, see:\n",
    "                # https://github.com/scikit-learn/scikit-learn/pull/26828/files#r1282398471  # noqa\n",
    "            )\n",
    "            return probabilities\n",
    "\n",
    "        neigh_dist, neigh_ind = self.radius_neighbors(X)\n",
    "        outlier_mask = np.zeros(n_queries, dtype=bool)\n",
    "        outlier_mask[:] = [len(nind) == 0 for nind in neigh_ind]\n",
    "        outliers = np.flatnonzero(outlier_mask)\n",
    "        inliers = np.flatnonzero(~outlier_mask)\n",
    "\n",
    "        classes_ = self.classes_\n",
    "        _y = self._y\n",
    "        if not self.outputs_2d_:\n",
    "            _y = self._y.reshape((-1, 1))\n",
    "            classes_ = [self.classes_]\n",
    "\n",
    "        if self.outlier_label_ is None and outliers.size > 0:\n",
    "            raise ValueError(\n",
    "                \"No neighbors found for test samples %r, \"\n",
    "                \"you can try using larger radius, \"\n",
    "                \"giving a label for outliers, \"\n",
    "                \"or considering removing them from your dataset.\" % outliers\n",
    "            )\n",
    "\n",
    "        weights = _get_weights(neigh_dist, self.weights)\n",
    "        if weights is not None:\n",
    "            weights = weights[inliers]\n",
    "\n",
    "        probabilities = []\n",
    "        # iterate over multi-output, measure probabilities of the k-th output.\n",
    "        for k, classes_k in enumerate(classes_):\n",
    "            pred_labels = np.zeros(len(neigh_ind), dtype=object)\n",
    "            pred_labels[:] = [_y[ind, k] for ind in neigh_ind]\n",
    "\n",
    "            proba_k = np.zeros((n_queries, classes_k.size))\n",
    "            proba_inl = np.zeros((len(inliers), classes_k.size))\n",
    "\n",
    "            # samples have different size of neighbors within the same radius\n",
    "            if weights is None:\n",
    "                for i, idx in enumerate(pred_labels[inliers]):\n",
    "                    proba_inl[i, :] = np.bincount(idx, minlength=classes_k.size)\n",
    "            else:\n",
    "                for i, idx in enumerate(pred_labels[inliers]):\n",
    "                    proba_inl[i, :] = np.bincount(\n",
    "                        idx, weights[i], minlength=classes_k.size\n",
    "                    )\n",
    "            proba_k[inliers, :] = proba_inl\n",
    "\n",
    "            if outliers.size > 0:\n",
    "                _outlier_label = self.outlier_label_[k]\n",
    "                label_index = np.flatnonzero(classes_k == _outlier_label)\n",
    "                if label_index.size == 1:\n",
    "                    proba_k[outliers, label_index[0]] = 1.0\n",
    "                else:\n",
    "                    warnings.warn(\n",
    "                        \"Outlier label {} is not in training \"\n",
    "                        \"classes. All class probabilities of \"\n",
    "                        \"outliers will be assigned with 0.\"\n",
    "                        \"\".format(self.outlier_label_[k])\n",
    "                    )\n",
    "\n",
    "            # normalize 'votes' into real [0,1] probabilities\n",
    "            normalizer = proba_k.sum(axis=1)[:, np.newaxis]\n",
    "            normalizer[normalizer == 0.0] = 1.0\n",
    "            proba_k /= normalizer\n",
    "\n",
    "            probabilities.append(proba_k)\n",
    "\n",
    "        if not self.outputs_2d_:\n",
    "            probabilities = probabilities[0]\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"multilabel\": True}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
