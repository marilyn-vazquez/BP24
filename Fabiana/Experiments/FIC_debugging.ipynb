{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0d46db-653f-4d57-8c27-03c90b6d22cb",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f53352d8-22d2-4075-b31f-d0ef01c764f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e2c78482-a1c3-4fc3-9a32-422305278d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/fabianafazio/Documents/GitHub/BP24/Fabiana/Data/D3Softball.xlsx', index_col=[0])\n",
    "df.replace('---', np.nan, inplace=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924e959-5c63-41bc-ac61-bf285cc1e4b9",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4a914886-1f4b-4340-84c7-3a8e6f506d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing data\n",
    "# test_size: this is the percentage of data used for testing (20% in this case), so the rest is used for training data (80% in this case)\n",
    "# random_state: this is a random number chosen that should be used each time to ensure we get the same data split each time\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size = 0.4, random_state = 52)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d63487-3dd7-49ed-bdd2-3b4432a519ef",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "868df05f-7458-4286-bef6-62df87febe7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_convert = [\n",
    "    'G', 'AB', 'H (Offensive)', 'H/G', 'BA', '2B (Offensive)', '2B/G',\n",
    "    '3B (Offensive)', '3B/G', 'Innings Pitched', 'K (Def)', 'K/G (Def)',\n",
    "    'BB Allowed', 'BB/G (Def)', 'K/BB', 'HA', 'HA/G', 'Runs Allowed',\n",
    "    'Runs Allowed/G', 'ER Allowed', 'ERA', 'WHIP', 'PO', 'A', 'E', 'E/G',\n",
    "    'FPCT', 'HBP (Offensive)', 'HBP/G', 'BB (Offensive)', 'BB/G (Off)',\n",
    "    'SF (Offensive)', 'SH (Offensive)', 'OBP', 'SHO', 'SHO %', 'SB',\n",
    "    'SB/G', 'TB', 'TB/G', 'SLG PCT', 'R', 'R/G', 'DP', 'DP/G',\n",
    "    'K (Off)', 'K/G (Off)'\n",
    "]\n",
    "\n",
    "# Convert the columns to float\n",
    "df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d3bfc-09bc-416e-b7c7-1f8c8370f16a",
   "metadata": {},
   "source": [
    "### FIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "16d03cd9-1b77-4591-adc5-a5ac9e6fa031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureClassifier:\n",
    "  def __init__(self,reqAcc = 0.01, classifier = 'DecisionTree', bias = [], control = None, n_jobs = None, random_state = None):\n",
    "    self.featureClassifiers=[] #list of all the classifiers of all the selected features\n",
    "    self.reqAcc=reqAcc #user specified cutoff value\n",
    "    self.indexLs=[] # list of mapped index values to featureClassifiers\n",
    "    self.flag=0\n",
    "    self.bias=bias # list of biases for each and every label\n",
    "    self.control=control #overfitting control for decision trees\n",
    "    self.classifier=classifier #the classifier which is preferred by the user\n",
    "    self.dic={'DecisionTree':0,'LinearRegression':1,'SVM':2,'LogisticRegression':3} #a dictionary which maps the classifier to its index\n",
    "    self.n_jobs=n_jobs\n",
    "    self.random_state=random_state\n",
    "    self.num_lables = None\n",
    "\n",
    "  def finIndex(self):\n",
    "    #finds the index where the reqAcc condition fails and also created the indexLs[] for mapping\n",
    "    for i in range(len(self.featureClassifiers)):\n",
    "      if self.featureClassifiers[i][1] < self.reqAcc:\n",
    "        return i\n",
    "      self.indexLs.append(self.featureClassifiers[i][2])\n",
    "    self.flag=1\n",
    "    return i\n",
    "\n",
    "  def fit(self,x,y):\n",
    "    #applied the model to the dataset. The model is trained and saved for further prediction\n",
    "    self.num_lables=len(set(y.flatten()))\n",
    "    bestfeatures = SelectKBest(score_func=chi2,k=1)\n",
    "    fit = bestfeatures.fit(x,y)\n",
    "\n",
    "    for i in range(len(x[0])):\n",
    "      clf=[DecisionTreeClassifier(max_depth=self.control,random_state=self.random_state),LinearRegression(n_jobs=self.n_jobs),SVC(gamma=self.control,random_state=self.random_state), LogisticRegression(penalty=self.control,random_state=self.random_state)][self.dic[self.classifier]]\n",
    "      X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33,random_state=self.random_state)\n",
    "      clf.fit(X_train[:,i:i+1],y_train)\n",
    "      self.featureClassifiers.append((clf,fit.scores_[i],i))\n",
    "    self.featureClassifiers.sort(key=lambda x:x[1],reverse=True)\n",
    "    index=self.finIndex()\n",
    "    if self.flag==0:\n",
    "      self.featureClassifiers=self.featureClassifiers[:index]\n",
    "    return\n",
    "\n",
    "  def predict(self,x):\n",
    "    #given a list of inputs, predicts the possible outputs\n",
    "    if not self.bias:\n",
    "      self.bias=np.zeros(self.num_lables)\n",
    "    if len(self.bias)<self.num_lables:\n",
    "      raise AttributeError('Please check the lenth of bias list')\n",
    "    yPred=[]\n",
    "    for i in range(len(x)):\n",
    "      pred_arr=np.zeros(self.num_lables)\n",
    "      for j in range(len(self.indexLs)):\n",
    "        pred=np.round(self.featureClassifiers[j][0].predict([[x[i][self.indexLs[j]]]]))\n",
    "        pred_arr[pred]+=self.featureClassifiers[j][1]+self.bias[pred[0]]\n",
    "      yPred.append(np.argmax(pred_arr))\n",
    "    return yPred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1bb7d-aa12-4ffc-aca3-573a9ee87008",
   "metadata": {},
   "source": [
    "# ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0009d4fe-6bf7-47fd-bd66-f67858d1504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9617834394904459\n",
      "[38, 41, 1, 2, 22, 36, 10, 29, 17, 34, 27, 5, 19, 9, 23, 24, 31, 15, 0, 7, 43, 18, 45, 42, 20, 16, 39, 14, 37, 46, 25, 35, 3, 28, 30, 11, 13, 12, 21, 6, 32, 40, 8, 33, 4, 44, 26]\n",
      "[38, 41, 1, 2, 22, 36, 10, 29, 17, 34, 27, 5, 19, 9, 23, 24, 31, 15, 0, 7, 43, 18, 45, 42, 20, 16, 39, 14, 37, 46, 25, 35, 3, 28, 30, 11, 13, 12, 21, 6, 32, 40, 8, 33, 4, 44, 26]\n"
     ]
    }
   ],
   "source": [
    "#train the model using the training sets\n",
    "clf1=FeatureClassifier(0,classifier='DecisionTree',control=3)\n",
    "# clf1.fit(X_train,y_train.reshape(-1,1))\n",
    "clf1.fit(np.array(X_train), np.array(y_train)[:,np.newaxis].astype(int))\n",
    "\n",
    "#predict the response for the test dataset\n",
    "#model accuracy (how often the classifier is correct)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(np.array(y_test).astype(int),clf1.predict(np.array(X_test))))\n",
    "\n",
    "print(clf1.indexLs)\n",
    "clf1.featureClassifiers\n",
    "print(clf1.indexLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e8ecc-77b1-4e10-9351-e8174e392c5b",
   "metadata": {},
   "source": [
    "# F1-SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cae97d84-13e6-4d02-971d-5e622aa4d574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.33333333333333337\n",
      "[38, 41, 1, 2, 22, 36, 10, 29, 17, 34, 27, 5, 19, 9, 23, 24, 31, 15, 0, 7, 43, 18, 45, 42, 20, 16, 39, 14, 37, 46, 25, 35, 3, 28, 30, 11, 13, 12, 21, 6, 32, 40, 8, 33, 4, 44, 26]\n"
     ]
    }
   ],
   "source": [
    "# Assuming FeatureClassifier is correctly implemented for DecisionTreeClassifier\n",
    "clf1 = FeatureClassifier(0, classifier='DecisionTree', control=3)\n",
    "\n",
    "# Fit model with the training data\n",
    "clf1.fit(np.array(X_train), np.array(y_train).astype(int))\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "y_pred = clf1.predict(np.array(X_test))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1 = metrics.f1_score(np.array(y_test).astype(int), y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Print other relevant outputs for debugging\n",
    "print(clf1.indexLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd7c9e-8e64-4868-b5dc-f6597062941e",
   "metadata": {},
   "source": [
    "# FABI EXAMPLE\n",
    "- We must only keep 1 splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3572ebe3-bbe7-4efe-a915-3c1274395250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureClassifier:\n",
    "    def __init__(self, reqAcc=0.01, classifier='DecisionTree', bias=[], control=None, n_jobs=None, random_state=None):\n",
    "        self.featureClassifiers = []  # list of all the classifiers of all the selected features\n",
    "        self.reqAcc = reqAcc  # user-specified cutoff value\n",
    "        self.indexLs = []  # list of mapped index values to featureClassifiers\n",
    "        self.flag = 0\n",
    "        self.bias = bias  # list of biases for each and every label\n",
    "        self.control = control  # overfitting control for decision trees\n",
    "        self.classifier = classifier  # the classifier which is preferred by the user\n",
    "        self.dic = {'DecisionTree': 0, 'LinearRegression': 1, 'SVM': 2, 'LogisticRegression': 3}  # a dictionary which maps the classifier to its index\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.num_labels = None\n",
    "\n",
    "    def finIndex(self):\n",
    "        # finds the index where the reqAcc condition fails and also creates the indexLs[] for mapping\n",
    "        for i in range(len(self.featureClassifiers)):\n",
    "            if self.featureClassifiers[i][1] < self.reqAcc:\n",
    "                return i\n",
    "            self.indexLs.append(self.featureClassifiers[i][2])\n",
    "        self.flag = 1\n",
    "        return i\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # applies the model to the dataset. The model is trained and saved for further prediction\n",
    "        self.num_labels = len(set(y.flatten()))\n",
    "        bestfeatures = SelectKBest(score_func=chi2, k=1)\n",
    "        fit = bestfeatures.fit(x, y)\n",
    "\n",
    "        for i in range(len(x[0])):\n",
    "            clf = [\n",
    "                DecisionTreeClassifier(max_depth=self.control, random_state=self.random_state),\n",
    "                LinearRegression(n_jobs=self.n_jobs),\n",
    "                SVC(gamma=self.control, random_state=self.random_state),\n",
    "                LogisticRegression(penalty=self.control, random_state=self.random_state)\n",
    "            ][self.dic[self.classifier]]\n",
    "            # Assuming x and y are already split before passing to this method\n",
    "            #clf.fit(x[:, i:i+1], y)\n",
    "            clf.fit(X_train[:,i:i+1],y_train)\n",
    "            self.featureClassifiers.append((clf, fit.scores_[i], i))\n",
    "        self.featureClassifiers.sort(key=lambda x: x[1], reverse=True)\n",
    "        index = self.finIndex()\n",
    "        if self.flag == 0:\n",
    "            self.featureClassifiers = self.featureClassifiers[:index]\n",
    "        return\n",
    "\n",
    "    def predict(self, x):\n",
    "        # given a list of inputs, predicts the possible outputs\n",
    "        if not self.bias:\n",
    "            self.bias = np.zeros(self.num_labels)\n",
    "        if len(self.bias) < self.num_labels:\n",
    "            raise AttributeError('Please check the length of bias list')\n",
    "        yPred = []\n",
    "        for i in range(len(x)):\n",
    "            pred_arr = np.zeros(self.num_labels)\n",
    "            for j in range(len(self.indexLs)):\n",
    "                pred = np.round(self.featureClassifiers[j][0].predict([[x[i][self.indexLs[j]]]]))\n",
    "                pred_arr[pred] += self.featureClassifiers[j][1] + self.bias[pred[0]]\n",
    "            yPred.append(np.argmax(pred_arr))\n",
    "        return yPred\n",
    "\n",
    "# Split the dataset into training and testing data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=0.4, random_state=52)\n",
    "\n",
    "# Assuming FeatureClassifier is correctly implemented for DecisionTreeClassifier\n",
    "#clf1 = FeatureClassifier(0, classifier='DecisionTree', control=3, random_state=52)\n",
    "\n",
    "# Fit model with the training data\n",
    "#clf1.fit(np.array(X_train), np.array(y_train).astype(int))\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "#y_pred = clf1.predict(np.array(X_test))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "#f1 = metrics.f1_score(np.array(y_test).astype(int), y_pred, average='weighted')\n",
    "#print(\"F1 Score:\", f1)\n",
    "\n",
    "# Print other relevant outputs for debugging\n",
    "#print(clf1.indexLs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9c39710c-5179-498a-b540-5da8b1d3e77f",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), slice(0, 1, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:158\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), slice(0, 1, None))' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[201], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m clf1 \u001b[38;5;241m=\u001b[39m FeatureClassifier(\u001b[38;5;241m0\u001b[39m, classifier\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecisionTree\u001b[39m\u001b[38;5;124m'\u001b[39m, control\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fit model with the training data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m clf1\u001b[38;5;241m.\u001b[39mfit(np\u001b[38;5;241m.\u001b[39marray(X_train), np\u001b[38;5;241m.\u001b[39marray(y_train)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Predict the response for the test dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf1\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray(X_test))\n",
      "Cell \u001b[0;32mIn[199], line 39\u001b[0m, in \u001b[0;36mFeatureClassifier.fit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m     clf \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m         DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state),\n\u001b[1;32m     33\u001b[0m         LinearRegression(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs),\n\u001b[1;32m     34\u001b[0m         SVC(gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state),\n\u001b[1;32m     35\u001b[0m         LogisticRegression(penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[1;32m     36\u001b[0m     ][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdic[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier]]\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Assuming x and y are already split before passing to this method\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#clf.fit(x[:, i:i+1], y)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(X_train[:,i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m],y_train)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatureClassifiers\u001b[38;5;241m.\u001b[39mappend((clf, fit\u001b[38;5;241m.\u001b[39mscores_[i], i))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatureClassifiers\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:5975\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5971\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5972\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5973\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5974\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5975\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), slice(0, 1, None))"
     ]
    }
   ],
   "source": [
    "# Assuming FeatureClassifier is correctly implemented for DecisionTreeClassifier\n",
    "clf1 = FeatureClassifier(0, classifier='DecisionTree', control=3)\n",
    "\n",
    "# Fit model with the training data\n",
    "clf1.fit(np.array(X_train), np.array(y_train).astype(int))\n",
    "\n",
    "# Predict the response for the test dataset\n",
    "y_pred = clf1.predict(np.array(X_test))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1 = metrics.f1_score(np.array(y_test).astype(int), y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Print other relevant outputs for debugging\n",
    "print(clf1.indexLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fe9d0-1df5-4a6f-ac82-7b7f675de8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
