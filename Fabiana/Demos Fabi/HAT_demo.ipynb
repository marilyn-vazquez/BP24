{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da77f20e-a3f5-48da-8aec-4960d9f07f79",
   "metadata": {},
   "source": [
    "# HAT\n",
    "- Histogram Augmentation Technique (HAT) is used widely to augment and classify any tabular data\n",
    "- HAT is designed such that the generated data retains the distribution of the original tabular data histogram\n",
    "- HAT analyses the data distribution of a particular feature and based on the feature type (i.e. continuous or discrete) it generates new samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a737b0-03df-46c9-84fa-51d0da483b24",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd4bc04-cb71-4cce-8d99-e5c6e58713ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT important libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d89b50-13e2-4a37-b0b5-cd58dec13f7e",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586ef7a-7cea-43f8-8e68-c4c0f4ff926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "#data = np.loadtxt(\"uniform_small_d_1.tex\")\n",
    "data = np.loadtxt(\"uniform_large_d_1.tex\")\n",
    "#data = np.loadtxt(\"gaussian_small_d_1.tex\")\n",
    "#data = np.loadtxt(\"gaussian_large_d_1.tex\")\n",
    "\n",
    "# Creating NumPy array\n",
    "array = np.array(data)\n",
    "\n",
    "# Converting to Pandas DataFrame\n",
    "df_table = pd.DataFrame(array)\n",
    "\n",
    "# Displaying the table\n",
    "df_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83e3b7-2f0e-49ca-b640-27873003618f",
   "metadata": {},
   "source": [
    "# Convert dataset to 'categorical' and 'numerical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc4b25-57a1-4bbb-9ba7-aec54f3097ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the dataset, change 25 columns to 'categorical'\n",
    "#Loop, converts floats to ints and then those ints to category\n",
    "for i in range(25):\n",
    "    df_table.iloc[:,i] = df_table.iloc[:,i].round()\n",
    "    df_table.iloc[:,i] = df_table.iloc[:,i].astype(int)\n",
    "    df_table.iloc[:,i] = df_table.iloc[:,i].astype(\"category\")\n",
    "\n",
    "df_table.iloc[:, 150] = df_table.iloc[:, 150].astype(\"category\")\n",
    "\n",
    "df_table.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0427d1-b916-4bdb-84eb-beff45436ab0",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b602743-0097-4505-ad9a-1f2e78e8bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into training set and test set\n",
    "#test_size: in this case it is 70% training and 30% testing\n",
    "#random_state: sets a seed for a random number generator that splits the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_table.iloc[:,0:150], df_table.iloc[:,-1], test_size=0.2, random_state=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7b34f-519c-4e49-a2ec-6322174c0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save training data (not necessary)\n",
    "#X_test.to_csv('X_test_XGB.csv', index=False)\n",
    "#y_test.to_csv('y_test_XGB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254a0b4-3f95-4116-ad30-b9fce5455cfa",
   "metadata": {},
   "source": [
    "# HAT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e6e5f-494b-4a96-9910-98a9c27829f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define histogram function\n",
    "#data: This is the original dataset that you want to use for generating new data\n",
    "#no_new_data: This parameter indicates how much new data you want to generate\n",
    "#data_feat: This parameter specifies the type of data feature. It can be either 'c' for continuous data or 'd' for discrete data.\n",
    "#preserve: This parameter determines whether to preserve the original dataset or not while generating new data. If set to True, the original dataset will be included in the generated data; otherwise, it won't be included.\n",
    "def histogram_sampler(data, no_new_data, data_feat, preserve):\n",
    "\n",
    "    if (data_feat == 'c'):\n",
    "        start_time = time.time()\n",
    "        print(\"Existing data:\", len(data))\n",
    "        print(\"New data to be produced:\", no_new_data)\n",
    "\n",
    "#Function parameters\n",
    "#X_new = New data for each iteration\n",
    "#len_X_new = length of the newly generated data\n",
    "#iter_count = number of iterations\n",
    "#data_gen = Augmented data (original data + newly generated data)\n",
    "        \n",
    "        X_new = []\n",
    "        len_X_new = len(X_new)\n",
    "        iter_count = 0\n",
    "        data_gen = data\n",
    "\n",
    "\n",
    "         # Histogram calculation and sampling logic goes here...\n",
    "        # Adjust the condition for the while loop\n",
    "        while (len_X_new < 0.7 * no_new_data):\n",
    "            iter_count += 1\n",
    "            print(\"Number of iterations ---> niter_count=\", iter_count)\n",
    "\n",
    "            \n",
    "#Histogram: Generating the histogram , choosing the mid-value of the bins, and normalizing frequency\n",
    "#fd- Freedmanâ€“Diaconis rule is employed to choose the bin size, as it depends on the spread of the data, without any presumption\n",
    "            if (iter_count == 1):\n",
    "                Y,X_interval=np.histogram(data_gen,bins='doane')\n",
    "                n_bins = len(Y)\n",
    "            else:\n",
    "                Y,X_interval=np.histogram(data_gen,bins=n_bins)\n",
    "\n",
    "            \n",
    "            X = ((X_interval[0:-1] + X_interval[1:])/2) \n",
    "            Y = Y/max(Y)\n",
    "\n",
    "            bin_val = list(np.round(X,8))\n",
    "            weight = list(Y)\n",
    "            hist = dict(zip(bin_val,weight))\n",
    "\n",
    "            for xi in bin_val[0:-1]:\n",
    "\n",
    "#Values: choosing the values for undergoing validity check\n",
    "\n",
    "                bin_width = ((max(bin_val) - min(bin_val)) / int(len(bin_val)-1))\n",
    "                xm = xi + (bin_width/2)\n",
    "                x1 = xi\n",
    "                y1 = hist[xi]\n",
    "\n",
    "                res = None\n",
    "                temp = iter(hist)\n",
    "                for key in temp:\n",
    "                    if(key == xi):\n",
    "                        res = next(temp,None)\n",
    "\n",
    "                y2 = hist[res]\n",
    "                ym = ((y1+y2)/2)\n",
    "#Validity check: checking if the specified value can be considered\n",
    "#if(no_new_data <= len(data)):\n",
    "#ym = ym*(np.random.rand()<=ym)\n",
    "#y1 = y1*(np.random.rand()<=y1)\n",
    "    \n",
    "                #else:\n",
    "                ym = ym*(abs(np.random.normal(0,0.5))<=ym)\n",
    "                y1 = y1*(abs(np.random.normal(0,0.5))<=y1)\n",
    "\n",
    "#Appending: appending the valid values\n",
    "                \n",
    "\n",
    "                if (ym!=0):\n",
    "                    X_new.append(np.round(xm,8))\n",
    "                    #X_new.append(np.round(xm+0.1*xm,8))\n",
    "                    #X_new.append(np.round(xm-0.1*xm,8))\n",
    "                if (y1!=0):\n",
    "                    X_new.append(np.round(x1,8))\n",
    "                    #X_new.append(np.round(x1+0.1*xm,8))\n",
    "                    #X_new.append(np.round(x1-0.1*xm,8))\n",
    "#Stopping: bins * 2, length check\n",
    "\n",
    "            data_gen = data_gen + X_new\n",
    "            n_bins = n_bins*2     \n",
    "            len_X_new+= len(X_new)\n",
    "            print(len_X_new)\n",
    "            X_new = []\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "        print(len(data_gen)-len(data),no_new_data)\n",
    "\n",
    "        if(len(data_gen)-len(data) >= no_new_data):\n",
    "            data_gen = data_gen[:len(data)] + list(np.random.choice(data_gen[len(data):], no_new_data, replace = False))\n",
    "            print('\\nNew data generated:', len(data_gen[len(data):]), '\\nNew data:', len(data_gen), '\\n')\n",
    "            #sns.distplot(data_gen)\n",
    "            if(preserve == False):\n",
    "                data_gen = data_gen[len(data):]\n",
    "            return data_gen\n",
    "\n",
    "        else:\n",
    "            print('to discrete...', no_new_data - (len(data_gen)-len(data)))\n",
    "            samples = histogram_sampler(data_gen, no_new_data - (len(data_gen)-len(data)), 'd', preserve = True)\n",
    "            if(preserve == False):\n",
    "                samples = samples[len(data):]\n",
    "            return samples\n",
    "            \n",
    "    elif(data_feat == 'd'):\n",
    "        X_new=[]\n",
    "        data_gen=[]\n",
    "        disc_data= list(set(data))\n",
    "\n",
    "        for i in disc_data: \n",
    "            x=data.count(i)\n",
    "            X_new.append(round(x*(no_new_data) / len(data)))\n",
    "        #print(x_new,sum(x_new))\n",
    "\n",
    "        for j in range(0,len(X_new)):\n",
    "            for i in range(X_new[j]):\n",
    "                data_gen.append(disc_data[j])\n",
    "\n",
    "        if(len(data_gen)==0):\n",
    "            data_gen = data + data_gen\n",
    "\n",
    "        print(no_new_data, sum(X_new))            \n",
    "        if(no_new_data > sum(X_new)):\n",
    "            data_gen = data + data_gen\n",
    "            data_gen = list(data_gen + list(np.random.choice(data_gen,int(no_new_data-sum(X_new)),replace = False)))    \n",
    "\n",
    "        data_gen = list(np.random.choice(data_gen,int(no_new_data),replace = False))   \n",
    "        \n",
    "        if(preserve == True):\n",
    "            data_gen = data + data_gen\n",
    "    \n",
    "        #sns.distplot(data_gen)     \n",
    "        print('\\nNew data generated:', len(data_gen[len(data):]), '\\nNew data:', len(data_gen), '\\n') \n",
    "        return data_gen\n",
    "        \n",
    "    else:\n",
    "        print('NA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2700468-ca31-4fff-8539-d6139ba6e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_class : This parameter represents the DataFrame containing the data.\n",
    "#diff :This parameter specifies the difference or ratio between the number of samples in the smallest and largest classes after splitting\n",
    "#label_name: It's the column in your DataFrame that you want to use for splitting the data.\n",
    "#cd: It can take values 'c' for continuous classes or 'd' for discrete classes.\n",
    "#label_column: It helps the function identify which column contains the labels or classes.\n",
    "#preserve: If set to `True`, the original dataset will be included in the split data; otherwise, it won't be included.\n",
    "def label_split(df_class, diff, label_name , cd, label_column, preserve):\n",
    "    cdi = 0\n",
    "    df_ = pd.DataFrame(columns=[])\n",
    "    del df_class[label_column]\n",
    "    for (columnName, columnData) in df_class.iteritems(): \n",
    "        print(columnName)\n",
    "        feat_type = cd[cdi]\n",
    "        df_[columnName] = histogram_sampler(list(columnData.values), diff, feat_type, preserve)\n",
    "        cdi+=1\n",
    "    df_[label_column] = label_name\n",
    "    print(df_)\n",
    "\n",
    "    return df_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38fa058-c7a2-4c45-9a9f-51ef5676efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balance(data, label_column, cd, augment = False, preserve = True):\n",
    "    split_list=[]\n",
    "    #label_column = 'species'\n",
    "    for label, df_label in data.groupby(label_column):\n",
    "        split_list.append(df_label)\n",
    "\n",
    "    maxLength = max(len(x) for x in split_list)\n",
    "\n",
    "    if(augment == False):\n",
    "        augmented_list=[]\n",
    "        for i in range(0,len(split_list)):\n",
    "\n",
    "            label_name = list(set(split_list[i][label_column]))[0]\n",
    "            diff = maxLength - len(split_list[i])\n",
    "            augmented_list.append(label_split(split_list[i], diff, label_name, cd, label_column, preserve))\n",
    "        finaldf = pd.DataFrame(columns=[])\n",
    "\n",
    "    elif(type(augment) == dict):\n",
    "        augmented_list=[]\n",
    "        for i in range(0,len(split_list)):\n",
    "\n",
    "            label_name = list(set(split_list[i][label_column]))[0]\n",
    "            diff = augment[label_name]\n",
    "            augmented_list.append(label_split(split_list[i], diff, label_name, cd, label_column, preserve))\n",
    "        finaldf = pd.DataFrame(columns=[])\n",
    "\n",
    "    elif(type(augment) == int):\n",
    "\n",
    "        label_count = dict(Counter(list(df[label_column])))\n",
    "        count_key = list(label_count.keys())\n",
    "        count_val = list(label_count.values())\n",
    "\n",
    "        for i in range(0,len(count_val)):\n",
    "            count_val[i] = round(count_val[i] * augment  /sum(list(label_count.values())))\n",
    "\n",
    "\n",
    "        while(sum(count_val) != augment):\n",
    "            if(sum(count_val) > augment):\n",
    "                rand_indx = int(np.random.rand() * len(count_val))\n",
    "                if(count_val[rand_indx] > 0):\n",
    "                    count_val[rand_indx]-= 1\n",
    "\n",
    "            else:\n",
    "                rand_indx = int(np.random.rand() * len(count_val))\n",
    "                count_val[rand_indx]+= 1\n",
    "\n",
    "        new_count = dict(zip(count_key, count_val))\n",
    "\n",
    "        augmented_list=[]\n",
    "        for i in range(0,len(split_list)):\n",
    "\n",
    "            label_name = list(set(split_list[i][label_column]))[0]\n",
    "            diff = new_count[label_name]\n",
    "            augmented_list.append(label_split(split_list[i], diff, label_name, cd, label_column, preserve))\n",
    "        finaldf = pd.DataFrame(columns=[])\n",
    "\n",
    "\n",
    "        #finaldf = class_balance(data, label_column, cd, augment = new_count, preserve = preserve)\n",
    "\n",
    "\n",
    "    for i in range(0,len(split_list)):\n",
    "        finaldf = pd.concat([finaldf,augmented_list[i]],axis=0)\n",
    "\n",
    "    return finaldf\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01651932-1b29-4f4e-82b4-f28c6e506afb",
   "metadata": {},
   "source": [
    "# This is where you begin coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde77460-b313-486b-98d2-f3ab04073f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Upload your dataset file\n",
    "#If the dataset is not imbalanced, you MUST make it\n",
    "#The program will run and give you a final table creating the new data and time for execution \n",
    "\n",
    "# Check if label column is imbalanced assuming label column has an index of 150\n",
    "print(df_table.iloc[:, 150].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d8138-50b4-433f-9c34-a65799ba8383",
   "metadata": {},
   "source": [
    "# Imbalance Label Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf41f8-3862-402d-bfa2-8a3af95ce6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_table is your DataFrame and the label column is at index 150\n",
    "# Create a new label column with 300 zeros and 200 ones\n",
    "label1 = np.array([0] * 300 + [1] * 200)\n",
    "\n",
    "# Shuffle randomly the new label column\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(label1)\n",
    "\n",
    "# Replace the label column with the new imbalanced labels\n",
    "df_table.iloc[:, 150] = label1\n",
    "\n",
    "# Optional: Shuffle the entire DataFrame to mix rows\n",
    "df_imbalance = df_table.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "print(df_imbalance.iloc[:, 150].value_counts())\n",
    "\n",
    "#print(df_table.iloc[:, 150]) #this prints label column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd04b9-e1aa-4b22-814b-8adf53a6babb",
   "metadata": {},
   "source": [
    "# Imbalance Label column again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b15fe-e52f-492c-a1ac-e8f72662126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second imbalanced label column with 200 zeros and 300 ones\n",
    "label_column = np.array([0] * 200 + [1] * 300)\n",
    "\n",
    "# Shuffle randomly the new label column\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(label_column)\n",
    "\n",
    "# Add the new label column to the DataFrame\n",
    "df_imbalance['label_column'] = label_column\n",
    "\n",
    "print(df_imbalance['label_column'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abadb93a-5c06-4af2-b178-b9495bfc538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Timing the Execution: (do not change this)\n",
    "start_time = time.time()\n",
    "\n",
    "#Create a List cd with Specific Values and prints it \n",
    "cd = []\n",
    "for i in range(0,151): #you need to input your number of columns \n",
    "    cd.append('c') \n",
    "print(cd)\n",
    "\n",
    "# Optional: Assign a temporary name to the label column --> Only do this if your label column has no label\n",
    "df_imbalance = df_imbalance.rename(columns={df_imbalance.columns[150]: 'label'})\n",
    "\n",
    "# Now you can use the renamed column in your class_balance function\n",
    "a_df = class_balance(df_imbalance, label_column = 'label', cd = cd, augment = False, preserve = True)\n",
    "\n",
    "# Optional: Print the resulting DataFrame or label counts to verify\n",
    "print(a_df['label'].value_counts())\n",
    "\n",
    "#Print the Time Taken for Execution:\n",
    "print(\"\\n\\n\\n>>>>>>>>> %s seconds \" % (time.time() - start_time))\n",
    "a_df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6bb87e-3f23-4b4e-b381-59b62be2556f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
