{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece7058b-ece8-41c0-8c75-8c925c7d0551",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09934d90-89ed-4db6-ad13-ca656b3e7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import tensorflow as tf\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a42f6be-c810-4fd2-b06e-3fc8b7cffb2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755933ed-d7e6-4e38-8df8-d14d33881048",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94638703-9415-47c6-b72d-44e5519387fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.645936</td>\n",
       "      <td>4.444420</td>\n",
       "      <td>5.882756</td>\n",
       "      <td>6.699313</td>\n",
       "      <td>3.362616</td>\n",
       "      <td>5.624176</td>\n",
       "      <td>4.552078</td>\n",
       "      <td>2.622931</td>\n",
       "      <td>6.745673</td>\n",
       "      <td>5.324437</td>\n",
       "      <td>...</td>\n",
       "      <td>5.005013</td>\n",
       "      <td>5.525472</td>\n",
       "      <td>5.835458</td>\n",
       "      <td>4.334682</td>\n",
       "      <td>5.882338</td>\n",
       "      <td>4.320398</td>\n",
       "      <td>7.020972</td>\n",
       "      <td>4.373049</td>\n",
       "      <td>6.877316</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.685283</td>\n",
       "      <td>6.174860</td>\n",
       "      <td>5.757977</td>\n",
       "      <td>6.520946</td>\n",
       "      <td>5.247406</td>\n",
       "      <td>7.182799</td>\n",
       "      <td>7.582562</td>\n",
       "      <td>5.345074</td>\n",
       "      <td>6.239722</td>\n",
       "      <td>5.959306</td>\n",
       "      <td>...</td>\n",
       "      <td>5.936925</td>\n",
       "      <td>7.035587</td>\n",
       "      <td>5.807185</td>\n",
       "      <td>6.260498</td>\n",
       "      <td>5.713241</td>\n",
       "      <td>4.741151</td>\n",
       "      <td>8.523618</td>\n",
       "      <td>7.544684</td>\n",
       "      <td>6.321774</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.833938</td>\n",
       "      <td>5.480186</td>\n",
       "      <td>4.660813</td>\n",
       "      <td>2.640568</td>\n",
       "      <td>4.991246</td>\n",
       "      <td>5.329018</td>\n",
       "      <td>4.527029</td>\n",
       "      <td>4.486931</td>\n",
       "      <td>5.577468</td>\n",
       "      <td>4.682285</td>\n",
       "      <td>...</td>\n",
       "      <td>3.801542</td>\n",
       "      <td>3.500481</td>\n",
       "      <td>5.433775</td>\n",
       "      <td>4.678873</td>\n",
       "      <td>5.093956</td>\n",
       "      <td>4.844797</td>\n",
       "      <td>5.067531</td>\n",
       "      <td>5.539606</td>\n",
       "      <td>4.852789</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.840690</td>\n",
       "      <td>3.332420</td>\n",
       "      <td>4.881624</td>\n",
       "      <td>4.554784</td>\n",
       "      <td>4.809637</td>\n",
       "      <td>2.666808</td>\n",
       "      <td>5.156898</td>\n",
       "      <td>4.464810</td>\n",
       "      <td>5.552007</td>\n",
       "      <td>4.911071</td>\n",
       "      <td>...</td>\n",
       "      <td>5.185953</td>\n",
       "      <td>5.964207</td>\n",
       "      <td>5.399361</td>\n",
       "      <td>3.848238</td>\n",
       "      <td>5.883973</td>\n",
       "      <td>5.656945</td>\n",
       "      <td>4.562846</td>\n",
       "      <td>4.012647</td>\n",
       "      <td>6.632066</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.443890</td>\n",
       "      <td>5.533707</td>\n",
       "      <td>5.561488</td>\n",
       "      <td>4.913582</td>\n",
       "      <td>5.843202</td>\n",
       "      <td>5.324853</td>\n",
       "      <td>5.749330</td>\n",
       "      <td>5.860838</td>\n",
       "      <td>5.765502</td>\n",
       "      <td>5.117370</td>\n",
       "      <td>...</td>\n",
       "      <td>3.163400</td>\n",
       "      <td>5.900974</td>\n",
       "      <td>4.625896</td>\n",
       "      <td>5.144522</td>\n",
       "      <td>4.693454</td>\n",
       "      <td>6.534560</td>\n",
       "      <td>4.659843</td>\n",
       "      <td>4.017394</td>\n",
       "      <td>6.300626</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  4.645936  4.444420  5.882756  6.699313  3.362616  5.624176  4.552078   \n",
       "1  6.685283  6.174860  5.757977  6.520946  5.247406  7.182799  7.582562   \n",
       "2  5.833938  5.480186  4.660813  2.640568  4.991246  5.329018  4.527029   \n",
       "3  3.840690  3.332420  4.881624  4.554784  4.809637  2.666808  5.156898   \n",
       "4  5.443890  5.533707  5.561488  4.913582  5.843202  5.324853  5.749330   \n",
       "\n",
       "        7         8         9    ...       141       142       143       144  \\\n",
       "0  2.622931  6.745673  5.324437  ...  5.005013  5.525472  5.835458  4.334682   \n",
       "1  5.345074  6.239722  5.959306  ...  5.936925  7.035587  5.807185  6.260498   \n",
       "2  4.486931  5.577468  4.682285  ...  3.801542  3.500481  5.433775  4.678873   \n",
       "3  4.464810  5.552007  4.911071  ...  5.185953  5.964207  5.399361  3.848238   \n",
       "4  5.860838  5.765502  5.117370  ...  3.163400  5.900974  4.625896  5.144522   \n",
       "\n",
       "        145       146       147       148       149  150  \n",
       "0  5.882338  4.320398  7.020972  4.373049  6.877316  1.0  \n",
       "1  5.713241  4.741151  8.523618  7.544684  6.321774  0.0  \n",
       "2  5.093956  4.844797  5.067531  5.539606  4.852789  1.0  \n",
       "3  5.883973  5.656945  4.562846  4.012647  6.632066  1.0  \n",
       "4  4.693454  6.534560  4.659843  4.017394  6.300626  1.0  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "#data = np.loadtxt(\"uniform_small_d_1.tex\")\n",
    "#data = np.loadtxt(\"uniform_large_d_1.tex\")\n",
    "#data = np.loadtxt(\"gaussian_small_d_1.tex\")\n",
    "data = np.loadtxt(\"gaussian_large_d_1.tex\")\n",
    "\n",
    "# Creating NumPy array\n",
    "array = np.array(data)\n",
    "\n",
    "# Converting to Pandas DataFrame\n",
    "df_table = pd.DataFrame(array)\n",
    "\n",
    "# Displaying the table\n",
    "df_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb966a3-0384-4744-ac1d-b457f183c5c6",
   "metadata": {},
   "source": [
    "# Split dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bf847fe-ba77-4ce5-b3ee-4d4744d3b159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.005013</td>\n",
       "      <td>5.525472</td>\n",
       "      <td>5.835458</td>\n",
       "      <td>4.334682</td>\n",
       "      <td>5.882338</td>\n",
       "      <td>4.320398</td>\n",
       "      <td>7.020972</td>\n",
       "      <td>4.373049</td>\n",
       "      <td>6.877316</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.936925</td>\n",
       "      <td>7.035587</td>\n",
       "      <td>5.807185</td>\n",
       "      <td>6.260498</td>\n",
       "      <td>5.713241</td>\n",
       "      <td>4.741151</td>\n",
       "      <td>8.523618</td>\n",
       "      <td>7.544684</td>\n",
       "      <td>6.321774</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.801542</td>\n",
       "      <td>3.500481</td>\n",
       "      <td>5.433775</td>\n",
       "      <td>4.678873</td>\n",
       "      <td>5.093956</td>\n",
       "      <td>4.844797</td>\n",
       "      <td>5.067531</td>\n",
       "      <td>5.539606</td>\n",
       "      <td>4.852789</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.185953</td>\n",
       "      <td>5.964207</td>\n",
       "      <td>5.399361</td>\n",
       "      <td>3.848238</td>\n",
       "      <td>5.883973</td>\n",
       "      <td>5.656945</td>\n",
       "      <td>4.562846</td>\n",
       "      <td>4.012647</td>\n",
       "      <td>6.632066</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.163400</td>\n",
       "      <td>5.900974</td>\n",
       "      <td>4.625896</td>\n",
       "      <td>5.144522</td>\n",
       "      <td>4.693454</td>\n",
       "      <td>6.534560</td>\n",
       "      <td>4.659843</td>\n",
       "      <td>4.017394</td>\n",
       "      <td>6.300626</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...       141       142  \\\n",
       "0  5.0  4.0  6.0  7.0  3.0  6.0  5.0  3.0  7.0  5.0  ...  5.005013  5.525472   \n",
       "1  7.0  6.0  6.0  7.0  5.0  7.0  8.0  5.0  6.0  6.0  ...  5.936925  7.035587   \n",
       "2  6.0  5.0  5.0  3.0  5.0  5.0  5.0  4.0  6.0  5.0  ...  3.801542  3.500481   \n",
       "3  4.0  3.0  5.0  5.0  5.0  3.0  5.0  4.0  6.0  5.0  ...  5.185953  5.964207   \n",
       "4  5.0  6.0  6.0  5.0  6.0  5.0  6.0  6.0  6.0  5.0  ...  3.163400  5.900974   \n",
       "\n",
       "        143       144       145       146       147       148       149  150  \n",
       "0  5.835458  4.334682  5.882338  4.320398  7.020972  4.373049  6.877316  1.0  \n",
       "1  5.807185  6.260498  5.713241  4.741151  8.523618  7.544684  6.321774  0.0  \n",
       "2  5.433775  4.678873  5.093956  4.844797  5.067531  5.539606  4.852789  1.0  \n",
       "3  5.399361  3.848238  5.883973  5.656945  4.562846  4.012647  6.632066  1.0  \n",
       "4  4.625896  5.144522  4.693454  6.534560  4.659843  4.017394  6.300626  1.0  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the dataset, change 25 columns to 'categorical'\n",
    "#Loop, converts floats to ints and then those ints to category\n",
    "for i in range(25):\n",
    "    df_table.iloc[:,i] = df_table.iloc[:,i].round()\n",
    "    df_table.iloc[:,i] = df_table.iloc[:,i].astype(int)\n",
    "    df_table.iloc[:,i] = df_table.iloc[:,i].astype(\"category\")\n",
    "\n",
    "df_table.iloc[:, 150] = df_table.iloc[:, 150].astype(\"category\")\n",
    "\n",
    "df_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8497a445-2262-4328-809b-b16f919feb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into training set and test set\n",
    "#test_size: in this case it is 70% training and 30% testing\n",
    "#random_state: sets a seed for a random number generator that splits the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_table.iloc[:,0:150], df_table.iloc[:,-1], test_size=0.2, random_state=52)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba5937-4cd7-4a48-98cf-254d8567d4df",
   "metadata": {},
   "source": [
    "# Call Super function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83d9c5ae-6e65-47d0-b056-ccb4e500841e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.metrics' has no attribute 'root_mean_squared_error'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 332\u001b[0m\n\u001b[1;32m    322\u001b[0m df \u001b[38;5;241m=\u001b[39m generateGaussianDistributions(\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Prints the F1 Score of the augmented data for each augmentation method\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# print('pmOne:', superFunction(df, \"pmOne\", 200, 15, \"kNN\", unit=0.1)[1]['F1 Score'][1])\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# print('randSwap:', superFunction(df, \"randSwap\", 200, 15, \"kNN\")[1]['F1 Score'][1])\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# print('gausNoise:', superFunction(df, \"gausNoise\", 200, 15, \"kNN\", noise=0.1)[1]['F1 Score'][1])\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# print('modPmOne:', superFunction(df, \"modpmone\", 200, 15, \"kNN\", unit=0.1)[1]['F1 Score'][1])\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# print('modGausNoise:', superFunction(df, \"modgausnoise\", 200, 15, \"kNN\")[1]['F1 Score'][1])\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m orig, new \u001b[38;5;241m=\u001b[39m superFunction(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpmOne\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkNN\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Augmentation:\u001b[39m\u001b[38;5;124m'\u001b[39m, orig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw/ Augmentation:\u001b[39m\u001b[38;5;124m'\u001b[39m, new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[22], line 309\u001b[0m, in \u001b[0;36msuperFunction\u001b[0;34m(data, method, nrows, nvalues, classifier, unit, noise)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Split into training and test set\u001b[39;00m\n\u001b[1;32m    306\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m    307\u001b[0m     X, Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m--> 309\u001b[0m original_accuracies \u001b[38;5;241m=\u001b[39m runClassifier(X_train, Y_train, X_test, Y_test, classifier)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Applies augmentation method to X_train\u001b[39;00m\n\u001b[1;32m    312\u001b[0m augmented \u001b[38;5;241m=\u001b[39m betterApplyAugmentationMethods(X_train, method, nrows, nvalues, unit\u001b[38;5;241m=\u001b[39munit, noise\u001b[38;5;241m=\u001b[39mnoise)\n",
      "Cell \u001b[0;32mIn[22], line 231\u001b[0m, in \u001b[0;36mrunClassifier\u001b[0;34m(X_train, Y_train, X_test, Y_test, classifier)\u001b[0m\n\u001b[1;32m    229\u001b[0m acc \u001b[38;5;241m=\u001b[39m skm\u001b[38;5;241m.\u001b[39maccuracy_score(Y_test, predicted_values)\n\u001b[1;32m    230\u001b[0m mae_accuracy \u001b[38;5;241m=\u001b[39m skm\u001b[38;5;241m.\u001b[39mmean_absolute_error(Y_test, predicted_values)\n\u001b[0;32m--> 231\u001b[0m rmse_accuracy \u001b[38;5;241m=\u001b[39m skm\u001b[38;5;241m.\u001b[39mroot_mean_squared_error(Y_test, predicted_values)\n\u001b[1;32m    232\u001b[0m f1_accuracy \u001b[38;5;241m=\u001b[39m skm\u001b[38;5;241m.\u001b[39mf1_score(Y_test, predicted_values)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Appends accuracies to accuracy table\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sklearn.metrics' has no attribute 'root_mean_squared_error'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr 23 13:45:56 2024\n",
    "\n",
    "@author: cdiet\n",
    "\"\"\"\n",
    "def betterApplyAugmentationMethods(X_train, method, nrows, nvalues=None, unit=None, noise=None):\n",
    "    # If nvalues not specified, entire column is selected\n",
    "    if nvalues is None:\n",
    "        nvalues = X_train.shape[1] - 1\n",
    "\n",
    "    if str(method).lower() == 'pmone':\n",
    "        # Creates empty dataframe to store augmented data\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects nvalues amount of unique column indexes\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Iterates through augmentedData and applies plus or minus to randCols indexes\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in randCols:\n",
    "                if (random.randint(0, 1) == 0):\n",
    "                    augmentedDf.iloc[i, col] += unit\n",
    "                else:\n",
    "                    augmentedDf.iloc[i, col] -= unit\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'modpmone':\n",
    "        # Creates empty dataframe to store augmented data\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects nvalues amount of unique column indexes\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Iterates through augmentedData and applies plus or minus to randCols indexes\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in randCols:\n",
    "                colMax = X_train.iloc[:, col].max()\n",
    "                colMin = X_train.iloc[:, col].min()\n",
    "\n",
    "                if (augmentedDf.iloc[i, col] + unit < colMax and augmentedDf.iloc[i, col] - unit > colMin):\n",
    "                    if (random.randint(0, 1) == 0):\n",
    "                        if (augmentedDf.iloc[i, col] + unit <= colMax):\n",
    "                            augmentedDf.iloc[i, col] += unit\n",
    "                        else:\n",
    "                            augmentedDf.iloc[i, col] -= unit\n",
    "                    else:\n",
    "                        if (augmentedDf.iloc[i, col] - unit >= colMin):\n",
    "                            augmentedDf.iloc[i, col] -= unit\n",
    "                        else:\n",
    "                            augmentedDf.iloc[i, col] += unit\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'gausnoise':\n",
    "        # Creates empty dataframe to hold augmented rows\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Selects random rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects random unique column index\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Applies Gaussian noise to randCols values stored in array\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for cols in randCols:\n",
    "                augmentedDf.iloc[i, cols] += np.random.normal(0, noise)\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'modgausnoise':\n",
    "        # Creates an empty dataframe to hold augmented observations\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects unique column indexs from data\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Appends randomly selected rows from data to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Generates Gaussian distribution based on columns summary statistics\n",
    "        # Swaps value with random value in generated Gaussian distribution\n",
    "        for col in randCols:\n",
    "            for i in range(augmentedDf.shape[0]):\n",
    "                mean = augmentedDf[col].mean()\n",
    "                stDev = augmentedDf[col].std()\n",
    "\n",
    "                augmentedDf.iloc[i, col] = np.random.normal(mean, stDev)\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'randswap':\n",
    "        # Creates empty dataframe to store augmented rows\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Copies nrows from original data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Picks UNIQUE column indexes to swap\n",
    "        columnIndexSwaps = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Swaps augmentedDf column value from same column in data\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in columnIndexSwaps:\n",
    "                randValue = X_train.iloc[random.randint(0, X_train.shape[0] - 1), col]\n",
    "\n",
    "                augmentedDf.iloc[i, col] = randValue\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    else:\n",
    "        print(\"Method not found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generateLabels(X_train, Y_train, augmented):\n",
    "    # import the class\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # instantiate the model (using the default parameters)\n",
    "    # random.seed(1)\n",
    "    logreg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "    # fit the model with data\n",
    "\n",
    "    # print(y_train)\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    # create the prediction\n",
    "    augmented_labels = pd.DataFrame(logreg.predict(augmented))\n",
    "\n",
    "    Y_train = pd.concat([Y_train, augmented_labels], axis=0, ignore_index=True)\n",
    "\n",
    "    X_train = pd.concat([X_train, augmented], axis=0, ignore_index=True)\n",
    "\n",
    "    return X_train, Y_train\n",
    "\n",
    "\n",
    "def runClassifier(X_train, Y_train, X_test, Y_test, classifier):\n",
    "    # Creates accuracy table dataframe\n",
    "    results_df = pd.DataFrame(columns=[\"Accuracy\", \"Mean Absolute Error\", \"Rooted Mean Square Error\", \"F1 Score\"])\n",
    "\n",
    "    if classifier == \"kNN\":\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "\n",
    "        knn.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "        # Predict on dataset which model has not seen before\n",
    "        predicted_values = knn.predict(X_test)\n",
    "\n",
    "    elif classifier == \"D_tree\":\n",
    "        clf_gini = DecisionTreeClassifier(criterion=\"gini\",\n",
    "                                          random_state=100, max_depth=3, min_samples_leaf=5)\n",
    "\n",
    "        clf_gini.fit(X_train, Y_train)\n",
    "\n",
    "        predicted_values = clf_gini.predict(X_test)\n",
    "\n",
    "\n",
    "    elif classifier == \"Naive_bayes\":\n",
    "\n",
    "        classifier = GaussianNB()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "\n",
    "        predicted_values = classifier.predict(X_test)\n",
    "\n",
    "    elif classifier == \"ANN\":\n",
    "        # Performing Feature Scaling\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "\n",
    "        # Initializing Artificial Neural Network\n",
    "        ann = tf.keras.models.Sequential()\n",
    "\n",
    "        # Adding Hidden Layers\n",
    "        ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))\n",
    "        ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))\n",
    "\n",
    "        # Adding output layers\n",
    "        ann.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "        # compiling the Artificial Neural Network\n",
    "        ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "        # Fitting the Artificial Neural Network\n",
    "        ann.fit(X_train, Y_train, batch_size=32, epochs=100)\n",
    "\n",
    "        # Generate the predicted labels\n",
    "        first_predicted_values = ann.predict(X_test)\n",
    "        second_predicted_labels = first_predicted_values > .5\n",
    "        final_predicted_labels = second_predicted_labels * 1\n",
    "        predicted_values = final_predicted_labels\n",
    "\n",
    "    # SVM\n",
    "    elif classifier == \"SVM\":\n",
    "        # random.seed(1)\n",
    "        svm = SVC(gamma=2, C=1, kernel='linear', max_iter=1000000, random_state=0)\n",
    "\n",
    "        # fit the model with data\n",
    "        # svm.fit(X_train,y_train)\n",
    "        svm.fit(X_train, Y_train)\n",
    "        predicted_values = svm.predict(X_test)\n",
    "\n",
    "    else:\n",
    "        print(\"Unknown classifier\")\n",
    "        return None\n",
    "\n",
    "    # Accuracy\n",
    "    acc = skm.accuracy_score(Y_test, predicted_values)\n",
    "    mae_accuracy = skm.mean_absolute_error(Y_test, predicted_values)\n",
    "    rmse_accuracy = skm.root_mean_squared_error(Y_test, predicted_values)\n",
    "    f1_accuracy = skm.f1_score(Y_test, predicted_values)\n",
    "\n",
    "    # Appends accuracies to accuracy table\n",
    "    results_df.loc[1, 'Accuracy'] = acc\n",
    "    results_df.loc[1, 'Mean Absolute Error'] = mae_accuracy\n",
    "    results_df.loc[1, 'Rooted Mean Square Error'] = rmse_accuracy\n",
    "    results_df.loc[1, 'F1 Score'] = f1_accuracy\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "generatedGaussianDistrubutions Inputs\n",
    "\n",
    "nrows: Number of rows\n",
    "ncolumns: Number of columns\n",
    "median1: First Gaussian distribution median (center)\n",
    "median2: Second Gaussian distribution median (center)\n",
    "spread1: First Gaussian distrbiution spread\n",
    "spread2: Second Gaussian distribution spread\n",
    "\n",
    "Note:\n",
    "if label == 0, first Gaussian distribution\n",
    "if label == 1, second Gaussian distribution\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generateGaussianDistributions(nrows, ncolumns, median1, median2, spread1, spread2):\n",
    "    # Creates first Gaussian distribution\n",
    "    label1 = pd.DataFrame(np.random.normal(median1, spread1, size=(int(nrows / 2), ncolumns)))\n",
    "    # Adds new column for label\n",
    "    label1['label'] = 0\n",
    "\n",
    "    # Creates second Gaussian distribution\n",
    "    label2 = pd.DataFrame(np.random.normal(median2, spread2, size=(int(nrows / 2), ncolumns)))\n",
    "    # Adds new column for label\n",
    "    label2['label'] = 1\n",
    "\n",
    "    # Combines both Gaussian distributions\n",
    "    df = pd.concat([label1, label2])\n",
    "\n",
    "    # Shuffles Gaussian distributions\n",
    "    shuffled_df = pd.DataFrame(np.random.permutation(df))\n",
    "\n",
    "    return shuffled_df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "superFunction applies all methods from the flowchart: augmentation, \n",
    "interpretable/uninterpretable classifiers, and accuracy, taking all inputs from\n",
    "these functions and outputs the accuracy of the augmented data.\n",
    "\n",
    "Inputs:\n",
    "    data: A dataframe where the labels are the LAST column\n",
    "    method: The augmentation method the user wants to use for the data\n",
    "    nrows: How many output augmentation rows are wanted\n",
    "    nvalues: The number of values in each row that need to be augmented\n",
    "    classifier: The classifier the user wants to use\n",
    "    unit(optional): Only for the pmOne augmentation method and is the unit the \n",
    "    augmented data will differ from original data by\n",
    "    noise(optional): Only for the gausNoise augmentation method and denotes the\n",
    "    percent by which the augmented data varies from original data\n",
    "\n",
    "\n",
    "Outputs:\n",
    "    Returns two dataframes of original and augmented data accuracy measures\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def superFunction(data, method, nrows, nvalues, classifier, unit=None, noise=None):\n",
    "    X = data.drop(data.shape[1] - 1, axis=1)\n",
    "    Y = data[data.shape[1] - 1]\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    original_accuracies = runClassifier(X_train, Y_train, X_test, Y_test, classifier)\n",
    "\n",
    "    # Applies augmentation method to X_train\n",
    "    augmented = betterApplyAugmentationMethods(X_train, method, nrows, nvalues, unit=unit, noise=noise)\n",
    "\n",
    "    # Generates labels and concat to X_train and Y_train\n",
    "    X_train, Y_train = generateLabels(X_train, Y_train, augmented)\n",
    "\n",
    "    augmented_accuracies = runClassifier(X_train, Y_train, X_test, Y_test, classifier)\n",
    "\n",
    "    return original_accuracies, augmented_accuracies\n",
    "\n",
    "\n",
    "df = generateGaussianDistributions(500, 150, 0, 0.25, 1, 1)\n",
    "\n",
    "# Prints the F1 Score of the augmented data for each augmentation method\n",
    "# print('pmOne:', superFunction(df, \"pmOne\", 200, 15, \"kNN\", unit=0.1)[1]['F1 Score'][1])\n",
    "# print('randSwap:', superFunction(df, \"randSwap\", 200, 15, \"kNN\")[1]['F1 Score'][1])\n",
    "# print('gausNoise:', superFunction(df, \"gausNoise\", 200, 15, \"kNN\", noise=0.1)[1]['F1 Score'][1])\n",
    "# print('modPmOne:', superFunction(df, \"modpmone\", 200, 15, \"kNN\", unit=0.1)[1]['F1 Score'][1])\n",
    "# print('modGausNoise:', superFunction(df, \"modgausnoise\", 200, 15, \"kNN\")[1]['F1 Score'][1])\n",
    "\n",
    "\n",
    "orig, new = superFunction(df, \"pmOne\", 200, 15, \"kNN\", unit=0.1)\n",
    "print('No Augmentation:', orig['F1 Score'][1])\n",
    "print('w/ Augmentation:', new['F1 Score'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16402599-3d20-42ad-a329-3f8e864f882f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
