{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c68aabc6-0f04-4d84-88a6-78c41c87bd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Install TensorFlow\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "433f8e46-2392-4582-86d7-53152a3ffabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def betterApplyAugmentationMethods(X_train, method, nrows, nvalues=None, unit=None, noise=None):\n",
    "    # If nvalues not specified, entire column is selected\n",
    "    if nvalues is None:\n",
    "        nvalues = X_train.shape[1] - 1\n",
    "\n",
    "    if str(method).lower() == 'pmone':\n",
    "        # Creates empty dataframe to store augmented data\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects nvalues amount of unique column indexes\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Iterates through augmentedData and applies plus or minus to randCols indexes\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in randCols:\n",
    "                if (random.randint(0, 1) == 0):\n",
    "                    augmentedDf.iloc[i, col] += unit\n",
    "                else:\n",
    "                    augmentedDf.iloc[i, col] -= unit\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'modpmone':\n",
    "        # Creates empty dataframe to store augmented data\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects nvalues amount of unique column indexes\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Iterates through augmentedData and applies plus or minus to randCols indexes\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in randCols:\n",
    "                colMax = X_train.iloc[:, col].max()\n",
    "                colMin = X_train.iloc[:, col].min()\n",
    "\n",
    "                if (augmentedDf.iloc[i, col] + unit < colMax and augmentedDf.iloc[i, col] - unit > colMin):\n",
    "                    if (random.randint(0, 1) == 0):\n",
    "                        if (augmentedDf.iloc[i, col] + unit <= colMax):\n",
    "                            augmentedDf.iloc[i, col] += unit\n",
    "                        else:\n",
    "                            augmentedDf.iloc[i, col] -= unit\n",
    "                    else:\n",
    "                        if (augmentedDf.iloc[i, col] - unit >= colMin):\n",
    "                            augmentedDf.iloc[i, col] -= unit\n",
    "                        else:\n",
    "                            augmentedDf.iloc[i, col] += unit\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'gausnoise':\n",
    "        # Creates empty dataframe to hold augmented rows\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Selects random rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects random unique column index\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Applies Gaussian noise to randCols values stored in array\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for cols in randCols:\n",
    "                augmentedDf.iloc[i, cols] += np.random.normal(0, noise)\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'modgausnoise':\n",
    "        # Creates an empty dataframe to hold augmented observations\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Randomly selects unique column indexs from data\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Appends randomly selected rows from data to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Generates Gaussian distribution based on columns summary statistics\n",
    "        # Swaps value with random value in generated Gaussian distribution\n",
    "        for col in randCols:\n",
    "            for i in range(augmentedDf.shape[0]):\n",
    "                mean = augmentedDf[col].mean()\n",
    "                stDev = augmentedDf[col].std()\n",
    "\n",
    "                augmentedDf.iloc[i, col] = np.random.normal(mean, stDev)\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    elif str(method).lower() == 'randswap':\n",
    "        # Creates empty dataframe to store augmented rows\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Copies nrows from original data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Picks UNIQUE column indexes to swap\n",
    "        columnIndexSwaps = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Swaps augmentedDf column value from same column in data\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in columnIndexSwaps:\n",
    "                randValue = X_train.iloc[random.randint(0, X_train.shape[0] - 1), col]\n",
    "\n",
    "                augmentedDf.iloc[i, col] = randValue\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    else:\n",
    "        print(\"Method not found\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc32f0-1bc5-4380-a01a-eb818572e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLabels(X_train, Y_train, augmented):\n",
    "    # import the class\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # instantiate the model (using the default parameters)\n",
    "    # random.seed(1)\n",
    "    logreg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "    # fit the model with data\n",
    "\n",
    "    # print(y_train)\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    # create the prediction\n",
    "    augmented_labels = pd.DataFrame(logreg.predict(augmented))\n",
    "\n",
    "    Y_train = pd.concat([Y_train, augmented_labels], axis=0, ignore_index=True)\n",
    "\n",
    "    X_train = pd.concat([X_train, augmented], axis=0, ignore_index=True)\n",
    "\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ff2de-35ee-47d4-acdc-ad91dfd18997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runClassifier(X_train, Y_train, X_test, Y_test, classifier):\n",
    "    # Creates accuracy table dataframe\n",
    "    results_df = pd.DataFrame(columns=[\"Accuracy\", \"Mean Absolute Error\", \"Rooted Mean Square Error\", \"F1 Score\"])\n",
    "\n",
    "    if classifier == \"kNN\":\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "\n",
    "        knn.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "        # Predict on dataset which model has not seen before\n",
    "        predicted_values = knn.predict(X_test)\n",
    "\n",
    "    elif classifier == \"D_tree\":\n",
    "        clf_gini = DecisionTreeClassifier(criterion=\"gini\",\n",
    "                                          random_state=100, max_depth=3, min_samples_leaf=5)\n",
    "\n",
    "        clf_gini.fit(X_train, Y_train)\n",
    "\n",
    "        predicted_values = clf_gini.predict(X_test)\n",
    "\n",
    "\n",
    "    elif classifier == \"Naive_bayes\":\n",
    "\n",
    "        classifier = GaussianNB()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "\n",
    "        predicted_values = classifier.predict(X_test)\n",
    "\n",
    "    elif classifier == \"ANN\":\n",
    "        # Performing Feature Scaling\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "\n",
    "        # Initializing Artificial Neural Network\n",
    "        ann = tf.keras.models.Sequential()\n",
    "\n",
    "        # Adding Hidden Layers\n",
    "        ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))\n",
    "        ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))\n",
    "\n",
    "        # Adding output layers\n",
    "        ann.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "        # compiling the Artificial Neural Network\n",
    "        ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "        # Fitting the Artificial Neural Network\n",
    "        ann.fit(X_train, Y_train, batch_size=32, epochs=100)\n",
    "\n",
    "        # Generate the predicted labels\n",
    "        first_predicted_values = ann.predict(X_test)\n",
    "        second_predicted_labels = first_predicted_values > .5\n",
    "        final_predicted_labels = second_predicted_labels * 1\n",
    "        predicted_values = final_predicted_labels\n",
    "\n",
    "    # SVM\n",
    "    elif classifier == \"SVM\":\n",
    "        # random.seed(1)\n",
    "        svm = SVC(gamma=2, C=1, kernel='linear', max_iter=1000000, random_state=0)\n",
    "\n",
    "        # fit the model with data\n",
    "        # svm.fit(X_train,y_train)\n",
    "        svm.fit(X_train, Y_train)\n",
    "        predicted_values = svm.predict(X_test)\n",
    "\n",
    "    # XGBoost\n",
    "    elif classifier == \"xgboost\":\n",
    "        # Create model instance\n",
    "        bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic', enable_categorical=True)\n",
    "        \n",
    "        # fit model with the training data\n",
    "        bst.fit(X_train, Y_train)\n",
    "        \n",
    "        # make predictions for the test dataset\n",
    "        predicted_values = bst.predict(X_test)\n",
    "\n",
    "    else:\n",
    "        print(\"Unknown classifier\")\n",
    "        return None\n",
    "\n",
    "    # Accuracy\n",
    "    acc = skm.accuracy_score(Y_test, predicted_values)\n",
    "    mae_accuracy = skm.mean_absolute_error(Y_test, predicted_values)\n",
    "    mse_accuracy = skm.mean_squared_error(Y_test, predicted_values) #Fabi\n",
    "    rmse_accuracy = np.sqrt(mse_accuracy) #Fabi\n",
    "    f1_accuracy = skm.f1_score(Y_test, predicted_values)\n",
    "\n",
    "    # Appends accuracies to accuracy table\n",
    "    results_df.loc[1, 'Accuracy'] = acc\n",
    "    results_df.loc[1, 'Mean Absolute Error'] = mae_accuracy\n",
    "    results_df.loc[1, 'Rooted Mean Square Error'] = rmse_accuracy\n",
    "    results_df.loc[1, 'F1 Score'] = f1_accuracy\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71787d-47a3-4ccf-9a79-a1242443a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generatedGaussianDistrubutions Inputs\n",
    "\n",
    "nrows: Number of rows\n",
    "ncolumns: Number of columns\n",
    "median1: First Gaussian distribution median (center)\n",
    "median2: Second Gaussian distribution median (center)\n",
    "spread1: First Gaussian distrbiution spread\n",
    "spread2: Second Gaussian distribution spread\n",
    "\n",
    "Note:\n",
    "if label == 0, first Gaussian distribution\n",
    "if label == 1, second Gaussian distribution\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generateGaussianDistributions(nrows, ncolumns, median1, median2, spread1, spread2):\n",
    "    # Creates first Gaussian distribution\n",
    "    label1 = pd.DataFrame(np.random.normal(median1, spread1, size=(int(nrows / 2), ncolumns)))\n",
    "    # Adds new column for label\n",
    "    label1['label'] = 0\n",
    "\n",
    "    # Creates second Gaussian distribution\n",
    "    label2 = pd.DataFrame(np.random.normal(median2, spread2, size=(int(nrows / 2), ncolumns)))\n",
    "    # Adds new column for label\n",
    "    label2['label'] = 1\n",
    "\n",
    "    # Combines both Gaussian distributions\n",
    "    df = pd.concat([label1, label2])\n",
    "\n",
    "    # Shuffles Gaussian distributions\n",
    "    shuffled_df = pd.DataFrame(np.random.permutation(df))\n",
    "\n",
    "    return shuffled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f871f-8621-4209-9db1-63d7c43a3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "superFunction applies all methods from the flowchart: augmentation, \n",
    "interpretable/uninterpretable classifiers, and accuracy, taking all inputs from\n",
    "these functions and outputs the accuracy of the augmented data.\n",
    "\n",
    "Inputs:\n",
    "    data: A dataframe where the labels are the LAST column\n",
    "    method: The augmentation method the user wants to use for the data\n",
    "    nrows: How many output augmentation rows are wanted\n",
    "    nvalues: The number of values in each row that need to be augmented\n",
    "    classifier: The classifier the user wants to use\n",
    "    unit(optional): Only for the pmOne augmentation method and is the unit the \n",
    "    augmented data will differ from original data by\n",
    "    noise(optional): Only for the gausNoise augmentation method and denotes the\n",
    "    percent by which the augmented data varies from original data\n",
    "\n",
    "\n",
    "Outputs:\n",
    "    Returns two dataframes of original and augmented data accuracy measures\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def superFunction(data, method, nrows, nvalues, classifier, unit=None, noise=None):\n",
    "    X = data.drop(data.shape[1] - 1, axis=1)\n",
    "    Y = data[data.shape[1] - 1]\n",
    "\n",
    "    # Split into training and test set\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=52)\n",
    "\n",
    "    original_accuracies = runClassifier(X_train, Y_train, X_test, Y_test, classifier)\n",
    "\n",
    "    # Applies augmentation method to X_train\n",
    "    augmented = betterApplyAugmentationMethods(X_train, method, nrows, nvalues, unit=unit, noise=noise)\n",
    "\n",
    "    # Generates labels and concat to X_train and Y_train\n",
    "    X_train_augmented, Y_train_augmented = generateLabels(X_train, Y_train, augmented)\n",
    "\n",
    "    augmented_accuracies = runClassifier(X_train_augmented, Y_train_augmented, X_test, Y_test, classifier)\n",
    "\n",
    "    return original_accuracies, augmented_accuracies\n",
    "\n",
    "\n",
    "# df = generateGaussianDistributions(500, 150, 0, 0.25, 1, 1)\n",
    "\n",
    "# Prints the F1 Score of the augmented data for each augmentation method\n",
    "# print('pmOne:', superFunction(df, \"pmOne\", 200, 15, \"kNN\", unit=0.1)[1]['F1 Score'][1])\n",
    "# print('randSwap:', superFunction(df, \"randSwap\", 200, 15, \"kNN\")[1]['F1 Score'][1])\n",
    "# print('gausNoise:', superFunction(df, \"gausNoise\", 200, 15, \"kNN\", noise=0.1)[1]['F1 Score'][1])\n",
    "# print('modPmOne:', superFunction(df, \"modpmone\", 200, 15, \"kNN\", unit=0.1)[1]['F1 Score'][1])\n",
    "# print('modGausNoise:', superFunction(df, \"modgausnoise\", 200, 15, \"kNN\")[1]['F1 Score'][1])\n",
    "\n",
    "\n",
    "# orig, new = superFunction(df, \"pmOne\", 200, 15, \"kNN\", unit=0.1)\n",
    "# print('No Augmentation:', orig['F1 Score'][1])\n",
    "# print('w/ Augmentation:', new['F1 Score'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71828927-309a-4dd9-9068-c8f83579f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"/Users/fabianafazio/Documents/GitHub/BP24/Fabiana/Data/gaussian_large_d_1.tex\")\n",
    "array = np.array(data)\n",
    "df = pd.DataFrame(array)\n",
    "\n",
    "for i in range(25):\n",
    "    df.iloc[:,i] = df.iloc[:,i].round() # Rounding\n",
    "    df.iloc[:,i] = df.iloc[:,i].astype(int) # Integer\n",
    "    df.iloc[:,i] = df.iloc[:,i].astype('category') # Categories\n",
    "\n",
    "   \n",
    "df.iloc[:,150] = df.iloc[:,150].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c12cbb81-7582-449e-a6f8-3ebb628baf8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Categorical is not ordered for operation max\nyou can use .as_ordered() to change the Categorical to an ordered one\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m orig, new \u001b[38;5;241m=\u001b[39m superFunction(df, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodpmone\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, nvalues\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, classifier\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAugmentation method modpmone with xgboost classifier:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBefore augmentation:\u001b[39m\u001b[38;5;124m'\u001b[39m, orig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[26], line 331\u001b[0m, in \u001b[0;36msuperFunction\u001b[0;34m(data, method, nrows, nvalues, classifier, unit, noise)\u001b[0m\n\u001b[1;32m    328\u001b[0m original_accuracies \u001b[38;5;241m=\u001b[39m runClassifier(X_train, Y_train, X_test, Y_test, classifier)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Applies augmentation method to X_train\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m augmented \u001b[38;5;241m=\u001b[39m betterApplyAugmentationMethods(X_train, method, nrows, nvalues, unit\u001b[38;5;241m=\u001b[39munit, noise\u001b[38;5;241m=\u001b[39mnoise)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# Generates labels and concat to X_train and Y_train\u001b[39;00m\n\u001b[1;32m    334\u001b[0m X_train_augmented, Y_train_augmented \u001b[38;5;241m=\u001b[39m generateLabels(X_train, Y_train, augmented)\n",
      "Cell \u001b[0;32mIn[34], line 56\u001b[0m, in \u001b[0;36mbetterApplyAugmentationMethods\u001b[0;34m(X_train, method, nrows, nvalues, unit, noise)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(augmentedDf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m randCols:\n\u001b[0;32m---> 56\u001b[0m         colMax \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39miloc[:, col]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     57\u001b[0m         colMin \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39miloc[:, col]\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (augmentedDf\u001b[38;5;241m.\u001b[39miloc[i, col] \u001b[38;5;241m+\u001b[39m unit \u001b[38;5;241m<\u001b[39m colMax \u001b[38;5;129;01mand\u001b[39;00m augmentedDf\u001b[38;5;241m.\u001b[39miloc[i, col] \u001b[38;5;241m-\u001b[39m unit \u001b[38;5;241m>\u001b[39m colMin):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:6193\u001b[0m, in \u001b[0;36mSeries.max\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6185\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   6186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(\n\u001b[1;32m   6187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6191\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   6192\u001b[0m ):\n\u001b[0;32m-> 6193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:11976\u001b[0m, in \u001b[0;36mNDFrame.max\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(\n\u001b[1;32m  11970\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  11971\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11974\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11975\u001b[0m ):\n\u001b[0;32m> 11976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[1;32m  11977\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m  11978\u001b[0m         nanops\u001b[38;5;241m.\u001b[39mnanmax,\n\u001b[1;32m  11979\u001b[0m         axis,\n\u001b[1;32m  11980\u001b[0m         skipna,\n\u001b[1;32m  11981\u001b[0m         numeric_only,\n\u001b[1;32m  11982\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11983\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:11949\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11945\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[1;32m  11947\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 11949\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(\n\u001b[1;32m  11950\u001b[0m     func, name\u001b[38;5;241m=\u001b[39mname, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[1;32m  11951\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:6119\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m   6117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delegate, ExtensionArray):\n\u001b[1;32m   6118\u001b[0m     \u001b[38;5;66;03m# dispatch to ExtensionArray interface\u001b[39;00m\n\u001b[0;32m-> 6119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m delegate\u001b[38;5;241m.\u001b[39m_reduce(name, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6122\u001b[0m     \u001b[38;5;66;03m# dispatch to numpy arrays\u001b[39;00m\n\u001b[1;32m   6123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m numeric_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miufcb\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   6124\u001b[0m         \u001b[38;5;66;03m# i.e. not is_numeric_dtype(self.dtype)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/categorical.py:2325\u001b[0m, in \u001b[0;36mCategorical._reduce\u001b[0;34m(self, name, skipna, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   2322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_reduce\u001b[39m(\n\u001b[1;32m   2323\u001b[0m     \u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, skipna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, keepdims: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   2324\u001b[0m ):\n\u001b[0;32m-> 2325\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_reduce(name, skipna\u001b[38;5;241m=\u001b[39mskipna, keepdims\u001b[38;5;241m=\u001b[39mkeepdims, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margmin\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   2327\u001b[0m         \u001b[38;5;66;03m# don't wrap in Categorical!\u001b[39;00m\n\u001b[1;32m   2328\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/base.py:1860\u001b[0m, in \u001b[0;36mExtensionArray._reduce\u001b[0;34m(self, name, skipna, keepdims, **kwargs)\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1857\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1858\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not support reduction \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1859\u001b[0m     )\n\u001b[0;32m-> 1860\u001b[0m result \u001b[38;5;241m=\u001b[39m meth(skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n\u001b[1;32m   1862\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([result])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/categorical.py:2383\u001b[0m, in \u001b[0;36mCategorical.max\u001b[0;34m(self, skipna, **kwargs)\u001b[0m\n\u001b[1;32m   2381\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_minmax_axis(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m   2382\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_max((), kwargs)\n\u001b[0;32m-> 2383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_for_ordered(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_codes):\n\u001b[1;32m   2386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mna_value\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/categorical.py:1844\u001b[0m, in \u001b[0;36mCategorical.check_for_ordered\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"assert that we are ordered\"\"\"\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordered:\n\u001b[0;32m-> 1844\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1845\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategorical is not ordered for operation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use .as_ordered() to change the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategorical to an ordered one\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1848\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Categorical is not ordered for operation max\nyou can use .as_ordered() to change the Categorical to an ordered one\n"
     ]
    }
   ],
   "source": [
    "orig, new = superFunction(df, method=\"modpmone\", nrows=200, nvalues=15, classifier=\"xgboost\", unit=0.1)\n",
    "print(\"Augmentation method modpmone with xgboost classifier:\")\n",
    "print('Before augmentation:', orig['Accuracy'][1])\n",
    "print('After augmentation:', new['Accuracy'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c3cda-52f3-48f6-9022-ec69cc0ef841",
   "metadata": {},
   "source": [
    "# Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12c7761d-48b6-49ec-9fb1-aa5efa650665",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot setitem on a Categorical with a new category (7.902380289776927), set the categories first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Apply the gausnoise augmentation method to the training set\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m augmented_df \u001b[38;5;241m=\u001b[39m betterApplyAugmentationMethods(X_train, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgausnoise\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, nvalues\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Example to show the augmented data\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(augmented_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[65], line 21\u001b[0m, in \u001b[0;36mbetterApplyAugmentationMethods\u001b[0;34m(X_train, method, nrows, nvalues, unit, noise)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(augmentedDf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m randCols:\n\u001b[0;32m---> 21\u001b[0m             augmentedDf\u001b[38;5;241m.\u001b[39miloc[i, col] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, noise)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m augmentedDf\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:885\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    884\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 885\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1893\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[1;32m   1892\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[0;32m-> 1893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1986\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1983\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1984\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[1;32m   1985\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[0;32m-> 1986\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_column(loc, value, pi)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:2095\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[0;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[1;32m   2091\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39misetitem(loc, value)\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2093\u001b[0m     \u001b[38;5;66;03m# set value into the column (first attempting to operate inplace, then\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m     \u001b[38;5;66;03m#  falling back to casting if necessary)\u001b[39;00m\n\u001b[0;32m-> 2095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mcolumn_setitem(loc, plane_indexer, value)\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:1308\u001b[0m, in \u001b[0;36mBlockManager.column_setitem\u001b[0;34m(self, loc, idx, value, inplace_only)\u001b[0m\n\u001b[1;32m   1306\u001b[0m     col_mgr\u001b[38;5;241m.\u001b[39msetitem_inplace(idx, value)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1308\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m col_mgr\u001b[38;5;241m.\u001b[39msetitem((idx,), value)\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miset(loc, new_mgr\u001b[38;5;241m.\u001b[39m_block\u001b[38;5;241m.\u001b[39mvalues, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:399\u001b[0m, in \u001b[0;36mBaseBlockManager.setitem\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# No need to split if we either set all columns or on a single block\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# manager\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetitem\u001b[39m\u001b[38;5;124m\"\u001b[39m, indexer\u001b[38;5;241m=\u001b[39mindexer, value\u001b[38;5;241m=\u001b[39mvalue)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    355\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    357\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py:1758\u001b[0m, in \u001b[0;36mEABackedBlock.setitem\u001b[0;34m(self, indexer, value, using_cow)\u001b[0m\n\u001b[1;32m   1755\u001b[0m check_setitem_lengths(indexer, value, values)\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1758\u001b[0m     values[indexer] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1760\u001b[0m     _catch_deprecated_value_error(err)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/_mixins.py:253\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     key \u001b[38;5;241m=\u001b[39m check_array_indexer(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[0;32m--> 253\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_setitem_value(value)\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ndarray[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/categorical.py:1562\u001b[0m, in \u001b[0;36mCategorical._validate_setitem_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   1560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_listlike(value)\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_scalar(value)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/categorical.py:1587\u001b[0m, in \u001b[0;36mCategorical._validate_scalar\u001b[0;34m(self, fill_value)\u001b[0m\n\u001b[1;32m   1585\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbox_scalar(fill_value)\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot setitem on a Categorical with a new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1589\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfill_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), set the categories first\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1590\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fill_value\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot setitem on a Categorical with a new category (7.902380289776927), set the categories first"
     ]
    }
   ],
   "source": [
    "def betterApplyAugmentationMethods(X_train, method, nrows, nvalues=None, unit=None, noise=None):\n",
    "    # If nvalues not specified, entire column is selected\n",
    "    if nvalues is None:\n",
    "        nvalues = X_train.shape[1] - 1\n",
    "\n",
    "    if str(method).lower() == 'gausnoise':\n",
    "        # Creates empty dataframe to hold augmented rows\n",
    "        augmentedDf = pd.DataFrame()\n",
    "\n",
    "        # Selects random rows from data and appends to augmentedDf\n",
    "        for i in range(nrows):\n",
    "            augmentedDf = pd.concat([augmentedDf, X_train.iloc[[random.randint(0, X_train.shape[0] - 1)]]],\n",
    "                                    ignore_index=True)\n",
    "\n",
    "        # Selects random unique column index\n",
    "        randCols = random.sample(range(0, X_train.shape[1] - 1), nvalues)\n",
    "\n",
    "        # Applies Gaussian noise to randCols values stored in array\n",
    "        for i in range(augmentedDf.shape[0]):\n",
    "            for col in randCols:\n",
    "                augmentedDf.iloc[i, col] += np.random.normal(0, noise)\n",
    "\n",
    "        return augmentedDf\n",
    "\n",
    "    else:\n",
    "        print(\"Method not found\")\n",
    "        return None\n",
    "\n",
    "##################################\n",
    "data = np.loadtxt(\"/Users/fabianafazio/Documents/GitHub/BP24/Fabiana/Data/gaussian_large_d_1.tex\")\n",
    "array = np.array(data)\n",
    "df = pd.DataFrame(array)\n",
    "\n",
    "for i in range(25):\n",
    "    df.iloc[:,i] = df.iloc[:,i].round() # Rounding\n",
    "    df.iloc[:,i] = df.iloc[:,i].astype(int) # Integer\n",
    "    df.iloc[:,i] = df.iloc[:,i].astype('category') # Categories \n",
    "df.iloc[:,150] = df.iloc[:,150].astype('category')\n",
    "\n",
    "# Assuming the last column is the target\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "\n",
    "# Split your dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply the gausnoise augmentation method to the training set\n",
    "augmented_df = betterApplyAugmentationMethods(X_train, method=\"gausnoise\", nrows=200, nvalues=15, noise=0.1)\n",
    "\n",
    "# Example to show the augmented data\n",
    "print(augmented_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc32215-d74c-4924-802b-da0e6bfee145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
